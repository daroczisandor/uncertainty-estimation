{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2d1677c2-6f13-46c7-92f3-089fdca29b68","showTitle":false,"title":""}},"outputs":[],"source":["import sys\n","import os\n","sys.path.append(\"../\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"07c31e55-b5b9-4bd3-bc14-2d6b46b744ee","showTitle":false,"title":""}},"outputs":[{"data":{"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}","text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}","text/plain":""},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"430ffee6-f1079e6f0ffea6969b1b7793"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"}],"source":["import warnings\n","import time\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","\n","from ngboost.scores import CRPScore, LogScore\n","from ngboost.learners import default_tree_learner\n","from lightgbm import LGBMRegressor\n","\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn import model_selection\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_log_error, mean_squared_error"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c00e403b-3b60-4442-97b2-37fcba9bc5dc","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n","Creating extension directory /root/.cache/torch_extensions/py39_cu117/split_decision...\n","Detected CUDA files, patching ldflags\n","Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/split_decision/build.ninja...\n","Building extension module split_decision...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","Loading extension module split_decision...\n","/databricks/python/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /databricks/python3/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n","  warn(f\"Failed to load image Python extension: {e}\")\n","GPU available: True (cuda), used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-f3f486ad-a318-4e86-b365-2ab193cdfcb2/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n","  rank_zero_warn(\n","GPU available: True (cuda), used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py39_cu117/split_decision...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/split_decision/build.ninja...\nBuilding extension module split_decision...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module split_decision...\n/databricks/python/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /databricks/python3/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  warn(f\"Failed to load image Python extension: {e}\")\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-f3f486ad-a318-4e86-b365-2ab193cdfcb2/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n  rank_zero_warn(\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from uncertainty_estimation.uncertainty_estimation_models import Model, XGBoost, CQR, LightGBM, LSF, NGBoost, TFTPytorchFC, PGBM, LightGBMQuantileRegressor\n","from uncertainty_estimation.constants import DistEnum, PredEnum"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fefb2db0-4650-42d4-8f10-da21e858eda9","showTitle":false,"title":""}},"source":["## 1 Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3d502576-4a20-448d-9d0b-155e79a97bb8","showTitle":false,"title":""}},"outputs":[],"source":["import os\n","\n","ue_dir_path = os.path.dirname(os.path.dirname(os.getcwd()))\n","full_df_path = os.path.join(ue_dir_path, 'datasets', 'rossmann_full_df.pickle')\n","\n","full_df = pd.read_pickle(full_df_path)"]},{"cell_type":"markdown","metadata":{},"source":["## REMOVE BEFORE PUBLISHING"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_df = full_df.iloc[:10000, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7f0a3d18-8e86-48c0-bf22-54d4692843fd","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Store</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>DayOfWeek</th>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <td>2015-07-31 00:00:00</td>\n","      <td>2015-07-31 00:00:00</td>\n","      <td>2015-07-31 00:00:00</td>\n","      <td>2015-07-31 00:00:00</td>\n","      <td>2015-07-31 00:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>Sales</th>\n","      <td>5263.0</td>\n","      <td>6064.0</td>\n","      <td>8314.0</td>\n","      <td>13995.0</td>\n","      <td>4822.0</td>\n","    </tr>\n","    <tr>\n","      <th>Customers</th>\n","      <td>555.0</td>\n","      <td>625.0</td>\n","      <td>821.0</td>\n","      <td>1498.0</td>\n","      <td>559.0</td>\n","    </tr>\n","    <tr>\n","      <th>Open</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Promo</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>StateHoliday</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>SchoolHoliday</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>StoreType</th>\n","      <td>c</td>\n","      <td>a</td>\n","      <td>a</td>\n","      <td>c</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>Assortment</th>\n","      <td>a</td>\n","      <td>a</td>\n","      <td>a</td>\n","      <td>c</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>CompetitionDistance</th>\n","      <td>1270.0</td>\n","      <td>570.0</td>\n","      <td>14130.0</td>\n","      <td>620.0</td>\n","      <td>29910.0</td>\n","    </tr>\n","    <tr>\n","      <th>CompetitionOpenSinceMonth</th>\n","      <td>9.0</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>9.0</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>CompetitionOpenSinceYear</th>\n","      <td>2008.0</td>\n","      <td>2007.0</td>\n","      <td>2006.0</td>\n","      <td>2009.0</td>\n","      <td>2015.0</td>\n","    </tr>\n","    <tr>\n","      <th>Promo2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Promo2SinceWeek</th>\n","      <td>NaN</td>\n","      <td>13.0</td>\n","      <td>14.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Promo2SinceYear</th>\n","      <td>NaN</td>\n","      <td>2010.0</td>\n","      <td>2011.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>PromoInterval</th>\n","      <td>NaN</td>\n","      <td>Jan,Apr,Jul,Oct</td>\n","      <td>Jan,Apr,Jul,Oct</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Id</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>AfterPromo</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>AfterStateHoliday</th>\n","      <td>57</td>\n","      <td>67</td>\n","      <td>57</td>\n","      <td>67</td>\n","      <td>57</td>\n","    </tr>\n","    <tr>\n","      <th>AfterSchoolHoliday</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>BeforePromo</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>BeforeStateHoliday</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>BeforeSchoolHoliday</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Promo_bw</th>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>StateHoliday_bw</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>SchoolHoliday_bw</th>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>Promo_fw</th>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>StateHoliday_fw</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>SchoolHoliday_fw</th>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Year</th>\n","      <td>2015</td>\n","      <td>2015</td>\n","      <td>2015</td>\n","      <td>2015</td>\n","      <td>2015</td>\n","    </tr>\n","    <tr>\n","      <th>Month</th>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>Week</th>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>Day</th>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>31</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>Dayofweek</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>Dayofyear</th>\n","      <td>212</td>\n","      <td>212</td>\n","      <td>212</td>\n","      <td>212</td>\n","      <td>212</td>\n","    </tr>\n","    <tr>\n","      <th>Is_month_end</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>Is_month_start</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>Is_quarter_end</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>Is_quarter_start</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>Is_year_end</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>Is_year_start</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>Elapsed</th>\n","      <td>1438300800.0</td>\n","      <td>1438300800.0</td>\n","      <td>1438300800.0</td>\n","      <td>1438300800.0</td>\n","      <td>1438300800.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Store</th>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>DayOfWeek</th>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <td>2015-07-31 00:00:00</td>\n      <td>2015-07-31 00:00:00</td>\n      <td>2015-07-31 00:00:00</td>\n      <td>2015-07-31 00:00:00</td>\n      <td>2015-07-31 00:00:00</td>\n    </tr>\n    <tr>\n      <th>Sales</th>\n      <td>5263.0</td>\n      <td>6064.0</td>\n      <td>8314.0</td>\n      <td>13995.0</td>\n      <td>4822.0</td>\n    </tr>\n    <tr>\n      <th>Customers</th>\n      <td>555.0</td>\n      <td>625.0</td>\n      <td>821.0</td>\n      <td>1498.0</td>\n      <td>559.0</td>\n    </tr>\n    <tr>\n      <th>Open</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Promo</th>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>StateHoliday</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>SchoolHoliday</th>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>StoreType</th>\n      <td>c</td>\n      <td>a</td>\n      <td>a</td>\n      <td>c</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>Assortment</th>\n      <td>a</td>\n      <td>a</td>\n      <td>a</td>\n      <td>c</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>CompetitionDistance</th>\n      <td>1270.0</td>\n      <td>570.0</td>\n      <td>14130.0</td>\n      <td>620.0</td>\n      <td>29910.0</td>\n    </tr>\n    <tr>\n      <th>CompetitionOpenSinceMonth</th>\n      <td>9.0</td>\n      <td>11.0</td>\n      <td>12.0</td>\n      <td>9.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>CompetitionOpenSinceYear</th>\n      <td>2008.0</td>\n      <td>2007.0</td>\n      <td>2006.0</td>\n      <td>2009.0</td>\n      <td>2015.0</td>\n    </tr>\n    <tr>\n      <th>Promo2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Promo2SinceWeek</th>\n      <td>NaN</td>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Promo2SinceYear</th>\n      <td>NaN</td>\n      <td>2010.0</td>\n      <td>2011.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>PromoInterval</th>\n      <td>NaN</td>\n      <td>Jan,Apr,Jul,Oct</td>\n      <td>Jan,Apr,Jul,Oct</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>AfterPromo</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>AfterStateHoliday</th>\n      <td>57</td>\n      <td>67</td>\n      <td>57</td>\n      <td>67</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>AfterSchoolHoliday</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>BeforePromo</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>BeforeStateHoliday</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>BeforeSchoolHoliday</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Promo_bw</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>StateHoliday_bw</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>SchoolHoliday_bw</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Promo_fw</th>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>StateHoliday_fw</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>SchoolHoliday_fw</th>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Year</th>\n      <td>2015</td>\n      <td>2015</td>\n      <td>2015</td>\n      <td>2015</td>\n      <td>2015</td>\n    </tr>\n    <tr>\n      <th>Month</th>\n      <td>7</td>\n      <td>7</td>\n      <td>7</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>Week</th>\n      <td>31</td>\n      <td>31</td>\n      <td>31</td>\n      <td>31</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>Day</th>\n      <td>31</td>\n      <td>31</td>\n      <td>31</td>\n      <td>31</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>Dayofweek</th>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>Dayofyear</th>\n      <td>212</td>\n      <td>212</td>\n      <td>212</td>\n      <td>212</td>\n      <td>212</td>\n    </tr>\n    <tr>\n      <th>Is_month_end</th>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>Is_month_start</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>Is_quarter_end</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>Is_quarter_start</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>Is_year_end</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>Is_year_start</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>Elapsed</th>\n      <td>1438300800.0</td>\n      <td>1438300800.0</td>\n      <td>1438300800.0</td>\n      <td>1438300800.0</td>\n      <td>1438300800.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["full_df.head().T"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0291b15b-e94a-4859-b540-028ec6e7e332","showTitle":false,"title":""}},"source":["After the data is loaded the train data is sorted after entity and date."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0a288776-f5c2-48bf-a869-978f72ec6e3b","showTitle":false,"title":""}},"outputs":[],"source":["full_df = full_df.sort_values(['Store', 'Date'])\n","full_df = full_df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e48e1250-e720-4f33-94fa-ca5c8764b2f6","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Timestamp('2015-07-31 00:00:00')Timestamp('2013-01-01 00:00:00')"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Timestamp('2015-07-31 00:00:00')Timestamp('2013-01-01 00:00:00')","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["display(full_df['Date'].max())\n","display(full_df['Date'].min())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bc97429f-8691-4d18-a262-84cb8e6e7c16","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["844338"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"844338","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["display(len(full_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fa586a8f-358c-45a2-9303-c73d88dd281b","showTitle":false,"title":""}},"outputs":[],"source":["cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'StoreType', 'Assortment', \n","    'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', 'Week', 'Promo_fw', \n","    'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw', 'SchoolHoliday_fw', 'SchoolHoliday_bw', 'CompetitionDistance_na']\n","\n","cont_vars = ['CompetitionDistance', 'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']\n","full_df[cat_vars] = full_df[cat_vars].astype('object')"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c3252f41-fd46-4d59-b895-891bd21e22a8","showTitle":false,"title":""}},"source":["## 2 Vectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"334fe7fd-e949-4889-9bd0-36eb4a8c99e7","showTitle":false,"title":""}},"outputs":[],"source":["from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.pipeline import FeatureUnion, Pipeline\n","\n","import category_encoders as ce\n","import feature_engine.imputation as fe\n","\n","class CategoricalSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, field):\n","        self.field = field\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, dataframe):\n","        dt = dataframe[self.field].dtype\n","        return dataframe[[self.field]]\n","\n","\n","class ItemSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, field):\n","        self.field = field\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, dataframe):\n","        dt = dataframe[self.field].dtype\n","        if is_categorical_dtype(dt):\n","            return dataframe[self.field].cat.codes[:, None]\n","        elif is_numeric_dtype(dt):\n","            return dataframe[self.field][:, None]\n","        else:\n","            return dataframe[self.field]\n","\n","        \n","def create_feature_vectorizer_without_nan():\n","    vectorizer_tree = FeatureUnion([\n","        # Categoricals\n","        ('Store',\n","         Pipeline([('select', CategoricalSelector('Store')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('DayOfWeek',\n","         Pipeline([('select', CategoricalSelector('DayOfWeek')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('Year',\n","         Pipeline([('select', CategoricalSelector('Year')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('Week',\n","         Pipeline([('select', CategoricalSelector('Week')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('Month',\n","         Pipeline([('select', CategoricalSelector('Month')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('Day',\n","         Pipeline([('select', CategoricalSelector('Day')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('StateHoliday',\n","         Pipeline([('select', CategoricalSelector('StateHoliday')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('StoreType',\n","         Pipeline([('select', CategoricalSelector('StoreType')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('Assortment',\n","         Pipeline([('select', CategoricalSelector('Assortment')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('PromoInterval',\n","         Pipeline([('select', CategoricalSelector('PromoInterval')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('CompetitionOpenSinceYear',\n","         Pipeline([('select', CategoricalSelector('CompetitionOpenSinceYear')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('Promo2SinceYear',\n","         Pipeline([('select', CategoricalSelector('Promo2SinceYear')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('Promo_fw',\n","         Pipeline([('select', CategoricalSelector('Promo_fw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('Promo_bw',\n","         Pipeline([('select', CategoricalSelector('Promo_bw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('StateHoliday_fw',\n","         Pipeline([('select', CategoricalSelector('StateHoliday_fw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('StateHoliday_bw',\n","         Pipeline([('select', CategoricalSelector('StateHoliday_bw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('SchoolHoliday_fw',\n","         Pipeline([('select', CategoricalSelector('SchoolHoliday_fw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('SchoolHoliday_bw',\n","         Pipeline([('select', CategoricalSelector('SchoolHoliday_bw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        ('CompetitionDistance_na',\n","         Pipeline([('select', CategoricalSelector('SchoolHoliday_bw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing=\"value\", handle_unknown='value'))])),\n","        \n","        # Continuous\n","        ('CompetitionDistance',Pipeline([('select', ItemSelector('CompetitionDistance')),\n","                                        ('fe', fe.ArbitraryNumberImputer(arbitrary_number=-1))])),\n","        ('AfterStateHoliday',Pipeline([('select', ItemSelector('AfterStateHoliday'))])),\n","        ('BeforeStateHoliday',Pipeline([('select', ItemSelector('BeforeStateHoliday'))])),\n","        ('Promo',Pipeline([('select', ItemSelector('Promo'))])),\n","        ('SchoolHoliday',Pipeline([('select', ItemSelector('SchoolHoliday'))])),\n","    ], n_jobs=1)\n","\n","    return vectorizer_tree"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9921a8c7-ace6-486b-afa7-8a51f4180a06","showTitle":false,"title":""}},"outputs":[],"source":["class CategoricalSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, field):\n","        self.field = field\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, dataframe):\n","        dt = dataframe[self.field].dtype\n","        return dataframe[[self.field]]\n","\n","\n","class ItemSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, field):\n","        self.field = field\n","\n","    def fit(self, x, y=None):\n","        return self\n","\n","    def transform(self, dataframe):\n","        dt = dataframe[self.field].dtype\n","        if is_categorical_dtype(dt):\n","            return dataframe[self.field].cat.codes[:, None]\n","        elif is_numeric_dtype(dt):\n","            return dataframe[self.field][:, None]\n","        else:\n","            return dataframe[self.field]\n","\n","def create_feature_vectorizer_with_nan():\n","    vectorizer_tree = FeatureUnion([\n","        # Categoricals\n","        ('Store',\n","         Pipeline([('select', CategoricalSelector('Store')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('DayOfWeek',\n","         Pipeline([('select', CategoricalSelector('DayOfWeek')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('Year',\n","         Pipeline([('select', CategoricalSelector('Year')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","         ('Week',\n","         Pipeline([('select', CategoricalSelector('Week')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('Month',\n","         Pipeline([('select', CategoricalSelector('Month')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('Day',\n","         Pipeline([('select', CategoricalSelector('Day')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('StateHoliday',\n","         Pipeline([('select', CategoricalSelector('StateHoliday')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('StoreType',\n","         Pipeline([('select', CategoricalSelector('StoreType')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('Assortment',\n","         Pipeline([('select', CategoricalSelector('Assortment')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('PromoInterval',\n","         Pipeline([('select', CategoricalSelector('PromoInterval')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('CompetitionOpenSinceYear',\n","         Pipeline([('select', CategoricalSelector('CompetitionOpenSinceYear')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('Promo2SinceYear',\n","         Pipeline([('select', CategoricalSelector('Promo2SinceYear')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('Promo_fw',\n","         Pipeline([('select', CategoricalSelector('Promo_fw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('Promo_bw',\n","         Pipeline([('select', CategoricalSelector('Promo_bw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('StateHoliday_fw',\n","         Pipeline([('select', CategoricalSelector('StateHoliday_fw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('StateHoliday_bw',\n","         Pipeline([('select', CategoricalSelector('StateHoliday_bw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('SchoolHoliday_fw',\n","         Pipeline([('select', CategoricalSelector('SchoolHoliday_fw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        ('SchoolHoliday_bw',\n","         Pipeline([('select', CategoricalSelector('SchoolHoliday_bw')),\n","                   ('oe', ce.OrdinalEncoder(handle_missing='return_nan', handle_unknown='return_nan'))])),\n","        \n","        # Continuous\n","        ('CompetitionDistance',Pipeline([('select', ItemSelector('CompetitionDistance'))])),\n","        ('AfterStateHoliday',Pipeline([('select', ItemSelector('AfterStateHoliday'))])),\n","        ('BeforeStateHoliday',Pipeline([('select', ItemSelector('BeforeStateHoliday'))])),\n","        ('Promo',Pipeline([('select', ItemSelector('Promo'))])),\n","        ('SchoolHoliday',Pipeline([('select', ItemSelector('SchoolHoliday'))])),\n","    ], n_jobs=1)\n","\n","    return vectorizer_tree"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cb37524f-550f-4b40-9bef-bbb31f6343f7","showTitle":false,"title":""}},"outputs":[],"source":["vectorizer_without_nan = create_feature_vectorizer_without_nan()\n","vectorizer_with_nan = create_feature_vectorizer_with_nan()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"86400e50-9630-4153-b48d-c312c7e64083","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["(844338, 24)\n","(844338, 23)\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"(844338, 24)\n(844338, 23)\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["print(vectorizer_without_nan.fit_transform(full_df).shape)\n","print(vectorizer_with_nan.fit_transform(full_df).shape)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"40eb358d-bc43-436b-a6c5-b3775ce8bd4a","showTitle":false,"title":""}},"source":["## 3 Model Application"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"51df2fd4-9b62-4186-a69e-d7a0c80eacac","showTitle":false,"title":""}},"source":["For our models taking in the data as tabular data we need to create train, val, test splits for our Forecast Horizon.\n","Each split only needs to contain the rows where the target is in which we want to train on / predict on. This means that the feature in the same row is the only information taken into account when doing the prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5ded7c03-7faf-417e-a7d6-0d1e204cc9dd","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>DayOfWeek</th><th>Date</th><th>Sales</th><th>Customers</th><th>Open</th><th>Promo</th><th>StateHoliday</th><th>SchoolHoliday</th><th>StoreType</th><th>Assortment</th><th>CompetitionDistance</th><th>CompetitionOpenSinceMonth</th><th>CompetitionOpenSinceYear</th><th>Promo2</th><th>Promo2SinceWeek</th><th>Promo2SinceYear</th><th>PromoInterval</th><th>Id</th><th>AfterPromo</th><th>AfterStateHoliday</th><th>AfterSchoolHoliday</th><th>BeforePromo</th><th>BeforeStateHoliday</th><th>BeforeSchoolHoliday</th><th>Promo_bw</th><th>StateHoliday_bw</th><th>SchoolHoliday_bw</th><th>Promo_fw</th><th>StateHoliday_fw</th><th>SchoolHoliday_fw</th><th>Year</th><th>Month</th><th>Week</th><th>Day</th><th>Dayofweek</th><th>Dayofyear</th><th>Is_month_end</th><th>Is_month_start</th><th>Is_quarter_end</th><th>Is_quarter_start</th><th>Is_year_end</th><th>Is_year_start</th><th>Elapsed</th><th>CompetitionDistance_na</th></tr></thead><tbody><tr><td>1115</td><td>2</td><td>2015-06-09T00:00:00.000+0000</td><td>5119.0</td><td>363.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>4</td><td>5</td><td>60</td><td>-6</td><td>0</td><td>-48</td><td>3.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>9</td><td>1</td><td>160</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.433808E9</td><td>0</td></tr><tr><td>1115</td><td>3</td><td>2015-06-10T00:00:00.000+0000</td><td>4676.0</td><td>357.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>5</td><td>6</td><td>61</td><td>-5</td><td>0</td><td>-47</td><td>2.0</td><td>1.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>10</td><td>2</td><td>161</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4338944E9</td><td>0</td></tr><tr><td>1115</td><td>4</td><td>2015-06-11T00:00:00.000+0000</td><td>5216.0</td><td>380.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>6</td><td>7</td><td>62</td><td>-4</td><td>0</td><td>-46</td><td>1.0</td><td>0.0</td><td>0.0</td><td>3.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>11</td><td>3</td><td>162</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4339808E9</td><td>0</td></tr><tr><td>1115</td><td>5</td><td>2015-06-12T00:00:00.000+0000</td><td>5315.0</td><td>378.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>7</td><td>8</td><td>63</td><td>-3</td><td>0</td><td>-45</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>12</td><td>4</td><td>163</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4340672E9</td><td>0</td></tr><tr><td>1115</td><td>6</td><td>2015-06-13T00:00:00.000+0000</td><td>7736.0</td><td>503.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>8</td><td>9</td><td>64</td><td>-2</td><td>0</td><td>-44</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>13</td><td>5</td><td>164</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4341536E9</td><td>0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1115,2,"2015-06-09T00:00:00.000+0000",5119,363,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,4,5,60,-6,0,-48,3,1,0,1,0,0,2015,6,24,9,1,160,false,false,false,false,false,false,1433808000,0],[1115,3,"2015-06-10T00:00:00.000+0000",4676,357,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,5,6,61,-5,0,-47,2,1,0,2,0,0,2015,6,24,10,2,161,false,false,false,false,false,false,1433894400,0],[1115,4,"2015-06-11T00:00:00.000+0000",5216,380,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,6,7,62,-4,0,-46,1,0,0,3,0,0,2015,6,24,11,3,162,false,false,false,false,false,false,1433980800,0],[1115,5,"2015-06-12T00:00:00.000+0000",5315,378,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,7,8,63,-3,0,-45,0,0,0,4,0,0,2015,6,24,12,4,163,false,false,false,false,false,false,1434067200,0],[1115,6,"2015-06-13T00:00:00.000+0000",7736,503,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,8,9,64,-2,0,-44,0,0,0,5,0,0,2015,6,24,13,5,164,false,false,false,false,false,false,1434153600,0]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"Store","type":"\"long\""},{"metadata":"{}","name":"DayOfWeek","type":"\"long\""},{"metadata":"{}","name":"Date","type":"\"timestamp\""},{"metadata":"{}","name":"Sales","type":"\"double\""},{"metadata":"{}","name":"Customers","type":"\"double\""},{"metadata":"{}","name":"Open","type":"\"double\""},{"metadata":"{}","name":"Promo","type":"\"boolean\""},{"metadata":"{}","name":"StateHoliday","type":"\"boolean\""},{"metadata":"{}","name":"SchoolHoliday","type":"\"boolean\""},{"metadata":"{}","name":"StoreType","type":"\"string\""},{"metadata":"{}","name":"Assortment","type":"\"string\""},{"metadata":"{}","name":"CompetitionDistance","type":"\"double\""},{"metadata":"{}","name":"CompetitionOpenSinceMonth","type":"\"double\""},{"metadata":"{}","name":"CompetitionOpenSinceYear","type":"\"void\""},{"metadata":"{}","name":"Promo2","type":"\"long\""},{"metadata":"{}","name":"Promo2SinceWeek","type":"\"double\""},{"metadata":"{}","name":"Promo2SinceYear","type":"\"double\""},{"metadata":"{}","name":"PromoInterval","type":"\"string\""},{"metadata":"{}","name":"Id","type":"\"double\""},{"metadata":"{}","name":"AfterPromo","type":"\"integer\""},{"metadata":"{}","name":"AfterStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"AfterSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforePromo","type":"\"integer\""},{"metadata":"{}","name":"BeforeStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforeSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"Promo_bw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"SchoolHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"Promo_fw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"SchoolHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"Year","type":"\"long\""},{"metadata":"{}","name":"Month","type":"\"long\""},{"metadata":"{}","name":"Week","type":"\"long\""},{"metadata":"{}","name":"Day","type":"\"long\""},{"metadata":"{}","name":"Dayofweek","type":"\"long\""},{"metadata":"{}","name":"Dayofyear","type":"\"long\""},{"metadata":"{}","name":"Is_month_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_month_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_start","type":"\"boolean\""},{"metadata":"{}","name":"Elapsed","type":"\"double\""},{"metadata":"{}","name":"CompetitionDistance_na","type":"\"long\""}],"type":"table"}},"output_type":"display_data"},{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>DayOfWeek</th><th>Date</th><th>Sales</th><th>Customers</th><th>Open</th><th>Promo</th><th>StateHoliday</th><th>SchoolHoliday</th><th>StoreType</th><th>Assortment</th><th>CompetitionDistance</th><th>CompetitionOpenSinceMonth</th><th>CompetitionOpenSinceYear</th><th>Promo2</th><th>Promo2SinceWeek</th><th>Promo2SinceYear</th><th>PromoInterval</th><th>Id</th><th>AfterPromo</th><th>AfterStateHoliday</th><th>AfterSchoolHoliday</th><th>BeforePromo</th><th>BeforeStateHoliday</th><th>BeforeSchoolHoliday</th><th>Promo_bw</th><th>StateHoliday_bw</th><th>SchoolHoliday_bw</th><th>Promo_fw</th><th>StateHoliday_fw</th><th>SchoolHoliday_fw</th><th>Year</th><th>Month</th><th>Week</th><th>Day</th><th>Dayofweek</th><th>Dayofyear</th><th>Is_month_end</th><th>Is_month_start</th><th>Is_quarter_end</th><th>Is_quarter_start</th><th>Is_year_end</th><th>Is_year_start</th><th>Elapsed</th><th>CompetitionDistance_na</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>2015-04-27T00:00:00.000+0000</td><td>5575.0</td><td>574.0</td><td>1.0</td><td>true</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>21</td><td>17</td><td>0</td><td>-4</td><td>-91</td><td>1.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>1.0</td><td>0.0</td><td>2015</td><td>4</td><td>18</td><td>27</td><td>0</td><td>117</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4300928E9</td><td>0</td></tr><tr><td>1</td><td>2</td><td>2015-04-28T00:00:00.000+0000</td><td>5199.0</td><td>552.0</td><td>1.0</td><td>true</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>22</td><td>18</td><td>0</td><td>-3</td><td>-90</td><td>2.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>1.0</td><td>0.0</td><td>2015</td><td>4</td><td>18</td><td>28</td><td>1</td><td>118</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4301792E9</td><td>0</td></tr><tr><td>1</td><td>3</td><td>2015-04-29T00:00:00.000+0000</td><td>5775.0</td><td>579.0</td><td>1.0</td><td>true</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>23</td><td>19</td><td>0</td><td>-2</td><td>-89</td><td>3.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>1.0</td><td>0.0</td><td>2015</td><td>4</td><td>18</td><td>29</td><td>2</td><td>119</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4302656E9</td><td>0</td></tr><tr><td>1</td><td>4</td><td>2015-04-30T00:00:00.000+0000</td><td>6228.0</td><td>650.0</td><td>1.0</td><td>true</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>24</td><td>20</td><td>0</td><td>-1</td><td>-88</td><td>4.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>1.0</td><td>0.0</td><td>2015</td><td>4</td><td>18</td><td>30</td><td>3</td><td>120</td><td>true</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.430352E9</td><td>0</td></tr><tr><td>1</td><td>6</td><td>2015-05-02T00:00:00.000+0000</td><td>5850.0</td><td>653.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td><td>1</td><td>22</td><td>-2</td><td>-12</td><td>-86</td><td>5.0</td><td>1.0</td><td>0.0</td><td>5.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>5</td><td>18</td><td>2</td><td>5</td><td>122</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4305248E9</td><td>0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1,1,"2015-04-27T00:00:00.000+0000",5575,574,1,true,false,false,"c","a",1270,9,2008,0,null,null,null,null,0,21,17,0,-4,-91,1,0,0,5,1,0,2015,4,18,27,0,117,false,false,false,false,false,false,1430092800,0],[1,2,"2015-04-28T00:00:00.000+0000",5199,552,1,true,false,false,"c","a",1270,9,2008,0,null,null,null,null,0,22,18,0,-3,-90,2,0,0,5,1,0,2015,4,18,28,1,118,false,false,false,false,false,false,1430179200,0],[1,3,"2015-04-29T00:00:00.000+0000",5775,579,1,true,false,false,"c","a",1270,9,2008,0,null,null,null,null,0,23,19,0,-2,-89,3,0,0,5,1,0,2015,4,18,29,2,119,false,false,false,false,false,false,1430265600,0],[1,4,"2015-04-30T00:00:00.000+0000",6228,650,1,true,false,false,"c","a",1270,9,2008,0,null,null,null,null,0,24,20,0,-1,-88,4,0,0,5,1,0,2015,4,18,30,3,120,true,false,false,false,false,false,1430352000,0],[1,6,"2015-05-02T00:00:00.000+0000",5850,653,1,false,false,false,"c","a",1270,9,2008,0,null,null,null,null,1,1,22,-2,-12,-86,5,1,0,5,0,0,2015,5,18,2,5,122,false,false,false,false,false,false,1430524800,0]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"Store","type":"\"long\""},{"metadata":"{}","name":"DayOfWeek","type":"\"long\""},{"metadata":"{}","name":"Date","type":"\"timestamp\""},{"metadata":"{}","name":"Sales","type":"\"double\""},{"metadata":"{}","name":"Customers","type":"\"double\""},{"metadata":"{}","name":"Open","type":"\"double\""},{"metadata":"{}","name":"Promo","type":"\"boolean\""},{"metadata":"{}","name":"StateHoliday","type":"\"boolean\""},{"metadata":"{}","name":"SchoolHoliday","type":"\"boolean\""},{"metadata":"{}","name":"StoreType","type":"\"string\""},{"metadata":"{}","name":"Assortment","type":"\"string\""},{"metadata":"{}","name":"CompetitionDistance","type":"\"double\""},{"metadata":"{}","name":"CompetitionOpenSinceMonth","type":"\"double\""},{"metadata":"{}","name":"CompetitionOpenSinceYear","type":"\"double\""},{"metadata":"{}","name":"Promo2","type":"\"long\""},{"metadata":"{}","name":"Promo2SinceWeek","type":"\"double\""},{"metadata":"{}","name":"Promo2SinceYear","type":"\"void\""},{"metadata":"{}","name":"PromoInterval","type":"\"void\""},{"metadata":"{}","name":"Id","type":"\"double\""},{"metadata":"{}","name":"AfterPromo","type":"\"integer\""},{"metadata":"{}","name":"AfterStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"AfterSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforePromo","type":"\"integer\""},{"metadata":"{}","name":"BeforeStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforeSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"Promo_bw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"SchoolHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"Promo_fw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"SchoolHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"Year","type":"\"long\""},{"metadata":"{}","name":"Month","type":"\"long\""},{"metadata":"{}","name":"Week","type":"\"long\""},{"metadata":"{}","name":"Day","type":"\"long\""},{"metadata":"{}","name":"Dayofweek","type":"\"long\""},{"metadata":"{}","name":"Dayofyear","type":"\"long\""},{"metadata":"{}","name":"Is_month_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_month_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_start","type":"\"boolean\""},{"metadata":"{}","name":"Elapsed","type":"\"double\""},{"metadata":"{}","name":"CompetitionDistance_na","type":"\"long\""}],"type":"table"}},"output_type":"display_data"},{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>DayOfWeek</th><th>Date</th><th>Sales</th><th>Customers</th><th>Open</th><th>Promo</th><th>StateHoliday</th><th>SchoolHoliday</th><th>StoreType</th><th>Assortment</th><th>CompetitionDistance</th><th>CompetitionOpenSinceMonth</th><th>CompetitionOpenSinceYear</th><th>Promo2</th><th>Promo2SinceWeek</th><th>Promo2SinceYear</th><th>PromoInterval</th><th>Id</th><th>AfterPromo</th><th>AfterStateHoliday</th><th>AfterSchoolHoliday</th><th>BeforePromo</th><th>BeforeStateHoliday</th><th>BeforeSchoolHoliday</th><th>Promo_bw</th><th>StateHoliday_bw</th><th>SchoolHoliday_bw</th><th>Promo_fw</th><th>StateHoliday_fw</th><th>SchoolHoliday_fw</th><th>Year</th><th>Month</th><th>Week</th><th>Day</th><th>Dayofweek</th><th>Dayofyear</th><th>Is_month_end</th><th>Is_month_start</th><th>Is_quarter_end</th><th>Is_quarter_start</th><th>Is_year_end</th><th>Is_year_start</th><th>Elapsed</th><th>CompetitionDistance_na</th></tr></thead><tbody><tr><td>1115</td><td>2</td><td>2015-06-09T00:00:00.000+0000</td><td>5119.0</td><td>363.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>4</td><td>5</td><td>60</td><td>-6</td><td>0</td><td>-48</td><td>3.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>9</td><td>1</td><td>160</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.433808E9</td><td>0</td></tr><tr><td>1115</td><td>3</td><td>2015-06-10T00:00:00.000+0000</td><td>4676.0</td><td>357.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>5</td><td>6</td><td>61</td><td>-5</td><td>0</td><td>-47</td><td>2.0</td><td>1.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>10</td><td>2</td><td>161</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4338944E9</td><td>0</td></tr><tr><td>1115</td><td>4</td><td>2015-06-11T00:00:00.000+0000</td><td>5216.0</td><td>380.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>6</td><td>7</td><td>62</td><td>-4</td><td>0</td><td>-46</td><td>1.0</td><td>0.0</td><td>0.0</td><td>3.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>11</td><td>3</td><td>162</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4339808E9</td><td>0</td></tr><tr><td>1115</td><td>5</td><td>2015-06-12T00:00:00.000+0000</td><td>5315.0</td><td>378.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>7</td><td>8</td><td>63</td><td>-3</td><td>0</td><td>-45</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>12</td><td>4</td><td>163</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4340672E9</td><td>0</td></tr><tr><td>1115</td><td>6</td><td>2015-06-13T00:00:00.000+0000</td><td>7736.0</td><td>503.0</td><td>1.0</td><td>false</td><td>false</td><td>false</td><td>d</td><td>c</td><td>5350.0</td><td>null</td><td>null</td><td>1</td><td>22.0</td><td>2012.0</td><td>Mar,Jun,Sept,Dec</td><td>null</td><td>8</td><td>9</td><td>64</td><td>-2</td><td>0</td><td>-44</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>24</td><td>13</td><td>5</td><td>164</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4341536E9</td><td>0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1115,2,"2015-06-09T00:00:00.000+0000",5119,363,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,4,5,60,-6,0,-48,3,1,0,1,0,0,2015,6,24,9,1,160,false,false,false,false,false,false,1433808000,0],[1115,3,"2015-06-10T00:00:00.000+0000",4676,357,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,5,6,61,-5,0,-47,2,1,0,2,0,0,2015,6,24,10,2,161,false,false,false,false,false,false,1433894400,0],[1115,4,"2015-06-11T00:00:00.000+0000",5216,380,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,6,7,62,-4,0,-46,1,0,0,3,0,0,2015,6,24,11,3,162,false,false,false,false,false,false,1433980800,0],[1115,5,"2015-06-12T00:00:00.000+0000",5315,378,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,7,8,63,-3,0,-45,0,0,0,4,0,0,2015,6,24,12,4,163,false,false,false,false,false,false,1434067200,0],[1115,6,"2015-06-13T00:00:00.000+0000",7736,503,1,false,false,false,"d","c",5350,null,null,1,22,2012,"Mar,Jun,Sept,Dec",null,8,9,64,-2,0,-44,0,0,0,5,0,0,2015,6,24,13,5,164,false,false,false,false,false,false,1434153600,0]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"Store","type":"\"long\""},{"metadata":"{}","name":"DayOfWeek","type":"\"long\""},{"metadata":"{}","name":"Date","type":"\"timestamp\""},{"metadata":"{}","name":"Sales","type":"\"double\""},{"metadata":"{}","name":"Customers","type":"\"double\""},{"metadata":"{}","name":"Open","type":"\"double\""},{"metadata":"{}","name":"Promo","type":"\"boolean\""},{"metadata":"{}","name":"StateHoliday","type":"\"boolean\""},{"metadata":"{}","name":"SchoolHoliday","type":"\"boolean\""},{"metadata":"{}","name":"StoreType","type":"\"string\""},{"metadata":"{}","name":"Assortment","type":"\"string\""},{"metadata":"{}","name":"CompetitionDistance","type":"\"double\""},{"metadata":"{}","name":"CompetitionOpenSinceMonth","type":"\"double\""},{"metadata":"{}","name":"CompetitionOpenSinceYear","type":"\"void\""},{"metadata":"{}","name":"Promo2","type":"\"long\""},{"metadata":"{}","name":"Promo2SinceWeek","type":"\"double\""},{"metadata":"{}","name":"Promo2SinceYear","type":"\"double\""},{"metadata":"{}","name":"PromoInterval","type":"\"string\""},{"metadata":"{}","name":"Id","type":"\"double\""},{"metadata":"{}","name":"AfterPromo","type":"\"integer\""},{"metadata":"{}","name":"AfterStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"AfterSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforePromo","type":"\"integer\""},{"metadata":"{}","name":"BeforeStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforeSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"Promo_bw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"SchoolHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"Promo_fw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"SchoolHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"Year","type":"\"long\""},{"metadata":"{}","name":"Month","type":"\"long\""},{"metadata":"{}","name":"Week","type":"\"long\""},{"metadata":"{}","name":"Day","type":"\"long\""},{"metadata":"{}","name":"Dayofweek","type":"\"long\""},{"metadata":"{}","name":"Dayofyear","type":"\"long\""},{"metadata":"{}","name":"Is_month_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_month_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_start","type":"\"boolean\""},{"metadata":"{}","name":"Elapsed","type":"\"double\""},{"metadata":"{}","name":"CompetitionDistance_na","type":"\"long\""}],"type":"table"}},"output_type":"display_data"},{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>DayOfWeek</th><th>Date</th><th>Sales</th><th>Customers</th><th>Open</th><th>Promo</th><th>StateHoliday</th><th>SchoolHoliday</th><th>StoreType</th><th>Assortment</th><th>CompetitionDistance</th><th>CompetitionOpenSinceMonth</th><th>CompetitionOpenSinceYear</th><th>Promo2</th><th>Promo2SinceWeek</th><th>Promo2SinceYear</th><th>PromoInterval</th><th>Id</th><th>AfterPromo</th><th>AfterStateHoliday</th><th>AfterSchoolHoliday</th><th>BeforePromo</th><th>BeforeStateHoliday</th><th>BeforeSchoolHoliday</th><th>Promo_bw</th><th>StateHoliday_bw</th><th>SchoolHoliday_bw</th><th>Promo_fw</th><th>StateHoliday_fw</th><th>SchoolHoliday_fw</th><th>Year</th><th>Month</th><th>Week</th><th>Day</th><th>Dayofweek</th><th>Dayofyear</th><th>Is_month_end</th><th>Is_month_start</th><th>Is_quarter_end</th><th>Is_quarter_start</th><th>Is_year_end</th><th>Is_year_start</th><th>Elapsed</th><th>CompetitionDistance_na</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>2015-06-15T00:00:00.000+0000</td><td>5518.0</td><td>586.0</td><td>1.0</td><td>true</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>11</td><td>66</td><td>0</td><td>0</td><td>-42</td><td>1.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>25</td><td>15</td><td>0</td><td>166</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4343264E9</td><td>0</td></tr><tr><td>1</td><td>2</td><td>2015-06-16T00:00:00.000+0000</td><td>4852.0</td><td>503.0</td><td>1.0</td><td>true</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>12</td><td>67</td><td>0</td><td>0</td><td>-41</td><td>2.0</td><td>0.0</td><td>0.0</td><td>4.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>25</td><td>16</td><td>1</td><td>167</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4344128E9</td><td>0</td></tr><tr><td>1</td><td>3</td><td>2015-06-17T00:00:00.000+0000</td><td>4000.0</td><td>476.0</td><td>1.0</td><td>true</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>13</td><td>68</td><td>0</td><td>0</td><td>-40</td><td>3.0</td><td>0.0</td><td>0.0</td><td>3.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>25</td><td>17</td><td>2</td><td>168</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4344992E9</td><td>0</td></tr><tr><td>1</td><td>4</td><td>2015-06-18T00:00:00.000+0000</td><td>4645.0</td><td>498.0</td><td>1.0</td><td>true</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>14</td><td>69</td><td>0</td><td>0</td><td>-39</td><td>4.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>25</td><td>18</td><td>3</td><td>169</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.4345856E9</td><td>0</td></tr><tr><td>1</td><td>5</td><td>2015-06-19T00:00:00.000+0000</td><td>4202.0</td><td>487.0</td><td>1.0</td><td>true</td><td>false</td><td>false</td><td>c</td><td>a</td><td>1270.0</td><td>9.0</td><td>2008.0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>15</td><td>70</td><td>0</td><td>0</td><td>-38</td><td>5.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>2015</td><td>6</td><td>25</td><td>19</td><td>4</td><td>170</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1.434672E9</td><td>0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1,1,"2015-06-15T00:00:00.000+0000",5518,586,1,true,false,false,"c","a",1270,9,2008,0,null,null,null,null,0,11,66,0,0,-42,1,0,0,5,0,0,2015,6,25,15,0,166,false,false,false,false,false,false,1434326400,0],[1,2,"2015-06-16T00:00:00.000+0000",4852,503,1,true,false,false,"c","a",1270,9,2008,0,null,null,null,null,0,12,67,0,0,-41,2,0,0,4,0,0,2015,6,25,16,1,167,false,false,false,false,false,false,1434412800,0],[1,3,"2015-06-17T00:00:00.000+0000",4000,476,1,true,false,false,"c","a",1270,9,2008,0,null,null,null,null,0,13,68,0,0,-40,3,0,0,3,0,0,2015,6,25,17,2,168,false,false,false,false,false,false,1434499200,0],[1,4,"2015-06-18T00:00:00.000+0000",4645,498,1,true,false,false,"c","a",1270,9,2008,0,null,null,null,null,0,14,69,0,0,-39,4,0,0,2,0,0,2015,6,25,18,3,169,false,false,false,false,false,false,1434585600,0],[1,5,"2015-06-19T00:00:00.000+0000",4202,487,1,true,false,false,"c","a",1270,9,2008,0,null,null,null,null,0,15,70,0,0,-38,5,0,0,1,0,0,2015,6,25,19,4,170,false,false,false,false,false,false,1434672000,0]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"Store","type":"\"long\""},{"metadata":"{}","name":"DayOfWeek","type":"\"long\""},{"metadata":"{}","name":"Date","type":"\"timestamp\""},{"metadata":"{}","name":"Sales","type":"\"double\""},{"metadata":"{}","name":"Customers","type":"\"double\""},{"metadata":"{}","name":"Open","type":"\"double\""},{"metadata":"{}","name":"Promo","type":"\"boolean\""},{"metadata":"{}","name":"StateHoliday","type":"\"boolean\""},{"metadata":"{}","name":"SchoolHoliday","type":"\"boolean\""},{"metadata":"{}","name":"StoreType","type":"\"string\""},{"metadata":"{}","name":"Assortment","type":"\"string\""},{"metadata":"{}","name":"CompetitionDistance","type":"\"double\""},{"metadata":"{}","name":"CompetitionOpenSinceMonth","type":"\"double\""},{"metadata":"{}","name":"CompetitionOpenSinceYear","type":"\"double\""},{"metadata":"{}","name":"Promo2","type":"\"long\""},{"metadata":"{}","name":"Promo2SinceWeek","type":"\"double\""},{"metadata":"{}","name":"Promo2SinceYear","type":"\"void\""},{"metadata":"{}","name":"PromoInterval","type":"\"void\""},{"metadata":"{}","name":"Id","type":"\"double\""},{"metadata":"{}","name":"AfterPromo","type":"\"integer\""},{"metadata":"{}","name":"AfterStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"AfterSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforePromo","type":"\"integer\""},{"metadata":"{}","name":"BeforeStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforeSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"Promo_bw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"SchoolHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"Promo_fw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"SchoolHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"Year","type":"\"long\""},{"metadata":"{}","name":"Month","type":"\"long\""},{"metadata":"{}","name":"Week","type":"\"long\""},{"metadata":"{}","name":"Day","type":"\"long\""},{"metadata":"{}","name":"Dayofweek","type":"\"long\""},{"metadata":"{}","name":"Dayofyear","type":"\"long\""},{"metadata":"{}","name":"Is_month_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_month_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_start","type":"\"boolean\""},{"metadata":"{}","name":"Elapsed","type":"\"double\""},{"metadata":"{}","name":"CompetitionDistance_na","type":"\"long\""}],"type":"table"}},"output_type":"display_data"}],"source":["# inputs required for networks taking in tabular data \n","train_val_df = full_df[full_df_copy['Date'] < \"20150427\"].sort_values(['Store', 'Date'])\n","valid_df = full_df[(full_df_copy['Date'] < \"20150614\") & (full_df['Date'] >= \"20150427\")].sort_values(['Store', 'Date'])\n","train_df = full_df[full_df_copy['Date'] < \"20150614\"].sort_values(['Store', 'Date'])\n","test_df = full_df[(full_df_copy['Date'] >= \"20150614\")].sort_values(['Store', 'Date'])\n","\n","display(train_df.tail())\n","display(valid_df.head())\n","display(valid_df.tail())\n","display(test_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0299af8f-fde7-4e76-bd8d-75ab64110a86","showTitle":false,"title":""}},"outputs":[],"source":["TARGET = 'Sales'\n","target_transformer = 'log1p'\n","forecast_horizon = 48"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"566873a5-fee4-4e4b-8fbf-3176df18090a","showTitle":false,"title":""}},"source":["## 3.1 LightGBM"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e4774308-b419-4cc1-9f85-0f0d69fd256f","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["e4bdf26cf59b40ddb18f8ff20915cf5a\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[1]\tvalid_0's l2: 0.160617\n","[2]\tvalid_0's l2: 0.153625\n","[3]\tvalid_0's l2: 0.14661\n","[4]\tvalid_0's l2: 0.141007\n","[5]\tvalid_0's l2: 0.135964\n","[6]\tvalid_0's l2: 0.130492\n","[7]\tvalid_0's l2: 0.127126\n","[8]\tvalid_0's l2: 0.123155\n","[9]\tvalid_0's l2: 0.119451\n","[10]\tvalid_0's l2: 0.115901\n","[11]\tvalid_0's l2: 0.114463\n","[12]\tvalid_0's l2: 0.111633\n","[13]\tvalid_0's l2: 0.108981\n","[14]\tvalid_0's l2: 0.107544\n","[15]\tvalid_0's l2: 0.105766\n","[16]\tvalid_0's l2: 0.104299\n","[17]\tvalid_0's l2: 0.102308\n","[18]\tvalid_0's l2: 0.0991005\n","[19]\tvalid_0's l2: 0.0977477\n","[20]\tvalid_0's l2: 0.0955909\n","[21]\tvalid_0's l2: 0.0926906\n","[22]\tvalid_0's l2: 0.0918178\n","[23]\tvalid_0's l2: 0.0909734\n","[24]\tvalid_0's l2: 0.0886536\n","[25]\tvalid_0's l2: 0.0873099\n","[26]\tvalid_0's l2: 0.0863762\n","[27]\tvalid_0's l2: 0.0858924\n","[28]\tvalid_0's l2: 0.0836161\n","[29]\tvalid_0's l2: 0.0815558\n","[30]\tvalid_0's l2: 0.0810937\n","[31]\tvalid_0's l2: 0.0805799\n","[32]\tvalid_0's l2: 0.0785332\n","[33]\tvalid_0's l2: 0.0779857\n","[34]\tvalid_0's l2: 0.0763855\n","[35]\tvalid_0's l2: 0.0742784\n","[36]\tvalid_0's l2: 0.0736242\n","[37]\tvalid_0's l2: 0.072675\n","[38]\tvalid_0's l2: 0.072303\n","[39]\tvalid_0's l2: 0.0709234\n","[40]\tvalid_0's l2: 0.0705547\n","[41]\tvalid_0's l2: 0.0701577\n","[42]\tvalid_0's l2: 0.0698485\n","[43]\tvalid_0's l2: 0.0695771\n","[44]\tvalid_0's l2: 0.0680874\n","[45]\tvalid_0's l2: 0.0679134\n","[46]\tvalid_0's l2: 0.0675005\n","[47]\tvalid_0's l2: 0.0665441\n","[48]\tvalid_0's l2: 0.0662122\n","[49]\tvalid_0's l2: 0.0651592\n","[50]\tvalid_0's l2: 0.064559\n","[51]\tvalid_0's l2: 0.0633161\n","[52]\tvalid_0's l2: 0.0638053\n","[53]\tvalid_0's l2: 0.0632173\n","[54]\tvalid_0's l2: 0.0620738\n","[55]\tvalid_0's l2: 0.0612039\n","[56]\tvalid_0's l2: 0.061104\n","[57]\tvalid_0's l2: 0.0601623\n","[58]\tvalid_0's l2: 0.0598367\n","[59]\tvalid_0's l2: 0.0592888\n","[60]\tvalid_0's l2: 0.0587344\n","[61]\tvalid_0's l2: 0.0579575\n","[62]\tvalid_0's l2: 0.0574032\n","[63]\tvalid_0's l2: 0.0566676\n","[64]\tvalid_0's l2: 0.0553941\n","[65]\tvalid_0's l2: 0.0551577\n","[66]\tvalid_0's l2: 0.0541875\n","[67]\tvalid_0's l2: 0.0531076\n","[68]\tvalid_0's l2: 0.0525344\n","[69]\tvalid_0's l2: 0.0524199\n","[70]\tvalid_0's l2: 0.0517432\n","[71]\tvalid_0's l2: 0.0515691\n","[72]\tvalid_0's l2: 0.050923\n","[73]\tvalid_0's l2: 0.0503988\n","[74]\tvalid_0's l2: 0.0502442\n","[75]\tvalid_0's l2: 0.0497385\n","[76]\tvalid_0's l2: 0.0493225\n","[77]\tvalid_0's l2: 0.0483756\n","[78]\tvalid_0's l2: 0.0483344\n","[79]\tvalid_0's l2: 0.0479998\n","[80]\tvalid_0's l2: 0.0479171\n","[81]\tvalid_0's l2: 0.0472956\n","[82]\tvalid_0's l2: 0.0467158\n","[83]\tvalid_0's l2: 0.04602\n","[84]\tvalid_0's l2: 0.0456554\n","[85]\tvalid_0's l2: 0.0453842\n","[86]\tvalid_0's l2: 0.0445794\n","[87]\tvalid_0's l2: 0.0439826\n","[88]\tvalid_0's l2: 0.0437708\n","[89]\tvalid_0's l2: 0.0434588\n","[90]\tvalid_0's l2: 0.0430457\n","[91]\tvalid_0's l2: 0.0429442\n","[92]\tvalid_0's l2: 0.0423459\n","[93]\tvalid_0's l2: 0.0420101\n","[94]\tvalid_0's l2: 0.0417991\n","[95]\tvalid_0's l2: 0.041453\n","[96]\tvalid_0's l2: 0.0411961\n","[97]\tvalid_0's l2: 0.041045\n","[98]\tvalid_0's l2: 0.0406509\n","[99]\tvalid_0's l2: 0.0399834\n","[100]\tvalid_0's l2: 0.0395812\n","[101]\tvalid_0's l2: 0.039532\n","[102]\tvalid_0's l2: 0.0394127\n","[103]\tvalid_0's l2: 0.0389651\n","[104]\tvalid_0's l2: 0.0387603\n","[105]\tvalid_0's l2: 0.0383749\n","[106]\tvalid_0's l2: 0.0380245\n","[107]\tvalid_0's l2: 0.0375163\n","[108]\tvalid_0's l2: 0.0372452\n","[109]\tvalid_0's l2: 0.0369101\n","[110]\tvalid_0's l2: 0.0366107\n","[111]\tvalid_0's l2: 0.0364151\n","[112]\tvalid_0's l2: 0.0359623\n","[113]\tvalid_0's l2: 0.0359049\n","[114]\tvalid_0's l2: 0.0358128\n","[115]\tvalid_0's l2: 0.0354383\n","[116]\tvalid_0's l2: 0.0353028\n","[117]\tvalid_0's l2: 0.0351676\n","[118]\tvalid_0's l2: 0.0347917\n","[119]\tvalid_0's l2: 0.0345072\n","[120]\tvalid_0's l2: 0.0344949\n","[121]\tvalid_0's l2: 0.0344972\n","[122]\tvalid_0's l2: 0.0343112\n","[123]\tvalid_0's l2: 0.0341809\n","[124]\tvalid_0's l2: 0.033926\n","[125]\tvalid_0's l2: 0.0338804\n","[126]\tvalid_0's l2: 0.0338564\n","[127]\tvalid_0's l2: 0.0337583\n","[128]\tvalid_0's l2: 0.033593\n","[129]\tvalid_0's l2: 0.033588\n","[130]\tvalid_0's l2: 0.0334962\n","[131]\tvalid_0's l2: 0.0336001\n","[132]\tvalid_0's l2: 0.0334265\n","[133]\tvalid_0's l2: 0.0332763\n","[134]\tvalid_0's l2: 0.0332378\n","[135]\tvalid_0's l2: 0.0329068\n","[136]\tvalid_0's l2: 0.0326551\n","[137]\tvalid_0's l2: 0.0326074\n","[138]\tvalid_0's l2: 0.0325967\n","[139]\tvalid_0's l2: 0.0323801\n","[140]\tvalid_0's l2: 0.0322834\n","[141]\tvalid_0's l2: 0.032186\n","[142]\tvalid_0's l2: 0.0319851\n","[143]\tvalid_0's l2: 0.0318899\n","[144]\tvalid_0's l2: 0.0317807\n","[145]\tvalid_0's l2: 0.0314412\n","[146]\tvalid_0's l2: 0.0313261\n","[147]\tvalid_0's l2: 0.0312241\n","[148]\tvalid_0's l2: 0.0310403\n","[149]\tvalid_0's l2: 0.031033\n","[150]\tvalid_0's l2: 0.0307246\n","[151]\tvalid_0's l2: 0.030609\n","[152]\tvalid_0's l2: 0.0304768\n","[153]\tvalid_0's l2: 0.030385\n","[154]\tvalid_0's l2: 0.0302836\n","[155]\tvalid_0's l2: 0.0302265\n","[156]\tvalid_0's l2: 0.0300021\n","[157]\tvalid_0's l2: 0.0297406\n","[158]\tvalid_0's l2: 0.0296474\n","[159]\tvalid_0's l2: 0.0295892\n","[160]\tvalid_0's l2: 0.0294475\n","[161]\tvalid_0's l2: 0.0292466\n","[162]\tvalid_0's l2: 0.0290367\n","[163]\tvalid_0's l2: 0.0290221\n","[164]\tvalid_0's l2: 0.0289195\n","[165]\tvalid_0's l2: 0.0287607\n","[166]\tvalid_0's l2: 0.0287515\n","[167]\tvalid_0's l2: 0.0287035\n","[168]\tvalid_0's l2: 0.0286956\n","[169]\tvalid_0's l2: 0.0286398\n","[170]\tvalid_0's l2: 0.028449\n","[171]\tvalid_0's l2: 0.0284683\n","[172]\tvalid_0's l2: 0.0284312\n","[173]\tvalid_0's l2: 0.0283465\n","[174]\tvalid_0's l2: 0.0282704\n","[175]\tvalid_0's l2: 0.0282283\n","[176]\tvalid_0's l2: 0.0282263\n","[177]\tvalid_0's l2: 0.0281487\n","[178]\tvalid_0's l2: 0.0279696\n","[179]\tvalid_0's l2: 0.0278947\n","[180]\tvalid_0's l2: 0.0278895\n","[181]\tvalid_0's l2: 0.0279438\n","[182]\tvalid_0's l2: 0.0278488\n","[183]\tvalid_0's l2: 0.0277933\n","[184]\tvalid_0's l2: 0.0277079\n","[185]\tvalid_0's l2: 0.0276787\n","[186]\tvalid_0's l2: 0.0275932\n","[187]\tvalid_0's l2: 0.0275472\n","[188]\tvalid_0's l2: 0.0275055\n","[189]\tvalid_0's l2: 0.0274482\n","[190]\tvalid_0's l2: 0.0274092\n","[191]\tvalid_0's l2: 0.0273464\n","[192]\tvalid_0's l2: 0.027286\n","[193]\tvalid_0's l2: 0.0272677\n","[194]\tvalid_0's l2: 0.0271417\n","[195]\tvalid_0's l2: 0.0269849\n","[196]\tvalid_0's l2: 0.0269469\n","[197]\tvalid_0's l2: 0.0269227\n","[198]\tvalid_0's l2: 0.0268464\n","[199]\tvalid_0's l2: 0.02676\n","[200]\tvalid_0's l2: 0.0266833\n","[201]\tvalid_0's l2: 0.0266479\n","[202]\tvalid_0's l2: 0.0265802\n","[203]\tvalid_0's l2: 0.0265459\n","[204]\tvalid_0's l2: 0.0265366\n","[205]\tvalid_0's l2: 0.0264094\n","[206]\tvalid_0's l2: 0.0262834\n","[207]\tvalid_0's l2: 0.0261057\n","[208]\tvalid_0's l2: 0.0260438\n","[209]\tvalid_0's l2: 0.0260009\n","[210]\tvalid_0's l2: 0.025842\n","[211]\tvalid_0's l2: 0.0257386\n","[212]\tvalid_0's l2: 0.0256511\n","[213]\tvalid_0's l2: 0.0256235\n","[214]\tvalid_0's l2: 0.0254708\n","[215]\tvalid_0's l2: 0.0254714\n","[216]\tvalid_0's l2: 0.0254023\n","[217]\tvalid_0's l2: 0.025327\n","[218]\tvalid_0's l2: 0.0253027\n","[219]\tvalid_0's l2: 0.0252206\n","[220]\tvalid_0's l2: 0.025224\n","[221]\tvalid_0's l2: 0.0251406\n","[222]\tvalid_0's l2: 0.0251156\n","[223]\tvalid_0's l2: 0.0250091\n","[224]\tvalid_0's l2: 0.0248646\n","[225]\tvalid_0's l2: 0.0247915\n","[226]\tvalid_0's l2: 0.0247893\n","[227]\tvalid_0's l2: 0.0246686\n","[228]\tvalid_0's l2: 0.0246472\n","[229]\tvalid_0's l2: 0.0245413\n","[230]\tvalid_0's l2: 0.0244501\n","[231]\tvalid_0's l2: 0.0244112\n","[232]\tvalid_0's l2: 0.0243861\n","[233]\tvalid_0's l2: 0.02434\n","[234]\tvalid_0's l2: 0.0242557\n","[235]\tvalid_0's l2: 0.0242195\n","[236]\tvalid_0's l2: 0.0241878\n","[237]\tvalid_0's l2: 0.0241843\n","[238]\tvalid_0's l2: 0.0241515\n","[239]\tvalid_0's l2: 0.0241869\n","[240]\tvalid_0's l2: 0.0240869\n","[241]\tvalid_0's l2: 0.0240598\n","[242]\tvalid_0's l2: 0.0240605\n","[243]\tvalid_0's l2: 0.0239868\n","[244]\tvalid_0's l2: 0.0239514\n","[245]\tvalid_0's l2: 0.0238974\n","[246]\tvalid_0's l2: 0.0238949\n","[247]\tvalid_0's l2: 0.023872\n","[248]\tvalid_0's l2: 0.0238719\n","[249]\tvalid_0's l2: 0.0237718\n","[250]\tvalid_0's l2: 0.0237409\n","[251]\tvalid_0's l2: 0.0237141\n","[252]\tvalid_0's l2: 0.0236544\n","[253]\tvalid_0's l2: 0.0236023\n","[254]\tvalid_0's l2: 0.0235619\n","[255]\tvalid_0's l2: 0.0234924\n","[256]\tvalid_0's l2: 0.023408\n","[257]\tvalid_0's l2: 0.0233913\n","[258]\tvalid_0's l2: 0.0233456\n","[259]\tvalid_0's l2: 0.0233112\n","[260]\tvalid_0's l2: 0.0232663\n","[261]\tvalid_0's l2: 0.0232658\n","[262]\tvalid_0's l2: 0.0232383\n","[263]\tvalid_0's l2: 0.0232202\n","[264]\tvalid_0's l2: 0.0231864\n","[265]\tvalid_0's l2: 0.0231871\n","[266]\tvalid_0's l2: 0.0231646\n","[267]\tvalid_0's l2: 0.0231611\n","[268]\tvalid_0's l2: 0.023115\n","[269]\tvalid_0's l2: 0.0230953\n","[270]\tvalid_0's l2: 0.0229928\n","[271]\tvalid_0's l2: 0.0229897\n","[272]\tvalid_0's l2: 0.0229063\n","[273]\tvalid_0's l2: 0.0228882\n","[274]\tvalid_0's l2: 0.0228732\n","[275]\tvalid_0's l2: 0.02286\n","[276]\tvalid_0's l2: 0.0228602\n","[277]\tvalid_0's l2: 0.022813\n","[278]\tvalid_0's l2: 0.0228065\n","[279]\tvalid_0's l2: 0.0229052\n","[280]\tvalid_0's l2: 0.0228513\n","[281]\tvalid_0's l2: 0.0227988\n","[282]\tvalid_0's l2: 0.0227563\n","[283]\tvalid_0's l2: 0.0226835\n","[284]\tvalid_0's l2: 0.0226592\n","[285]\tvalid_0's l2: 0.0226544\n","[286]\tvalid_0's l2: 0.0226475\n","[287]\tvalid_0's l2: 0.0226252\n","[288]\tvalid_0's l2: 0.0225739\n","[289]\tvalid_0's l2: 0.0225734\n","[290]\tvalid_0's l2: 0.022551\n","[291]\tvalid_0's l2: 0.0224878\n","[292]\tvalid_0's l2: 0.0224317\n","[293]\tvalid_0's l2: 0.0223995\n","[294]\tvalid_0's l2: 0.0223829\n","[295]\tvalid_0's l2: 0.0223703\n","[296]\tvalid_0's l2: 0.0223221\n","[297]\tvalid_0's l2: 0.0223135\n","[298]\tvalid_0's l2: 0.0222842\n","[299]\tvalid_0's l2: 0.0222326\n","[300]\tvalid_0's l2: 0.0222317\n","[301]\tvalid_0's l2: 0.0222367\n","[302]\tvalid_0's l2: 0.0221874\n","[303]\tvalid_0's l2: 0.0221368\n","[304]\tvalid_0's l2: 0.0221118\n","[305]\tvalid_0's l2: 0.0220756\n","[306]\tvalid_0's l2: 0.0220529\n","[307]\tvalid_0's l2: 0.0219962\n","[308]\tvalid_0's l2: 0.0219402\n","[309]\tvalid_0's l2: 0.0219113\n","[310]\tvalid_0's l2: 0.0218612\n","[311]\tvalid_0's l2: 0.0218372\n","[312]\tvalid_0's l2: 0.0218368\n","[313]\tvalid_0's l2: 0.021798\n","[314]\tvalid_0's l2: 0.0217441\n","[315]\tvalid_0's l2: 0.0217285\n","[316]\tvalid_0's l2: 0.0217293\n","[317]\tvalid_0's l2: 0.0219091\n","[318]\tvalid_0's l2: 0.0218837\n","[319]\tvalid_0's l2: 0.0218645\n","[320]\tvalid_0's l2: 0.0217964\n","[321]\tvalid_0's l2: 0.0217427\n","[322]\tvalid_0's l2: 0.0218492\n","[323]\tvalid_0's l2: 0.0218033\n","[324]\tvalid_0's l2: 0.0217797\n","[325]\tvalid_0's l2: 0.0217836\n","[326]\tvalid_0's l2: 0.0217665\n","[327]\tvalid_0's l2: 0.0217116\n","[328]\tvalid_0's l2: 0.0217518\n","[329]\tvalid_0's l2: 0.0217296\n","[330]\tvalid_0's l2: 0.0216909\n","[331]\tvalid_0's l2: 0.021694\n","[332]\tvalid_0's l2: 0.0216949\n","[333]\tvalid_0's l2: 0.0216935\n","[334]\tvalid_0's l2: 0.0216933\n","[335]\tvalid_0's l2: 0.0217145\n","[336]\tvalid_0's l2: 0.0217053\n","[337]\tvalid_0's l2: 0.0216474\n","[338]\tvalid_0's l2: 0.0216162\n","[339]\tvalid_0's l2: 0.0215875\n","[340]\tvalid_0's l2: 0.0215869\n","[341]\tvalid_0's l2: 0.0215578\n","[342]\tvalid_0's l2: 0.021507\n","[343]\tvalid_0's l2: 0.0214719\n","[344]\tvalid_0's l2: 0.0214607\n","[345]\tvalid_0's l2: 0.0214159\n","[346]\tvalid_0's l2: 0.0214027\n","[347]\tvalid_0's l2: 0.0213993\n","[348]\tvalid_0's l2: 0.0213599\n","[349]\tvalid_0's l2: 0.0213222\n","[350]\tvalid_0's l2: 0.0213098\n","[351]\tvalid_0's l2: 0.021287\n","[352]\tvalid_0's l2: 0.021252\n","[353]\tvalid_0's l2: 0.0212405\n","[354]\tvalid_0's l2: 0.0212008\n","[355]\tvalid_0's l2: 0.0211934\n","[356]\tvalid_0's l2: 0.0211451\n","[357]\tvalid_0's l2: 0.02116\n","[358]\tvalid_0's l2: 0.0211508\n","[359]\tvalid_0's l2: 0.0211282\n","[360]\tvalid_0's l2: 0.0210916\n","[361]\tvalid_0's l2: 0.0210545\n","[362]\tvalid_0's l2: 0.0210329\n","[363]\tvalid_0's l2: 0.0209997\n","[364]\tvalid_0's l2: 0.020998\n","[365]\tvalid_0's l2: 0.0209733\n","[366]\tvalid_0's l2: 0.0209555\n","[367]\tvalid_0's l2: 0.0208985\n","[368]\tvalid_0's l2: 0.0208717\n","[369]\tvalid_0's l2: 0.0208709\n","[370]\tvalid_0's l2: 0.0208388\n","[371]\tvalid_0's l2: 0.0208107\n","[372]\tvalid_0's l2: 0.0207806\n","[373]\tvalid_0's l2: 0.0207568\n","[374]\tvalid_0's l2: 0.0207581\n","[375]\tvalid_0's l2: 0.0207445\n","[376]\tvalid_0's l2: 0.0207268\n","[377]\tvalid_0's l2: 0.0206922\n","[378]\tvalid_0's l2: 0.0206521\n","[379]\tvalid_0's l2: 0.0206482\n","[380]\tvalid_0's l2: 0.0206457\n","[381]\tvalid_0's l2: 0.0206264\n","[382]\tvalid_0's l2: 0.0206091\n","[383]\tvalid_0's l2: 0.0205933\n","[384]\tvalid_0's l2: 0.020588\n","[385]\tvalid_0's l2: 0.0205872\n","[386]\tvalid_0's l2: 0.020551\n","[387]\tvalid_0's l2: 0.0205265\n","[388]\tvalid_0's l2: 0.0204802\n","[389]\tvalid_0's l2: 0.020473\n","[390]\tvalid_0's l2: 0.0204678\n","[391]\tvalid_0's l2: 0.0204359\n","[392]\tvalid_0's l2: 0.0204355\n","[393]\tvalid_0's l2: 0.0204193\n","[394]\tvalid_0's l2: 0.0203992\n","[395]\tvalid_0's l2: 0.0203993\n","[396]\tvalid_0's l2: 0.0203487\n","[397]\tvalid_0's l2: 0.020327\n","[398]\tvalid_0's l2: 0.0202888\n","[399]\tvalid_0's l2: 0.0202285\n","[400]\tvalid_0's l2: 0.020204\n","[401]\tvalid_0's l2: 0.0201863\n","[402]\tvalid_0's l2: 0.0201609\n","[403]\tvalid_0's l2: 0.0201244\n","[404]\tvalid_0's l2: 0.0200998\n","[405]\tvalid_0's l2: 0.0200665\n","[406]\tvalid_0's l2: 0.0200328\n","[407]\tvalid_0's l2: 0.0200298\n","[408]\tvalid_0's l2: 0.0200293\n","[409]\tvalid_0's l2: 0.0200116\n","[410]\tvalid_0's l2: 0.0199556\n","[411]\tvalid_0's l2: 0.0199358\n","[412]\tvalid_0's l2: 0.0198784\n","[413]\tvalid_0's l2: 0.0198666\n","[414]\tvalid_0's l2: 0.0198345\n","[415]\tvalid_0's l2: 0.0198102\n","[416]\tvalid_0's l2: 0.0198101\n","[417]\tvalid_0's l2: 0.0197564\n","[418]\tvalid_0's l2: 0.0197559\n","[419]\tvalid_0's l2: 0.0197487\n","[420]\tvalid_0's l2: 0.0197187\n","[421]\tvalid_0's l2: 0.0197157\n","[422]\tvalid_0's l2: 0.0197085\n","[423]\tvalid_0's l2: 0.0196943\n","[424]\tvalid_0's l2: 0.0196768\n","[425]\tvalid_0's l2: 0.0196761\n","[426]\tvalid_0's l2: 0.019676\n","[427]\tvalid_0's l2: 0.0196526\n","[428]\tvalid_0's l2: 0.0196462\n","[429]\tvalid_0's l2: 0.0196469\n","[430]\tvalid_0's l2: 0.0196473\n","[431]\tvalid_0's l2: 0.0196448\n","[432]\tvalid_0's l2: 0.0196454\n","[433]\tvalid_0's l2: 0.0196208\n","[434]\tvalid_0's l2: 0.0195932\n","[435]\tvalid_0's l2: 0.0195932\n","[436]\tvalid_0's l2: 0.0195544\n","[437]\tvalid_0's l2: 0.0195345\n","[438]\tvalid_0's l2: 0.0195243\n","[439]\tvalid_0's l2: 0.0195206\n","[440]\tvalid_0's l2: 0.0195058\n","[441]\tvalid_0's l2: 0.0194812\n","[442]\tvalid_0's l2: 0.0194785\n","[443]\tvalid_0's l2: 0.019478\n","[444]\tvalid_0's l2: 0.0194276\n","[445]\tvalid_0's l2: 0.0193827\n","[446]\tvalid_0's l2: 0.0193771\n","[447]\tvalid_0's l2: 0.019359\n","[448]\tvalid_0's l2: 0.0193554\n","[449]\tvalid_0's l2: 0.019355\n","[450]\tvalid_0's l2: 0.0193465\n","[451]\tvalid_0's l2: 0.0193433\n","[452]\tvalid_0's l2: 0.0193245\n","[453]\tvalid_0's l2: 0.019315\n","[454]\tvalid_0's l2: 0.0193149\n","[455]\tvalid_0's l2: 0.0193143\n","[456]\tvalid_0's l2: 0.0192779\n","[457]\tvalid_0's l2: 0.0192766\n","[458]\tvalid_0's l2: 0.0192723\n","[459]\tvalid_0's l2: 0.0192465\n","[460]\tvalid_0's l2: 0.0192433\n","[461]\tvalid_0's l2: 0.0192439\n","[462]\tvalid_0's l2: 0.0192343\n","[463]\tvalid_0's l2: 0.0192121\n","[464]\tvalid_0's l2: 0.0192119\n","[465]\tvalid_0's l2: 0.0191873\n","[466]\tvalid_0's l2: 0.0191884\n","[467]\tvalid_0's l2: 0.0191855\n","[468]\tvalid_0's l2: 0.0190898\n","[469]\tvalid_0's l2: 0.0190846\n","[470]\tvalid_0's l2: 0.0190683\n","[471]\tvalid_0's l2: 0.0190684\n","[472]\tvalid_0's l2: 0.0190499\n","[473]\tvalid_0's l2: 0.0190496\n","[474]\tvalid_0's l2: 0.0190486\n","[475]\tvalid_0's l2: 0.019027\n","[476]\tvalid_0's l2: 0.0189688\n","[477]\tvalid_0's l2: 0.0189462\n","[478]\tvalid_0's l2: 0.0190423\n","[479]\tvalid_0's l2: 0.0190419\n","[480]\tvalid_0's l2: 0.0190423\n","[481]\tvalid_0's l2: 0.0190325\n","[482]\tvalid_0's l2: 0.0190044\n","[483]\tvalid_0's l2: 0.019005\n","[484]\tvalid_0's l2: 0.0190068\n","[485]\tvalid_0's l2: 0.0190032\n","[486]\tvalid_0's l2: 0.0190005\n","[487]\tvalid_0's l2: 0.0189974\n","[488]\tvalid_0's l2: 0.0189691\n","[489]\tvalid_0's l2: 0.0189664\n","[490]\tvalid_0's l2: 0.0189017\n","[491]\tvalid_0's l2: 0.0188913\n","[492]\tvalid_0's l2: 0.0188835\n","[493]\tvalid_0's l2: 0.0188713\n","[494]\tvalid_0's l2: 0.0188496\n","[495]\tvalid_0's l2: 0.0188372\n","[496]\tvalid_0's l2: 0.0188234\n","[497]\tvalid_0's l2: 0.0188224\n","[498]\tvalid_0's l2: 0.0188098\n","[499]\tvalid_0's l2: 0.0188216\n","[500]\tvalid_0's l2: 0.018797\n","[501]\tvalid_0's l2: 0.0187684\n","[502]\tvalid_0's l2: 0.0187703\n","[503]\tvalid_0's l2: 0.0187619\n","[504]\tvalid_0's l2: 0.0187491\n","[505]\tvalid_0's l2: 0.0187287\n","[506]\tvalid_0's l2: 0.0187233\n","[507]\tvalid_0's l2: 0.0186932\n","[508]\tvalid_0's l2: 0.0186799\n","[509]\tvalid_0's l2: 0.0186785\n","[510]\tvalid_0's l2: 0.0186598\n","[511]\tvalid_0's l2: 0.0186365\n","[512]\tvalid_0's l2: 0.0186202\n","[513]\tvalid_0's l2: 0.0186199\n","[514]\tvalid_0's l2: 0.0186046\n","[515]\tvalid_0's l2: 0.0186228\n","[516]\tvalid_0's l2: 0.0186137\n","[517]\tvalid_0's l2: 0.018592\n","[518]\tvalid_0's l2: 0.0185656\n","[519]\tvalid_0's l2: 0.018541\n","[520]\tvalid_0's l2: 0.0185381\n","[521]\tvalid_0's l2: 0.018524\n","[522]\tvalid_0's l2: 0.0185315\n","[523]\tvalid_0's l2: 0.0185323\n","[524]\tvalid_0's l2: 0.0185111\n","[525]\tvalid_0's l2: 0.0185031\n","[526]\tvalid_0's l2: 0.0184754\n","[527]\tvalid_0's l2: 0.0184743\n","[528]\tvalid_0's l2: 0.0184637\n","[529]\tvalid_0's l2: 0.0184467\n","[530]\tvalid_0's l2: 0.0184482\n","[531]\tvalid_0's l2: 0.0184479\n","[532]\tvalid_0's l2: 0.0183935\n","[533]\tvalid_0's l2: 0.0183883\n","[534]\tvalid_0's l2: 0.0183832\n","[535]\tvalid_0's l2: 0.0183605\n","[536]\tvalid_0's l2: 0.018361\n","[537]\tvalid_0's l2: 0.0183609\n","[538]\tvalid_0's l2: 0.0183464\n","[539]\tvalid_0's l2: 0.0183464\n","[540]\tvalid_0's l2: 0.0183166\n","[541]\tvalid_0's l2: 0.0183168\n","[542]\tvalid_0's l2: 0.0183163\n","[543]\tvalid_0's l2: 0.018298\n","[544]\tvalid_0's l2: 0.0182911\n","[545]\tvalid_0's l2: 0.0182914\n","[546]\tvalid_0's l2: 0.0182956\n","[547]\tvalid_0's l2: 0.0182796\n","[548]\tvalid_0's l2: 0.0182609\n","[549]\tvalid_0's l2: 0.0182616\n","[550]\tvalid_0's l2: 0.0182412\n","[551]\tvalid_0's l2: 0.0182373\n","[552]\tvalid_0's l2: 0.0182291\n","[553]\tvalid_0's l2: 0.0182195\n","[554]\tvalid_0's l2: 0.0182216\n","[555]\tvalid_0's l2: 0.0182221\n","[556]\tvalid_0's l2: 0.0182233\n","[557]\tvalid_0's l2: 0.0182206\n","[558]\tvalid_0's l2: 0.0182095\n","[559]\tvalid_0's l2: 0.0182064\n","[560]\tvalid_0's l2: 0.0182064\n","[561]\tvalid_0's l2: 0.0182059\n","[562]\tvalid_0's l2: 0.0181963\n","[563]\tvalid_0's l2: 0.0181962\n","[564]\tvalid_0's l2: 0.0181947\n","[565]\tvalid_0's l2: 0.018194\n","[566]\tvalid_0's l2: 0.0181932\n","[567]\tvalid_0's l2: 0.0181757\n","[568]\tvalid_0's l2: 0.018176\n","[569]\tvalid_0's l2: 0.0181606\n","[570]\tvalid_0's l2: 0.0181367\n","[571]\tvalid_0's l2: 0.018123\n","[572]\tvalid_0's l2: 0.0181099\n","[573]\tvalid_0's l2: 0.0181054\n","[574]\tvalid_0's l2: 0.0180959\n","[575]\tvalid_0's l2: 0.0181397\n","[576]\tvalid_0's l2: 0.0181359\n","[577]\tvalid_0's l2: 0.0181302\n","[578]\tvalid_0's l2: 0.0181299\n","[579]\tvalid_0's l2: 0.0181141\n","[580]\tvalid_0's l2: 0.0180883\n","[581]\tvalid_0's l2: 0.0180698\n","[582]\tvalid_0's l2: 0.0180635\n","[583]\tvalid_0's l2: 0.0180631\n","[584]\tvalid_0's l2: 0.0180637\n","[585]\tvalid_0's l2: 0.0180558\n","[586]\tvalid_0's l2: 0.0180613\n","[587]\tvalid_0's l2: 0.0180498\n","[588]\tvalid_0's l2: 0.018047\n","[589]\tvalid_0's l2: 0.0180271\n","[590]\tvalid_0's l2: 0.018005\n","[591]\tvalid_0's l2: 0.0179849\n","[592]\tvalid_0's l2: 0.0179752\n","[593]\tvalid_0's l2: 0.0178995\n","[594]\tvalid_0's l2: 0.0178929\n","[595]\tvalid_0's l2: 0.0178921\n","[596]\tvalid_0's l2: 0.0178418\n","[597]\tvalid_0's l2: 0.0178358\n","[598]\tvalid_0's l2: 0.0178296\n","[599]\tvalid_0's l2: 0.0178295\n","[600]\tvalid_0's l2: 0.0178275\n","[601]\tvalid_0's l2: 0.0178226\n","[602]\tvalid_0's l2: 0.0178227\n","[603]\tvalid_0's l2: 0.0178159\n","[604]\tvalid_0's l2: 0.0178001\n","[605]\tvalid_0's l2: 0.017794\n","[606]\tvalid_0's l2: 0.0177389\n","[607]\tvalid_0's l2: 0.0177379\n","[608]\tvalid_0's l2: 0.0177115\n","[609]\tvalid_0's l2: 0.0177003\n","[610]\tvalid_0's l2: 0.0176991\n","[611]\tvalid_0's l2: 0.0176995\n","[612]\tvalid_0's l2: 0.0176969\n","[613]\tvalid_0's l2: 0.0176968\n","[614]\tvalid_0's l2: 0.0176931\n","[615]\tvalid_0's l2: 0.0176895\n","[616]\tvalid_0's l2: 0.017666\n","[617]\tvalid_0's l2: 0.0176479\n","[618]\tvalid_0's l2: 0.0176322\n","[619]\tvalid_0's l2: 0.0176067\n","[620]\tvalid_0's l2: 0.0176024\n","[621]\tvalid_0's l2: 0.0175697\n","[622]\tvalid_0's l2: 0.017652\n","[623]\tvalid_0's l2: 0.0176454\n","[624]\tvalid_0's l2: 0.0176297\n","[625]\tvalid_0's l2: 0.0176285\n","[626]\tvalid_0's l2: 0.0176088\n","[627]\tvalid_0's l2: 0.0176023\n","[628]\tvalid_0's l2: 0.0175865\n","[629]\tvalid_0's l2: 0.0175749\n","[630]\tvalid_0's l2: 0.0175586\n","[631]\tvalid_0's l2: 0.0175529\n","[632]\tvalid_0's l2: 0.0175286\n","[633]\tvalid_0's l2: 0.0175114\n","[634]\tvalid_0's l2: 0.0175112\n","[635]\tvalid_0's l2: 0.0175047\n","[636]\tvalid_0's l2: 0.0174807\n","[637]\tvalid_0's l2: 0.0174725\n","[638]\tvalid_0's l2: 0.0174754\n","[639]\tvalid_0's l2: 0.0174695\n","[640]\tvalid_0's l2: 0.0174675\n","[641]\tvalid_0's l2: 0.0174549\n","[642]\tvalid_0's l2: 0.0174462\n","[643]\tvalid_0's l2: 0.0174384\n","[644]\tvalid_0's l2: 0.0174338\n","[645]\tvalid_0's l2: 0.0174223\n","[646]\tvalid_0's l2: 0.0174003\n","[647]\tvalid_0's l2: 0.0174009\n","[648]\tvalid_0's l2: 0.0174041\n","[649]\tvalid_0's l2: 0.017392\n","[650]\tvalid_0's l2: 0.0173825\n","[651]\tvalid_0's l2: 0.0173475\n","[652]\tvalid_0's l2: 0.0173282\n","[653]\tvalid_0's l2: 0.0173163\n","[654]\tvalid_0's l2: 0.0173164\n","[655]\tvalid_0's l2: 0.0173017\n","[656]\tvalid_0's l2: 0.0172892\n","[657]\tvalid_0's l2: 0.0172803\n","[658]\tvalid_0's l2: 0.0172344\n","[659]\tvalid_0's l2: 0.0172337\n","[660]\tvalid_0's l2: 0.0172423\n","[661]\tvalid_0's l2: 0.0172333\n","[662]\tvalid_0's l2: 0.017228\n","[663]\tvalid_0's l2: 0.0172151\n","[664]\tvalid_0's l2: 0.0172151\n","[665]\tvalid_0's l2: 0.0172143\n","[666]\tvalid_0's l2: 0.0172107\n","[667]\tvalid_0's l2: 0.0171665\n","[668]\tvalid_0's l2: 0.0171668\n","[669]\tvalid_0's l2: 0.0171817\n","[670]\tvalid_0's l2: 0.0171721\n","[671]\tvalid_0's l2: 0.0171682\n","[672]\tvalid_0's l2: 0.0171684\n","[673]\tvalid_0's l2: 0.0171646\n","[674]\tvalid_0's l2: 0.0171645\n","[675]\tvalid_0's l2: 0.0171507\n","[676]\tvalid_0's l2: 0.0171427\n","[677]\tvalid_0's l2: 0.0171429\n","[678]\tvalid_0's l2: 0.017142\n","[679]\tvalid_0's l2: 0.0171225\n","[680]\tvalid_0's l2: 0.0171236\n","[681]\tvalid_0's l2: 0.0171242\n","[682]\tvalid_0's l2: 0.0171077\n","[683]\tvalid_0's l2: 0.0171054\n","[684]\tvalid_0's l2: 0.0170962\n","[685]\tvalid_0's l2: 0.0170959\n","[686]\tvalid_0's l2: 0.0170828\n","[687]\tvalid_0's l2: 0.0170763\n","[688]\tvalid_0's l2: 0.0170765\n","[689]\tvalid_0's l2: 0.0170605\n","[690]\tvalid_0's l2: 0.0170602\n","[691]\tvalid_0's l2: 0.0170602\n","[692]\tvalid_0's l2: 0.0170635\n","[693]\tvalid_0's l2: 0.0170686\n","[694]\tvalid_0's l2: 0.0170678\n","[695]\tvalid_0's l2: 0.0170621\n","[696]\tvalid_0's l2: 0.0170548\n","[697]\tvalid_0's l2: 0.0170488\n","[698]\tvalid_0's l2: 0.0170469\n","[699]\tvalid_0's l2: 0.0170464\n","[700]\tvalid_0's l2: 0.0170321\n","[701]\tvalid_0's l2: 0.0170319\n","[702]\tvalid_0's l2: 0.0170143\n","[703]\tvalid_0's l2: 0.0170076\n","[704]\tvalid_0's l2: 0.017002\n","[705]\tvalid_0's l2: 0.0170008\n","[706]\tvalid_0's l2: 0.0169711\n","[707]\tvalid_0's l2: 0.0169672\n","[708]\tvalid_0's l2: 0.0169675\n","[709]\tvalid_0's l2: 0.0169638\n","[710]\tvalid_0's l2: 0.0169646\n","[711]\tvalid_0's l2: 0.0169641\n","[712]\tvalid_0's l2: 0.0169631\n","[713]\tvalid_0's l2: 0.0169625\n","[714]\tvalid_0's l2: 0.0169627\n","[715]\tvalid_0's l2: 0.0169563\n","[716]\tvalid_0's l2: 0.0169542\n","[717]\tvalid_0's l2: 0.0169489\n","[718]\tvalid_0's l2: 0.0169467\n","[719]\tvalid_0's l2: 0.0169464\n","[720]\tvalid_0's l2: 0.0169436\n","[721]\tvalid_0's l2: 0.0169334\n","[722]\tvalid_0's l2: 0.0169373\n","[723]\tvalid_0's l2: 0.0169294\n","[724]\tvalid_0's l2: 0.0169296\n","[725]\tvalid_0's l2: 0.0169292\n","[726]\tvalid_0's l2: 0.0169288\n","[727]\tvalid_0's l2: 0.0169302\n","[728]\tvalid_0's l2: 0.016916\n","[729]\tvalid_0's l2: 0.0169085\n","[730]\tvalid_0's l2: 0.0169064\n","[731]\tvalid_0's l2: 0.0169003\n","[732]\tvalid_0's l2: 0.0168978\n","[733]\tvalid_0's l2: 0.0168959\n","[734]\tvalid_0's l2: 0.0168963\n","[735]\tvalid_0's l2: 0.0168959\n","[736]\tvalid_0's l2: 0.0168862\n","[737]\tvalid_0's l2: 0.0168658\n","[738]\tvalid_0's l2: 0.0168592\n","[739]\tvalid_0's l2: 0.0168485\n","[740]\tvalid_0's l2: 0.0168412\n","[741]\tvalid_0's l2: 0.0168361\n","[742]\tvalid_0's l2: 0.0168357\n","[743]\tvalid_0's l2: 0.0168366\n","[744]\tvalid_0's l2: 0.0168244\n","[745]\tvalid_0's l2: 0.0168242\n","[746]\tvalid_0's l2: 0.0168151\n","[747]\tvalid_0's l2: 0.016798\n","[748]\tvalid_0's l2: 0.0167957\n","[749]\tvalid_0's l2: 0.0167955\n","[750]\tvalid_0's l2: 0.0167918\n","[751]\tvalid_0's l2: 0.0167836\n","[752]\tvalid_0's l2: 0.0167806\n","[753]\tvalid_0's l2: 0.0168086\n","[754]\tvalid_0's l2: 0.0168048\n","[755]\tvalid_0's l2: 0.0168044\n","[756]\tvalid_0's l2: 0.0168042\n","[757]\tvalid_0's l2: 0.0167997\n","[758]\tvalid_0's l2: 0.016787\n","[759]\tvalid_0's l2: 0.0167832\n","[760]\tvalid_0's l2: 0.0167918\n","[761]\tvalid_0's l2: 0.0167814\n","[762]\tvalid_0's l2: 0.0167817\n","[763]\tvalid_0's l2: 0.0167747\n","[764]\tvalid_0's l2: 0.0167748\n","[765]\tvalid_0's l2: 0.0167669\n","[766]\tvalid_0's l2: 0.0167553\n","[767]\tvalid_0's l2: 0.0167038\n","[768]\tvalid_0's l2: 0.0166872\n","[769]\tvalid_0's l2: 0.0166572\n","[770]\tvalid_0's l2: 0.0166496\n","[771]\tvalid_0's l2: 0.0166333\n","[772]\tvalid_0's l2: 0.0166331\n","[773]\tvalid_0's l2: 0.0166258\n","[774]\tvalid_0's l2: 0.0166154\n","[775]\tvalid_0's l2: 0.0166149\n","[776]\tvalid_0's l2: 0.0166149\n","[777]\tvalid_0's l2: 0.01661\n","[778]\tvalid_0's l2: 0.0166024\n","[779]\tvalid_0's l2: 0.0165912\n","[780]\tvalid_0's l2: 0.0165786\n","[781]\tvalid_0's l2: 0.0165419\n","[782]\tvalid_0's l2: 0.0165399\n","[783]\tvalid_0's l2: 0.0165423\n","[784]\tvalid_0's l2: 0.0165422\n","[785]\tvalid_0's l2: 0.0165344\n","[786]\tvalid_0's l2: 0.0165258\n","[787]\tvalid_0's l2: 0.0165251\n","[788]\tvalid_0's l2: 0.0165257\n","[789]\tvalid_0's l2: 0.0165256\n","[790]\tvalid_0's l2: 0.0165255\n","[791]\tvalid_0's l2: 0.0165168\n","[792]\tvalid_0's l2: 0.0165071\n","[793]\tvalid_0's l2: 0.0165\n","[794]\tvalid_0's l2: 0.0164952\n","[795]\tvalid_0's l2: 0.0164953\n","[796]\tvalid_0's l2: 0.0164887\n","[797]\tvalid_0's l2: 0.0164752\n","[798]\tvalid_0's l2: 0.0164693\n","[799]\tvalid_0's l2: 0.0164721\n","[800]\tvalid_0's l2: 0.0164685\n","[801]\tvalid_0's l2: 0.0164683\n","[802]\tvalid_0's l2: 0.0164644\n","[803]\tvalid_0's l2: 0.0164575\n","[804]\tvalid_0's l2: 0.0164485\n","[805]\tvalid_0's l2: 0.0164376\n","[806]\tvalid_0's l2: 0.0164381\n","[807]\tvalid_0's l2: 0.0164138\n","[808]\tvalid_0's l2: 0.0164135\n","[809]\tvalid_0's l2: 0.0163968\n","[810]\tvalid_0's l2: 0.0163925\n","[811]\tvalid_0's l2: 0.0163925\n","[812]\tvalid_0's l2: 0.0163901\n","[813]\tvalid_0's l2: 0.0163907\n","[814]\tvalid_0's l2: 0.0163841\n","[815]\tvalid_0's l2: 0.0163796\n","[816]\tvalid_0's l2: 0.0163782\n","[817]\tvalid_0's l2: 0.0163744\n","[818]\tvalid_0's l2: 0.0163751\n","[819]\tvalid_0's l2: 0.0163718\n","[820]\tvalid_0's l2: 0.0163715\n","[821]\tvalid_0's l2: 0.0163147\n","[822]\tvalid_0's l2: 0.0163152\n","[823]\tvalid_0's l2: 0.0163081\n","[824]\tvalid_0's l2: 0.0163065\n","[825]\tvalid_0's l2: 0.0163022\n","[826]\tvalid_0's l2: 0.0162939\n","[827]\tvalid_0's l2: 0.0162933\n","[828]\tvalid_0's l2: 0.0162465\n","[829]\tvalid_0's l2: 0.0162464\n","[830]\tvalid_0's l2: 0.0162441\n","[831]\tvalid_0's l2: 0.0162272\n","[832]\tvalid_0's l2: 0.0162155\n","[833]\tvalid_0's l2: 0.0162157\n","[834]\tvalid_0's l2: 0.0162064\n","[835]\tvalid_0's l2: 0.0162012\n","[836]\tvalid_0's l2: 0.0161984\n","[837]\tvalid_0's l2: 0.0162025\n","[838]\tvalid_0's l2: 0.0162025\n","[839]\tvalid_0's l2: 0.0162027\n","[840]\tvalid_0's l2: 0.0162027\n","[841]\tvalid_0's l2: 0.0162016\n","[842]\tvalid_0's l2: 0.0161972\n","[843]\tvalid_0's l2: 0.0161927\n","[844]\tvalid_0's l2: 0.0161785\n","[845]\tvalid_0's l2: 0.0161636\n","[846]\tvalid_0's l2: 0.0161575\n","[847]\tvalid_0's l2: 0.0161535\n","[848]\tvalid_0's l2: 0.0161527\n","[849]\tvalid_0's l2: 0.0161461\n","[850]\tvalid_0's l2: 0.016167\n","[851]\tvalid_0's l2: 0.0161545\n","[852]\tvalid_0's l2: 0.0161551\n","[853]\tvalid_0's l2: 0.0161549\n","[854]\tvalid_0's l2: 0.0161549\n","[855]\tvalid_0's l2: 0.0161543\n","[856]\tvalid_0's l2: 0.0161485\n","[857]\tvalid_0's l2: 0.0161407\n","[858]\tvalid_0's l2: 0.0161322\n","[859]\tvalid_0's l2: 0.0161322\n","[860]\tvalid_0's l2: 0.0161213\n","[861]\tvalid_0's l2: 0.0161212\n","[862]\tvalid_0's l2: 0.0161126\n","[863]\tvalid_0's l2: 0.0161125\n","[864]\tvalid_0's l2: 0.0161121\n","[865]\tvalid_0's l2: 0.0161091\n","[866]\tvalid_0's l2: 0.0161074\n","[867]\tvalid_0's l2: 0.0161057\n","[868]\tvalid_0's l2: 0.0160962\n","[869]\tvalid_0's l2: 0.0160981\n","[870]\tvalid_0's l2: 0.0160981\n","[871]\tvalid_0's l2: 0.0160977\n","[872]\tvalid_0's l2: 0.0160937\n","[873]\tvalid_0's l2: 0.0160927\n","[874]\tvalid_0's l2: 0.0160859\n","[875]\tvalid_0's l2: 0.0160849\n","[876]\tvalid_0's l2: 0.0160785\n","[877]\tvalid_0's l2: 0.0160775\n","[878]\tvalid_0's l2: 0.0160776\n","[879]\tvalid_0's l2: 0.0160768\n","[880]\tvalid_0's l2: 0.0160446\n","[881]\tvalid_0's l2: 0.016037\n","[882]\tvalid_0's l2: 0.0160332\n","[883]\tvalid_0's l2: 0.0160261\n","[884]\tvalid_0's l2: 0.0160216\n","[885]\tvalid_0's l2: 0.016022\n","[886]\tvalid_0's l2: 0.0160075\n","[887]\tvalid_0's l2: 0.0160041\n","[888]\tvalid_0's l2: 0.0160024\n","[889]\tvalid_0's l2: 0.0160026\n","[890]\tvalid_0's l2: 0.0160016\n","[891]\tvalid_0's l2: 0.0159986\n","[892]\tvalid_0's l2: 0.0160013\n","[893]\tvalid_0's l2: 0.0159394\n","[894]\tvalid_0's l2: 0.0159374\n","[895]\tvalid_0's l2: 0.0159369\n","[896]\tvalid_0's l2: 0.0159108\n","[897]\tvalid_0's l2: 0.0159064\n","[898]\tvalid_0's l2: 0.0159005\n","[899]\tvalid_0's l2: 0.0159009\n","[900]\tvalid_0's l2: 0.0159005\n","[901]\tvalid_0's l2: 0.0158902\n","[902]\tvalid_0's l2: 0.0158879\n","[903]\tvalid_0's l2: 0.0158828\n","[904]\tvalid_0's l2: 0.0158831\n","[905]\tvalid_0's l2: 0.0158788\n","[906]\tvalid_0's l2: 0.0158767\n","[907]\tvalid_0's l2: 0.0158771\n","[908]\tvalid_0's l2: 0.0158737\n","[909]\tvalid_0's l2: 0.0158741\n","[910]\tvalid_0's l2: 0.0158666\n","[911]\tvalid_0's l2: 0.0158663\n","[912]\tvalid_0's l2: 0.0158663\n","[913]\tvalid_0's l2: 0.0158666\n","[914]\tvalid_0's l2: 0.0158641\n","[915]\tvalid_0's l2: 0.0158583\n","[916]\tvalid_0's l2: 0.0158583\n","[917]\tvalid_0's l2: 0.0158561\n","[918]\tvalid_0's l2: 0.0158556\n","[919]\tvalid_0's l2: 0.0158516\n","[920]\tvalid_0's l2: 0.0158524\n","[921]\tvalid_0's l2: 0.0158416\n","[922]\tvalid_0's l2: 0.0158398\n","[923]\tvalid_0's l2: 0.0158358\n","[924]\tvalid_0's l2: 0.0158352\n","[925]\tvalid_0's l2: 0.0158269\n","[926]\tvalid_0's l2: 0.0158265\n","[927]\tvalid_0's l2: 0.0158379\n","[928]\tvalid_0's l2: 0.0158587\n","[929]\tvalid_0's l2: 0.015858\n","[930]\tvalid_0's l2: 0.0158531\n","[931]\tvalid_0's l2: 0.015852\n","[932]\tvalid_0's l2: 0.0158491\n","[933]\tvalid_0's l2: 0.0158495\n","[934]\tvalid_0's l2: 0.0158364\n","[935]\tvalid_0's l2: 0.0158169\n","[936]\tvalid_0's l2: 0.0158148\n","[937]\tvalid_0's l2: 0.015815\n","[938]\tvalid_0's l2: 0.015808\n","[939]\tvalid_0's l2: 0.015807\n","[940]\tvalid_0's l2: 0.0157803\n","[941]\tvalid_0's l2: 0.015774\n","[942]\tvalid_0's l2: 0.0157717\n","[943]\tvalid_0's l2: 0.0157674\n","[944]\tvalid_0's l2: 0.0157641\n","[945]\tvalid_0's l2: 0.0157547\n","[946]\tvalid_0's l2: 0.0157544\n","[947]\tvalid_0's l2: 0.0157583\n","[948]\tvalid_0's l2: 0.015758\n","[949]\tvalid_0's l2: 0.0157584\n","[950]\tvalid_0's l2: 0.0157544\n","[951]\tvalid_0's l2: 0.0157425\n","[952]\tvalid_0's l2: 0.0157207\n","[953]\tvalid_0's l2: 0.0157169\n","[954]\tvalid_0's l2: 0.0157044\n","[955]\tvalid_0's l2: 0.0157037\n","[956]\tvalid_0's l2: 0.0156964\n","[957]\tvalid_0's l2: 0.0156926\n","[958]\tvalid_0's l2: 0.0156865\n","[959]\tvalid_0's l2: 0.0156787\n","[960]\tvalid_0's l2: 0.0156774\n","[961]\tvalid_0's l2: 0.0156388\n","[962]\tvalid_0's l2: 0.0156385\n","[963]\tvalid_0's l2: 0.0156315\n","[964]\tvalid_0's l2: 0.0156311\n","[965]\tvalid_0's l2: 0.0156141\n","[966]\tvalid_0's l2: 0.0156126\n","[967]\tvalid_0's l2: 0.01561\n","[968]\tvalid_0's l2: 0.0156099\n","[969]\tvalid_0's l2: 0.0156089\n","[970]\tvalid_0's l2: 0.0155926\n","[971]\tvalid_0's l2: 0.0155919\n","[972]\tvalid_0's l2: 0.0155886\n","[973]\tvalid_0's l2: 0.0155682\n","[974]\tvalid_0's l2: 0.0155678\n","[975]\tvalid_0's l2: 0.0155585\n","[976]\tvalid_0's l2: 0.015512\n","[977]\tvalid_0's l2: 0.0155096\n","[978]\tvalid_0's l2: 0.0155096\n","[979]\tvalid_0's l2: 0.0155097\n","[980]\tvalid_0's l2: 0.0155033\n","[981]\tvalid_0's l2: 0.0154978\n","[982]\tvalid_0's l2: 0.0154928\n","[983]\tvalid_0's l2: 0.0154928\n","[984]\tvalid_0's l2: 0.0154895\n","[985]\tvalid_0's l2: 0.0154902\n","[986]\tvalid_0's l2: 0.0154907\n","[987]\tvalid_0's l2: 0.0154844\n","[988]\tvalid_0's l2: 0.0154882\n","[989]\tvalid_0's l2: 0.0154879\n","[990]\tvalid_0's l2: 0.0154838\n","[991]\tvalid_0's l2: 0.0154841\n","[992]\tvalid_0's l2: 0.0154824\n","[993]\tvalid_0's l2: 0.0154808\n","[994]\tvalid_0's l2: 0.0154796\n","[995]\tvalid_0's l2: 0.01548\n","[996]\tvalid_0's l2: 0.0154757\n","[997]\tvalid_0's l2: 0.0154757\n","[998]\tvalid_0's l2: 0.0154685\n","[999]\tvalid_0's l2: 0.0154661\n","[1000]\tvalid_0's l2: 0.0154652\n","Elapsed time for fitting LightGBM model: 21.25 s\n","Early stopping performed. Best iteration: 1000\n","Elapsed time for fitting LightGBM model: 26.95 s\n","path: predictions/pointpredictions_lightgbm.csv\n","dirname: predictions\n","filename: pointpredictions_lightgbm.csv\n","artifact_path: predictions\n","path: /tmp/tmpvseqmbbr/pointpredictions_lightgbm.csv\n","tmp_path: /tmp/tmpvseqmbbr/pointpredictions_lightgbm.csv\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"e4bdf26cf59b40ddb18f8ff20915cf5a\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[1]\tvalid_0's l2: 0.160617\n[2]\tvalid_0's l2: 0.153625\n[3]\tvalid_0's l2: 0.14661\n[4]\tvalid_0's l2: 0.141007\n[5]\tvalid_0's l2: 0.135964\n[6]\tvalid_0's l2: 0.130492\n[7]\tvalid_0's l2: 0.127126\n[8]\tvalid_0's l2: 0.123155\n[9]\tvalid_0's l2: 0.119451\n[10]\tvalid_0's l2: 0.115901\n[11]\tvalid_0's l2: 0.114463\n[12]\tvalid_0's l2: 0.111633\n[13]\tvalid_0's l2: 0.108981\n[14]\tvalid_0's l2: 0.107544\n[15]\tvalid_0's l2: 0.105766\n[16]\tvalid_0's l2: 0.104299\n[17]\tvalid_0's l2: 0.102308\n[18]\tvalid_0's l2: 0.0991005\n[19]\tvalid_0's l2: 0.0977477\n[20]\tvalid_0's l2: 0.0955909\n[21]\tvalid_0's l2: 0.0926906\n[22]\tvalid_0's l2: 0.0918178\n[23]\tvalid_0's l2: 0.0909734\n[24]\tvalid_0's l2: 0.0886536\n[25]\tvalid_0's l2: 0.0873099\n[26]\tvalid_0's l2: 0.0863762\n[27]\tvalid_0's l2: 0.0858924\n[28]\tvalid_0's l2: 0.0836161\n[29]\tvalid_0's l2: 0.0815558\n[30]\tvalid_0's l2: 0.0810937\n[31]\tvalid_0's l2: 0.0805799\n[32]\tvalid_0's l2: 0.0785332\n[33]\tvalid_0's l2: 0.0779857\n[34]\tvalid_0's l2: 0.0763855\n[35]\tvalid_0's l2: 0.0742784\n[36]\tvalid_0's l2: 0.0736242\n[37]\tvalid_0's l2: 0.072675\n[38]\tvalid_0's l2: 0.072303\n[39]\tvalid_0's l2: 0.0709234\n[40]\tvalid_0's l2: 0.0705547\n[41]\tvalid_0's l2: 0.0701577\n[42]\tvalid_0's l2: 0.0698485\n[43]\tvalid_0's l2: 0.0695771\n[44]\tvalid_0's l2: 0.0680874\n[45]\tvalid_0's l2: 0.0679134\n[46]\tvalid_0's l2: 0.0675005\n[47]\tvalid_0's l2: 0.0665441\n[48]\tvalid_0's l2: 0.0662122\n[49]\tvalid_0's l2: 0.0651592\n[50]\tvalid_0's l2: 0.064559\n[51]\tvalid_0's l2: 0.0633161\n[52]\tvalid_0's l2: 0.0638053\n[53]\tvalid_0's l2: 0.0632173\n[54]\tvalid_0's l2: 0.0620738\n[55]\tvalid_0's l2: 0.0612039\n[56]\tvalid_0's l2: 0.061104\n[57]\tvalid_0's l2: 0.0601623\n[58]\tvalid_0's l2: 0.0598367\n[59]\tvalid_0's l2: 0.0592888\n[60]\tvalid_0's l2: 0.0587344\n[61]\tvalid_0's l2: 0.0579575\n[62]\tvalid_0's l2: 0.0574032\n[63]\tvalid_0's l2: 0.0566676\n[64]\tvalid_0's l2: 0.0553941\n[65]\tvalid_0's l2: 0.0551577\n[66]\tvalid_0's l2: 0.0541875\n[67]\tvalid_0's l2: 0.0531076\n[68]\tvalid_0's l2: 0.0525344\n[69]\tvalid_0's l2: 0.0524199\n[70]\tvalid_0's l2: 0.0517432\n[71]\tvalid_0's l2: 0.0515691\n[72]\tvalid_0's l2: 0.050923\n[73]\tvalid_0's l2: 0.0503988\n[74]\tvalid_0's l2: 0.0502442\n[75]\tvalid_0's l2: 0.0497385\n[76]\tvalid_0's l2: 0.0493225\n[77]\tvalid_0's l2: 0.0483756\n[78]\tvalid_0's l2: 0.0483344\n[79]\tvalid_0's l2: 0.0479998\n[80]\tvalid_0's l2: 0.0479171\n[81]\tvalid_0's l2: 0.0472956\n[82]\tvalid_0's l2: 0.0467158\n[83]\tvalid_0's l2: 0.04602\n[84]\tvalid_0's l2: 0.0456554\n[85]\tvalid_0's l2: 0.0453842\n[86]\tvalid_0's l2: 0.0445794\n[87]\tvalid_0's l2: 0.0439826\n[88]\tvalid_0's l2: 0.0437708\n[89]\tvalid_0's l2: 0.0434588\n[90]\tvalid_0's l2: 0.0430457\n[91]\tvalid_0's l2: 0.0429442\n[92]\tvalid_0's l2: 0.0423459\n[93]\tvalid_0's l2: 0.0420101\n[94]\tvalid_0's l2: 0.0417991\n[95]\tvalid_0's l2: 0.041453\n[96]\tvalid_0's l2: 0.0411961\n[97]\tvalid_0's l2: 0.041045\n[98]\tvalid_0's l2: 0.0406509\n[99]\tvalid_0's l2: 0.0399834\n[100]\tvalid_0's l2: 0.0395812\n[101]\tvalid_0's l2: 0.039532\n[102]\tvalid_0's l2: 0.0394127\n[103]\tvalid_0's l2: 0.0389651\n[104]\tvalid_0's l2: 0.0387603\n[105]\tvalid_0's l2: 0.0383749\n[106]\tvalid_0's l2: 0.0380245\n[107]\tvalid_0's l2: 0.0375163\n[108]\tvalid_0's l2: 0.0372452\n[109]\tvalid_0's l2: 0.0369101\n[110]\tvalid_0's l2: 0.0366107\n[111]\tvalid_0's l2: 0.0364151\n[112]\tvalid_0's l2: 0.0359623\n[113]\tvalid_0's l2: 0.0359049\n[114]\tvalid_0's l2: 0.0358128\n[115]\tvalid_0's l2: 0.0354383\n[116]\tvalid_0's l2: 0.0353028\n[117]\tvalid_0's l2: 0.0351676\n[118]\tvalid_0's l2: 0.0347917\n[119]\tvalid_0's l2: 0.0345072\n[120]\tvalid_0's l2: 0.0344949\n[121]\tvalid_0's l2: 0.0344972\n[122]\tvalid_0's l2: 0.0343112\n[123]\tvalid_0's l2: 0.0341809\n[124]\tvalid_0's l2: 0.033926\n[125]\tvalid_0's l2: 0.0338804\n[126]\tvalid_0's l2: 0.0338564\n[127]\tvalid_0's l2: 0.0337583\n[128]\tvalid_0's l2: 0.033593\n[129]\tvalid_0's l2: 0.033588\n[130]\tvalid_0's l2: 0.0334962\n[131]\tvalid_0's l2: 0.0336001\n[132]\tvalid_0's l2: 0.0334265\n[133]\tvalid_0's l2: 0.0332763\n[134]\tvalid_0's l2: 0.0332378\n[135]\tvalid_0's l2: 0.0329068\n[136]\tvalid_0's l2: 0.0326551\n[137]\tvalid_0's l2: 0.0326074\n[138]\tvalid_0's l2: 0.0325967\n[139]\tvalid_0's l2: 0.0323801\n[140]\tvalid_0's l2: 0.0322834\n[141]\tvalid_0's l2: 0.032186\n[142]\tvalid_0's l2: 0.0319851\n[143]\tvalid_0's l2: 0.0318899\n[144]\tvalid_0's l2: 0.0317807\n[145]\tvalid_0's l2: 0.0314412\n[146]\tvalid_0's l2: 0.0313261\n[147]\tvalid_0's l2: 0.0312241\n[148]\tvalid_0's l2: 0.0310403\n[149]\tvalid_0's l2: 0.031033\n[150]\tvalid_0's l2: 0.0307246\n[151]\tvalid_0's l2: 0.030609\n[152]\tvalid_0's l2: 0.0304768\n[153]\tvalid_0's l2: 0.030385\n[154]\tvalid_0's l2: 0.0302836\n[155]\tvalid_0's l2: 0.0302265\n[156]\tvalid_0's l2: 0.0300021\n[157]\tvalid_0's l2: 0.0297406\n[158]\tvalid_0's l2: 0.0296474\n[159]\tvalid_0's l2: 0.0295892\n[160]\tvalid_0's l2: 0.0294475\n[161]\tvalid_0's l2: 0.0292466\n[162]\tvalid_0's l2: 0.0290367\n[163]\tvalid_0's l2: 0.0290221\n[164]\tvalid_0's l2: 0.0289195\n[165]\tvalid_0's l2: 0.0287607\n[166]\tvalid_0's l2: 0.0287515\n[167]\tvalid_0's l2: 0.0287035\n[168]\tvalid_0's l2: 0.0286956\n[169]\tvalid_0's l2: 0.0286398\n[170]\tvalid_0's l2: 0.028449\n[171]\tvalid_0's l2: 0.0284683\n[172]\tvalid_0's l2: 0.0284312\n[173]\tvalid_0's l2: 0.0283465\n[174]\tvalid_0's l2: 0.0282704\n[175]\tvalid_0's l2: 0.0282283\n[176]\tvalid_0's l2: 0.0282263\n[177]\tvalid_0's l2: 0.0281487\n[178]\tvalid_0's l2: 0.0279696\n[179]\tvalid_0's l2: 0.0278947\n[180]\tvalid_0's l2: 0.0278895\n[181]\tvalid_0's l2: 0.0279438\n[182]\tvalid_0's l2: 0.0278488\n[183]\tvalid_0's l2: 0.0277933\n[184]\tvalid_0's l2: 0.0277079\n[185]\tvalid_0's l2: 0.0276787\n[186]\tvalid_0's l2: 0.0275932\n[187]\tvalid_0's l2: 0.0275472\n[188]\tvalid_0's l2: 0.0275055\n[189]\tvalid_0's l2: 0.0274482\n[190]\tvalid_0's l2: 0.0274092\n[191]\tvalid_0's l2: 0.0273464\n[192]\tvalid_0's l2: 0.027286\n[193]\tvalid_0's l2: 0.0272677\n[194]\tvalid_0's l2: 0.0271417\n[195]\tvalid_0's l2: 0.0269849\n[196]\tvalid_0's l2: 0.0269469\n[197]\tvalid_0's l2: 0.0269227\n[198]\tvalid_0's l2: 0.0268464\n[199]\tvalid_0's l2: 0.02676\n[200]\tvalid_0's l2: 0.0266833\n[201]\tvalid_0's l2: 0.0266479\n[202]\tvalid_0's l2: 0.0265802\n[203]\tvalid_0's l2: 0.0265459\n[204]\tvalid_0's l2: 0.0265366\n[205]\tvalid_0's l2: 0.0264094\n[206]\tvalid_0's l2: 0.0262834\n[207]\tvalid_0's l2: 0.0261057\n[208]\tvalid_0's l2: 0.0260438\n[209]\tvalid_0's l2: 0.0260009\n[210]\tvalid_0's l2: 0.025842\n[211]\tvalid_0's l2: 0.0257386\n[212]\tvalid_0's l2: 0.0256511\n[213]\tvalid_0's l2: 0.0256235\n[214]\tvalid_0's l2: 0.0254708\n[215]\tvalid_0's l2: 0.0254714\n[216]\tvalid_0's l2: 0.0254023\n[217]\tvalid_0's l2: 0.025327\n[218]\tvalid_0's l2: 0.0253027\n[219]\tvalid_0's l2: 0.0252206\n[220]\tvalid_0's l2: 0.025224\n[221]\tvalid_0's l2: 0.0251406\n[222]\tvalid_0's l2: 0.0251156\n[223]\tvalid_0's l2: 0.0250091\n[224]\tvalid_0's l2: 0.0248646\n[225]\tvalid_0's l2: 0.0247915\n[226]\tvalid_0's l2: 0.0247893\n[227]\tvalid_0's l2: 0.0246686\n[228]\tvalid_0's l2: 0.0246472\n[229]\tvalid_0's l2: 0.0245413\n[230]\tvalid_0's l2: 0.0244501\n[231]\tvalid_0's l2: 0.0244112\n[232]\tvalid_0's l2: 0.0243861\n[233]\tvalid_0's l2: 0.02434\n[234]\tvalid_0's l2: 0.0242557\n[235]\tvalid_0's l2: 0.0242195\n[236]\tvalid_0's l2: 0.0241878\n[237]\tvalid_0's l2: 0.0241843\n[238]\tvalid_0's l2: 0.0241515\n[239]\tvalid_0's l2: 0.0241869\n[240]\tvalid_0's l2: 0.0240869\n[241]\tvalid_0's l2: 0.0240598\n[242]\tvalid_0's l2: 0.0240605\n[243]\tvalid_0's l2: 0.0239868\n[244]\tvalid_0's l2: 0.0239514\n[245]\tvalid_0's l2: 0.0238974\n[246]\tvalid_0's l2: 0.0238949\n[247]\tvalid_0's l2: 0.023872\n[248]\tvalid_0's l2: 0.0238719\n[249]\tvalid_0's l2: 0.0237718\n[250]\tvalid_0's l2: 0.0237409\n[251]\tvalid_0's l2: 0.0237141\n[252]\tvalid_0's l2: 0.0236544\n[253]\tvalid_0's l2: 0.0236023\n[254]\tvalid_0's l2: 0.0235619\n[255]\tvalid_0's l2: 0.0234924\n[256]\tvalid_0's l2: 0.023408\n[257]\tvalid_0's l2: 0.0233913\n[258]\tvalid_0's l2: 0.0233456\n[259]\tvalid_0's l2: 0.0233112\n[260]\tvalid_0's l2: 0.0232663\n[261]\tvalid_0's l2: 0.0232658\n[262]\tvalid_0's l2: 0.0232383\n[263]\tvalid_0's l2: 0.0232202\n[264]\tvalid_0's l2: 0.0231864\n[265]\tvalid_0's l2: 0.0231871\n[266]\tvalid_0's l2: 0.0231646\n[267]\tvalid_0's l2: 0.0231611\n[268]\tvalid_0's l2: 0.023115\n[269]\tvalid_0's l2: 0.0230953\n[270]\tvalid_0's l2: 0.0229928\n[271]\tvalid_0's l2: 0.0229897\n[272]\tvalid_0's l2: 0.0229063\n[273]\tvalid_0's l2: 0.0228882\n[274]\tvalid_0's l2: 0.0228732\n[275]\tvalid_0's l2: 0.02286\n[276]\tvalid_0's l2: 0.0228602\n[277]\tvalid_0's l2: 0.022813\n[278]\tvalid_0's l2: 0.0228065\n[279]\tvalid_0's l2: 0.0229052\n[280]\tvalid_0's l2: 0.0228513\n[281]\tvalid_0's l2: 0.0227988\n[282]\tvalid_0's l2: 0.0227563\n[283]\tvalid_0's l2: 0.0226835\n[284]\tvalid_0's l2: 0.0226592\n[285]\tvalid_0's l2: 0.0226544\n[286]\tvalid_0's l2: 0.0226475\n[287]\tvalid_0's l2: 0.0226252\n[288]\tvalid_0's l2: 0.0225739\n[289]\tvalid_0's l2: 0.0225734\n[290]\tvalid_0's l2: 0.022551\n[291]\tvalid_0's l2: 0.0224878\n[292]\tvalid_0's l2: 0.0224317\n[293]\tvalid_0's l2: 0.0223995\n[294]\tvalid_0's l2: 0.0223829\n[295]\tvalid_0's l2: 0.0223703\n[296]\tvalid_0's l2: 0.0223221\n[297]\tvalid_0's l2: 0.0223135\n[298]\tvalid_0's l2: 0.0222842\n[299]\tvalid_0's l2: 0.0222326\n[300]\tvalid_0's l2: 0.0222317\n[301]\tvalid_0's l2: 0.0222367\n[302]\tvalid_0's l2: 0.0221874\n[303]\tvalid_0's l2: 0.0221368\n[304]\tvalid_0's l2: 0.0221118\n[305]\tvalid_0's l2: 0.0220756\n[306]\tvalid_0's l2: 0.0220529\n[307]\tvalid_0's l2: 0.0219962\n[308]\tvalid_0's l2: 0.0219402\n[309]\tvalid_0's l2: 0.0219113\n[310]\tvalid_0's l2: 0.0218612\n[311]\tvalid_0's l2: 0.0218372\n[312]\tvalid_0's l2: 0.0218368\n[313]\tvalid_0's l2: 0.021798\n[314]\tvalid_0's l2: 0.0217441\n[315]\tvalid_0's l2: 0.0217285\n[316]\tvalid_0's l2: 0.0217293\n[317]\tvalid_0's l2: 0.0219091\n[318]\tvalid_0's l2: 0.0218837\n[319]\tvalid_0's l2: 0.0218645\n[320]\tvalid_0's l2: 0.0217964\n[321]\tvalid_0's l2: 0.0217427\n[322]\tvalid_0's l2: 0.0218492\n[323]\tvalid_0's l2: 0.0218033\n[324]\tvalid_0's l2: 0.0217797\n[325]\tvalid_0's l2: 0.0217836\n[326]\tvalid_0's l2: 0.0217665\n[327]\tvalid_0's l2: 0.0217116\n[328]\tvalid_0's l2: 0.0217518\n[329]\tvalid_0's l2: 0.0217296\n[330]\tvalid_0's l2: 0.0216909\n[331]\tvalid_0's l2: 0.021694\n[332]\tvalid_0's l2: 0.0216949\n[333]\tvalid_0's l2: 0.0216935\n[334]\tvalid_0's l2: 0.0216933\n[335]\tvalid_0's l2: 0.0217145\n[336]\tvalid_0's l2: 0.0217053\n[337]\tvalid_0's l2: 0.0216474\n[338]\tvalid_0's l2: 0.0216162\n[339]\tvalid_0's l2: 0.0215875\n[340]\tvalid_0's l2: 0.0215869\n[341]\tvalid_0's l2: 0.0215578\n[342]\tvalid_0's l2: 0.021507\n[343]\tvalid_0's l2: 0.0214719\n[344]\tvalid_0's l2: 0.0214607\n[345]\tvalid_0's l2: 0.0214159\n[346]\tvalid_0's l2: 0.0214027\n[347]\tvalid_0's l2: 0.0213993\n[348]\tvalid_0's l2: 0.0213599\n[349]\tvalid_0's l2: 0.0213222\n[350]\tvalid_0's l2: 0.0213098\n[351]\tvalid_0's l2: 0.021287\n[352]\tvalid_0's l2: 0.021252\n[353]\tvalid_0's l2: 0.0212405\n[354]\tvalid_0's l2: 0.0212008\n[355]\tvalid_0's l2: 0.0211934\n[356]\tvalid_0's l2: 0.0211451\n[357]\tvalid_0's l2: 0.02116\n[358]\tvalid_0's l2: 0.0211508\n[359]\tvalid_0's l2: 0.0211282\n[360]\tvalid_0's l2: 0.0210916\n[361]\tvalid_0's l2: 0.0210545\n[362]\tvalid_0's l2: 0.0210329\n[363]\tvalid_0's l2: 0.0209997\n[364]\tvalid_0's l2: 0.020998\n[365]\tvalid_0's l2: 0.0209733\n[366]\tvalid_0's l2: 0.0209555\n[367]\tvalid_0's l2: 0.0208985\n[368]\tvalid_0's l2: 0.0208717\n[369]\tvalid_0's l2: 0.0208709\n[370]\tvalid_0's l2: 0.0208388\n[371]\tvalid_0's l2: 0.0208107\n[372]\tvalid_0's l2: 0.0207806\n[373]\tvalid_0's l2: 0.0207568\n[374]\tvalid_0's l2: 0.0207581\n[375]\tvalid_0's l2: 0.0207445\n[376]\tvalid_0's l2: 0.0207268\n[377]\tvalid_0's l2: 0.0206922\n[378]\tvalid_0's l2: 0.0206521\n[379]\tvalid_0's l2: 0.0206482\n[380]\tvalid_0's l2: 0.0206457\n[381]\tvalid_0's l2: 0.0206264\n[382]\tvalid_0's l2: 0.0206091\n[383]\tvalid_0's l2: 0.0205933\n[384]\tvalid_0's l2: 0.020588\n[385]\tvalid_0's l2: 0.0205872\n[386]\tvalid_0's l2: 0.020551\n[387]\tvalid_0's l2: 0.0205265\n[388]\tvalid_0's l2: 0.0204802\n[389]\tvalid_0's l2: 0.020473\n[390]\tvalid_0's l2: 0.0204678\n[391]\tvalid_0's l2: 0.0204359\n[392]\tvalid_0's l2: 0.0204355\n[393]\tvalid_0's l2: 0.0204193\n[394]\tvalid_0's l2: 0.0203992\n[395]\tvalid_0's l2: 0.0203993\n[396]\tvalid_0's l2: 0.0203487\n[397]\tvalid_0's l2: 0.020327\n[398]\tvalid_0's l2: 0.0202888\n[399]\tvalid_0's l2: 0.0202285\n[400]\tvalid_0's l2: 0.020204\n[401]\tvalid_0's l2: 0.0201863\n[402]\tvalid_0's l2: 0.0201609\n[403]\tvalid_0's l2: 0.0201244\n[404]\tvalid_0's l2: 0.0200998\n[405]\tvalid_0's l2: 0.0200665\n[406]\tvalid_0's l2: 0.0200328\n[407]\tvalid_0's l2: 0.0200298\n[408]\tvalid_0's l2: 0.0200293\n[409]\tvalid_0's l2: 0.0200116\n[410]\tvalid_0's l2: 0.0199556\n[411]\tvalid_0's l2: 0.0199358\n[412]\tvalid_0's l2: 0.0198784\n[413]\tvalid_0's l2: 0.0198666\n[414]\tvalid_0's l2: 0.0198345\n[415]\tvalid_0's l2: 0.0198102\n[416]\tvalid_0's l2: 0.0198101\n[417]\tvalid_0's l2: 0.0197564\n[418]\tvalid_0's l2: 0.0197559\n[419]\tvalid_0's l2: 0.0197487\n[420]\tvalid_0's l2: 0.0197187\n[421]\tvalid_0's l2: 0.0197157\n[422]\tvalid_0's l2: 0.0197085\n[423]\tvalid_0's l2: 0.0196943\n[424]\tvalid_0's l2: 0.0196768\n[425]\tvalid_0's l2: 0.0196761\n[426]\tvalid_0's l2: 0.019676\n[427]\tvalid_0's l2: 0.0196526\n[428]\tvalid_0's l2: 0.0196462\n[429]\tvalid_0's l2: 0.0196469\n[430]\tvalid_0's l2: 0.0196473\n[431]\tvalid_0's l2: 0.0196448\n[432]\tvalid_0's l2: 0.0196454\n[433]\tvalid_0's l2: 0.0196208\n[434]\tvalid_0's l2: 0.0195932\n[435]\tvalid_0's l2: 0.0195932\n[436]\tvalid_0's l2: 0.0195544\n[437]\tvalid_0's l2: 0.0195345\n[438]\tvalid_0's l2: 0.0195243\n[439]\tvalid_0's l2: 0.0195206\n[440]\tvalid_0's l2: 0.0195058\n[441]\tvalid_0's l2: 0.0194812\n[442]\tvalid_0's l2: 0.0194785\n[443]\tvalid_0's l2: 0.019478\n[444]\tvalid_0's l2: 0.0194276\n[445]\tvalid_0's l2: 0.0193827\n[446]\tvalid_0's l2: 0.0193771\n[447]\tvalid_0's l2: 0.019359\n[448]\tvalid_0's l2: 0.0193554\n[449]\tvalid_0's l2: 0.019355\n[450]\tvalid_0's l2: 0.0193465\n[451]\tvalid_0's l2: 0.0193433\n[452]\tvalid_0's l2: 0.0193245\n[453]\tvalid_0's l2: 0.019315\n[454]\tvalid_0's l2: 0.0193149\n[455]\tvalid_0's l2: 0.0193143\n[456]\tvalid_0's l2: 0.0192779\n[457]\tvalid_0's l2: 0.0192766\n[458]\tvalid_0's l2: 0.0192723\n[459]\tvalid_0's l2: 0.0192465\n[460]\tvalid_0's l2: 0.0192433\n[461]\tvalid_0's l2: 0.0192439\n[462]\tvalid_0's l2: 0.0192343\n[463]\tvalid_0's l2: 0.0192121\n[464]\tvalid_0's l2: 0.0192119\n[465]\tvalid_0's l2: 0.0191873\n[466]\tvalid_0's l2: 0.0191884\n[467]\tvalid_0's l2: 0.0191855\n[468]\tvalid_0's l2: 0.0190898\n[469]\tvalid_0's l2: 0.0190846\n[470]\tvalid_0's l2: 0.0190683\n[471]\tvalid_0's l2: 0.0190684\n[472]\tvalid_0's l2: 0.0190499\n[473]\tvalid_0's l2: 0.0190496\n[474]\tvalid_0's l2: 0.0190486\n[475]\tvalid_0's l2: 0.019027\n[476]\tvalid_0's l2: 0.0189688\n[477]\tvalid_0's l2: 0.0189462\n[478]\tvalid_0's l2: 0.0190423\n[479]\tvalid_0's l2: 0.0190419\n[480]\tvalid_0's l2: 0.0190423\n[481]\tvalid_0's l2: 0.0190325\n[482]\tvalid_0's l2: 0.0190044\n[483]\tvalid_0's l2: 0.019005\n[484]\tvalid_0's l2: 0.0190068\n[485]\tvalid_0's l2: 0.0190032\n[486]\tvalid_0's l2: 0.0190005\n[487]\tvalid_0's l2: 0.0189974\n[488]\tvalid_0's l2: 0.0189691\n[489]\tvalid_0's l2: 0.0189664\n[490]\tvalid_0's l2: 0.0189017\n[491]\tvalid_0's l2: 0.0188913\n[492]\tvalid_0's l2: 0.0188835\n[493]\tvalid_0's l2: 0.0188713\n[494]\tvalid_0's l2: 0.0188496\n[495]\tvalid_0's l2: 0.0188372\n[496]\tvalid_0's l2: 0.0188234\n[497]\tvalid_0's l2: 0.0188224\n[498]\tvalid_0's l2: 0.0188098\n[499]\tvalid_0's l2: 0.0188216\n[500]\tvalid_0's l2: 0.018797\n[501]\tvalid_0's l2: 0.0187684\n[502]\tvalid_0's l2: 0.0187703\n[503]\tvalid_0's l2: 0.0187619\n[504]\tvalid_0's l2: 0.0187491\n[505]\tvalid_0's l2: 0.0187287\n[506]\tvalid_0's l2: 0.0187233\n[507]\tvalid_0's l2: 0.0186932\n[508]\tvalid_0's l2: 0.0186799\n[509]\tvalid_0's l2: 0.0186785\n[510]\tvalid_0's l2: 0.0186598\n[511]\tvalid_0's l2: 0.0186365\n[512]\tvalid_0's l2: 0.0186202\n[513]\tvalid_0's l2: 0.0186199\n[514]\tvalid_0's l2: 0.0186046\n[515]\tvalid_0's l2: 0.0186228\n[516]\tvalid_0's l2: 0.0186137\n[517]\tvalid_0's l2: 0.018592\n[518]\tvalid_0's l2: 0.0185656\n[519]\tvalid_0's l2: 0.018541\n[520]\tvalid_0's l2: 0.0185381\n[521]\tvalid_0's l2: 0.018524\n[522]\tvalid_0's l2: 0.0185315\n[523]\tvalid_0's l2: 0.0185323\n[524]\tvalid_0's l2: 0.0185111\n[525]\tvalid_0's l2: 0.0185031\n[526]\tvalid_0's l2: 0.0184754\n[527]\tvalid_0's l2: 0.0184743\n[528]\tvalid_0's l2: 0.0184637\n[529]\tvalid_0's l2: 0.0184467\n[530]\tvalid_0's l2: 0.0184482\n[531]\tvalid_0's l2: 0.0184479\n[532]\tvalid_0's l2: 0.0183935\n[533]\tvalid_0's l2: 0.0183883\n[534]\tvalid_0's l2: 0.0183832\n[535]\tvalid_0's l2: 0.0183605\n[536]\tvalid_0's l2: 0.018361\n[537]\tvalid_0's l2: 0.0183609\n[538]\tvalid_0's l2: 0.0183464\n[539]\tvalid_0's l2: 0.0183464\n[540]\tvalid_0's l2: 0.0183166\n[541]\tvalid_0's l2: 0.0183168\n[542]\tvalid_0's l2: 0.0183163\n[543]\tvalid_0's l2: 0.018298\n[544]\tvalid_0's l2: 0.0182911\n[545]\tvalid_0's l2: 0.0182914\n[546]\tvalid_0's l2: 0.0182956\n[547]\tvalid_0's l2: 0.0182796\n[548]\tvalid_0's l2: 0.0182609\n[549]\tvalid_0's l2: 0.0182616\n[550]\tvalid_0's l2: 0.0182412\n[551]\tvalid_0's l2: 0.0182373\n[552]\tvalid_0's l2: 0.0182291\n[553]\tvalid_0's l2: 0.0182195\n[554]\tvalid_0's l2: 0.0182216\n[555]\tvalid_0's l2: 0.0182221\n[556]\tvalid_0's l2: 0.0182233\n[557]\tvalid_0's l2: 0.0182206\n[558]\tvalid_0's l2: 0.0182095\n[559]\tvalid_0's l2: 0.0182064\n[560]\tvalid_0's l2: 0.0182064\n[561]\tvalid_0's l2: 0.0182059\n[562]\tvalid_0's l2: 0.0181963\n[563]\tvalid_0's l2: 0.0181962\n[564]\tvalid_0's l2: 0.0181947\n[565]\tvalid_0's l2: 0.018194\n[566]\tvalid_0's l2: 0.0181932\n[567]\tvalid_0's l2: 0.0181757\n[568]\tvalid_0's l2: 0.018176\n[569]\tvalid_0's l2: 0.0181606\n[570]\tvalid_0's l2: 0.0181367\n[571]\tvalid_0's l2: 0.018123\n[572]\tvalid_0's l2: 0.0181099\n[573]\tvalid_0's l2: 0.0181054\n[574]\tvalid_0's l2: 0.0180959\n[575]\tvalid_0's l2: 0.0181397\n[576]\tvalid_0's l2: 0.0181359\n[577]\tvalid_0's l2: 0.0181302\n[578]\tvalid_0's l2: 0.0181299\n[579]\tvalid_0's l2: 0.0181141\n[580]\tvalid_0's l2: 0.0180883\n[581]\tvalid_0's l2: 0.0180698\n[582]\tvalid_0's l2: 0.0180635\n[583]\tvalid_0's l2: 0.0180631\n[584]\tvalid_0's l2: 0.0180637\n[585]\tvalid_0's l2: 0.0180558\n[586]\tvalid_0's l2: 0.0180613\n[587]\tvalid_0's l2: 0.0180498\n[588]\tvalid_0's l2: 0.018047\n[589]\tvalid_0's l2: 0.0180271\n[590]\tvalid_0's l2: 0.018005\n[591]\tvalid_0's l2: 0.0179849\n[592]\tvalid_0's l2: 0.0179752\n[593]\tvalid_0's l2: 0.0178995\n[594]\tvalid_0's l2: 0.0178929\n[595]\tvalid_0's l2: 0.0178921\n[596]\tvalid_0's l2: 0.0178418\n[597]\tvalid_0's l2: 0.0178358\n[598]\tvalid_0's l2: 0.0178296\n[599]\tvalid_0's l2: 0.0178295\n[600]\tvalid_0's l2: 0.0178275\n[601]\tvalid_0's l2: 0.0178226\n[602]\tvalid_0's l2: 0.0178227\n[603]\tvalid_0's l2: 0.0178159\n[604]\tvalid_0's l2: 0.0178001\n[605]\tvalid_0's l2: 0.017794\n[606]\tvalid_0's l2: 0.0177389\n[607]\tvalid_0's l2: 0.0177379\n[608]\tvalid_0's l2: 0.0177115\n[609]\tvalid_0's l2: 0.0177003\n[610]\tvalid_0's l2: 0.0176991\n[611]\tvalid_0's l2: 0.0176995\n[612]\tvalid_0's l2: 0.0176969\n[613]\tvalid_0's l2: 0.0176968\n[614]\tvalid_0's l2: 0.0176931\n[615]\tvalid_0's l2: 0.0176895\n[616]\tvalid_0's l2: 0.017666\n[617]\tvalid_0's l2: 0.0176479\n[618]\tvalid_0's l2: 0.0176322\n[619]\tvalid_0's l2: 0.0176067\n[620]\tvalid_0's l2: 0.0176024\n[621]\tvalid_0's l2: 0.0175697\n[622]\tvalid_0's l2: 0.017652\n[623]\tvalid_0's l2: 0.0176454\n[624]\tvalid_0's l2: 0.0176297\n[625]\tvalid_0's l2: 0.0176285\n[626]\tvalid_0's l2: 0.0176088\n[627]\tvalid_0's l2: 0.0176023\n[628]\tvalid_0's l2: 0.0175865\n[629]\tvalid_0's l2: 0.0175749\n[630]\tvalid_0's l2: 0.0175586\n[631]\tvalid_0's l2: 0.0175529\n[632]\tvalid_0's l2: 0.0175286\n[633]\tvalid_0's l2: 0.0175114\n[634]\tvalid_0's l2: 0.0175112\n[635]\tvalid_0's l2: 0.0175047\n[636]\tvalid_0's l2: 0.0174807\n[637]\tvalid_0's l2: 0.0174725\n[638]\tvalid_0's l2: 0.0174754\n[639]\tvalid_0's l2: 0.0174695\n[640]\tvalid_0's l2: 0.0174675\n[641]\tvalid_0's l2: 0.0174549\n[642]\tvalid_0's l2: 0.0174462\n[643]\tvalid_0's l2: 0.0174384\n[644]\tvalid_0's l2: 0.0174338\n[645]\tvalid_0's l2: 0.0174223\n[646]\tvalid_0's l2: 0.0174003\n[647]\tvalid_0's l2: 0.0174009\n[648]\tvalid_0's l2: 0.0174041\n[649]\tvalid_0's l2: 0.017392\n[650]\tvalid_0's l2: 0.0173825\n[651]\tvalid_0's l2: 0.0173475\n[652]\tvalid_0's l2: 0.0173282\n[653]\tvalid_0's l2: 0.0173163\n[654]\tvalid_0's l2: 0.0173164\n[655]\tvalid_0's l2: 0.0173017\n[656]\tvalid_0's l2: 0.0172892\n[657]\tvalid_0's l2: 0.0172803\n[658]\tvalid_0's l2: 0.0172344\n[659]\tvalid_0's l2: 0.0172337\n[660]\tvalid_0's l2: 0.0172423\n[661]\tvalid_0's l2: 0.0172333\n[662]\tvalid_0's l2: 0.017228\n[663]\tvalid_0's l2: 0.0172151\n[664]\tvalid_0's l2: 0.0172151\n[665]\tvalid_0's l2: 0.0172143\n[666]\tvalid_0's l2: 0.0172107\n[667]\tvalid_0's l2: 0.0171665\n[668]\tvalid_0's l2: 0.0171668\n[669]\tvalid_0's l2: 0.0171817\n[670]\tvalid_0's l2: 0.0171721\n[671]\tvalid_0's l2: 0.0171682\n[672]\tvalid_0's l2: 0.0171684\n[673]\tvalid_0's l2: 0.0171646\n[674]\tvalid_0's l2: 0.0171645\n[675]\tvalid_0's l2: 0.0171507\n[676]\tvalid_0's l2: 0.0171427\n[677]\tvalid_0's l2: 0.0171429\n[678]\tvalid_0's l2: 0.017142\n[679]\tvalid_0's l2: 0.0171225\n[680]\tvalid_0's l2: 0.0171236\n[681]\tvalid_0's l2: 0.0171242\n[682]\tvalid_0's l2: 0.0171077\n[683]\tvalid_0's l2: 0.0171054\n[684]\tvalid_0's l2: 0.0170962\n[685]\tvalid_0's l2: 0.0170959\n[686]\tvalid_0's l2: 0.0170828\n[687]\tvalid_0's l2: 0.0170763\n[688]\tvalid_0's l2: 0.0170765\n[689]\tvalid_0's l2: 0.0170605\n[690]\tvalid_0's l2: 0.0170602\n[691]\tvalid_0's l2: 0.0170602\n[692]\tvalid_0's l2: 0.0170635\n[693]\tvalid_0's l2: 0.0170686\n[694]\tvalid_0's l2: 0.0170678\n[695]\tvalid_0's l2: 0.0170621\n[696]\tvalid_0's l2: 0.0170548\n[697]\tvalid_0's l2: 0.0170488\n[698]\tvalid_0's l2: 0.0170469\n[699]\tvalid_0's l2: 0.0170464\n[700]\tvalid_0's l2: 0.0170321\n[701]\tvalid_0's l2: 0.0170319\n[702]\tvalid_0's l2: 0.0170143\n[703]\tvalid_0's l2: 0.0170076\n[704]\tvalid_0's l2: 0.017002\n[705]\tvalid_0's l2: 0.0170008\n[706]\tvalid_0's l2: 0.0169711\n[707]\tvalid_0's l2: 0.0169672\n[708]\tvalid_0's l2: 0.0169675\n[709]\tvalid_0's l2: 0.0169638\n[710]\tvalid_0's l2: 0.0169646\n[711]\tvalid_0's l2: 0.0169641\n[712]\tvalid_0's l2: 0.0169631\n[713]\tvalid_0's l2: 0.0169625\n[714]\tvalid_0's l2: 0.0169627\n[715]\tvalid_0's l2: 0.0169563\n[716]\tvalid_0's l2: 0.0169542\n[717]\tvalid_0's l2: 0.0169489\n[718]\tvalid_0's l2: 0.0169467\n[719]\tvalid_0's l2: 0.0169464\n[720]\tvalid_0's l2: 0.0169436\n[721]\tvalid_0's l2: 0.0169334\n[722]\tvalid_0's l2: 0.0169373\n[723]\tvalid_0's l2: 0.0169294\n[724]\tvalid_0's l2: 0.0169296\n[725]\tvalid_0's l2: 0.0169292\n[726]\tvalid_0's l2: 0.0169288\n[727]\tvalid_0's l2: 0.0169302\n[728]\tvalid_0's l2: 0.016916\n[729]\tvalid_0's l2: 0.0169085\n[730]\tvalid_0's l2: 0.0169064\n[731]\tvalid_0's l2: 0.0169003\n[732]\tvalid_0's l2: 0.0168978\n[733]\tvalid_0's l2: 0.0168959\n[734]\tvalid_0's l2: 0.0168963\n[735]\tvalid_0's l2: 0.0168959\n[736]\tvalid_0's l2: 0.0168862\n[737]\tvalid_0's l2: 0.0168658\n[738]\tvalid_0's l2: 0.0168592\n[739]\tvalid_0's l2: 0.0168485\n[740]\tvalid_0's l2: 0.0168412\n[741]\tvalid_0's l2: 0.0168361\n[742]\tvalid_0's l2: 0.0168357\n[743]\tvalid_0's l2: 0.0168366\n[744]\tvalid_0's l2: 0.0168244\n[745]\tvalid_0's l2: 0.0168242\n[746]\tvalid_0's l2: 0.0168151\n[747]\tvalid_0's l2: 0.016798\n[748]\tvalid_0's l2: 0.0167957\n[749]\tvalid_0's l2: 0.0167955\n[750]\tvalid_0's l2: 0.0167918\n[751]\tvalid_0's l2: 0.0167836\n[752]\tvalid_0's l2: 0.0167806\n[753]\tvalid_0's l2: 0.0168086\n[754]\tvalid_0's l2: 0.0168048\n[755]\tvalid_0's l2: 0.0168044\n[756]\tvalid_0's l2: 0.0168042\n[757]\tvalid_0's l2: 0.0167997\n[758]\tvalid_0's l2: 0.016787\n[759]\tvalid_0's l2: 0.0167832\n[760]\tvalid_0's l2: 0.0167918\n[761]\tvalid_0's l2: 0.0167814\n[762]\tvalid_0's l2: 0.0167817\n[763]\tvalid_0's l2: 0.0167747\n[764]\tvalid_0's l2: 0.0167748\n[765]\tvalid_0's l2: 0.0167669\n[766]\tvalid_0's l2: 0.0167553\n[767]\tvalid_0's l2: 0.0167038\n[768]\tvalid_0's l2: 0.0166872\n[769]\tvalid_0's l2: 0.0166572\n[770]\tvalid_0's l2: 0.0166496\n[771]\tvalid_0's l2: 0.0166333\n[772]\tvalid_0's l2: 0.0166331\n[773]\tvalid_0's l2: 0.0166258\n[774]\tvalid_0's l2: 0.0166154\n[775]\tvalid_0's l2: 0.0166149\n[776]\tvalid_0's l2: 0.0166149\n[777]\tvalid_0's l2: 0.01661\n[778]\tvalid_0's l2: 0.0166024\n[779]\tvalid_0's l2: 0.0165912\n[780]\tvalid_0's l2: 0.0165786\n[781]\tvalid_0's l2: 0.0165419\n[782]\tvalid_0's l2: 0.0165399\n[783]\tvalid_0's l2: 0.0165423\n[784]\tvalid_0's l2: 0.0165422\n[785]\tvalid_0's l2: 0.0165344\n[786]\tvalid_0's l2: 0.0165258\n[787]\tvalid_0's l2: 0.0165251\n[788]\tvalid_0's l2: 0.0165257\n[789]\tvalid_0's l2: 0.0165256\n[790]\tvalid_0's l2: 0.0165255\n[791]\tvalid_0's l2: 0.0165168\n[792]\tvalid_0's l2: 0.0165071\n[793]\tvalid_0's l2: 0.0165\n[794]\tvalid_0's l2: 0.0164952\n[795]\tvalid_0's l2: 0.0164953\n[796]\tvalid_0's l2: 0.0164887\n[797]\tvalid_0's l2: 0.0164752\n[798]\tvalid_0's l2: 0.0164693\n[799]\tvalid_0's l2: 0.0164721\n[800]\tvalid_0's l2: 0.0164685\n[801]\tvalid_0's l2: 0.0164683\n[802]\tvalid_0's l2: 0.0164644\n[803]\tvalid_0's l2: 0.0164575\n[804]\tvalid_0's l2: 0.0164485\n[805]\tvalid_0's l2: 0.0164376\n[806]\tvalid_0's l2: 0.0164381\n[807]\tvalid_0's l2: 0.0164138\n[808]\tvalid_0's l2: 0.0164135\n[809]\tvalid_0's l2: 0.0163968\n[810]\tvalid_0's l2: 0.0163925\n[811]\tvalid_0's l2: 0.0163925\n[812]\tvalid_0's l2: 0.0163901\n[813]\tvalid_0's l2: 0.0163907\n[814]\tvalid_0's l2: 0.0163841\n[815]\tvalid_0's l2: 0.0163796\n[816]\tvalid_0's l2: 0.0163782\n[817]\tvalid_0's l2: 0.0163744\n[818]\tvalid_0's l2: 0.0163751\n[819]\tvalid_0's l2: 0.0163718\n[820]\tvalid_0's l2: 0.0163715\n[821]\tvalid_0's l2: 0.0163147\n[822]\tvalid_0's l2: 0.0163152\n[823]\tvalid_0's l2: 0.0163081\n[824]\tvalid_0's l2: 0.0163065\n[825]\tvalid_0's l2: 0.0163022\n[826]\tvalid_0's l2: 0.0162939\n[827]\tvalid_0's l2: 0.0162933\n[828]\tvalid_0's l2: 0.0162465\n[829]\tvalid_0's l2: 0.0162464\n[830]\tvalid_0's l2: 0.0162441\n[831]\tvalid_0's l2: 0.0162272\n[832]\tvalid_0's l2: 0.0162155\n[833]\tvalid_0's l2: 0.0162157\n[834]\tvalid_0's l2: 0.0162064\n[835]\tvalid_0's l2: 0.0162012\n[836]\tvalid_0's l2: 0.0161984\n[837]\tvalid_0's l2: 0.0162025\n[838]\tvalid_0's l2: 0.0162025\n[839]\tvalid_0's l2: 0.0162027\n[840]\tvalid_0's l2: 0.0162027\n[841]\tvalid_0's l2: 0.0162016\n[842]\tvalid_0's l2: 0.0161972\n[843]\tvalid_0's l2: 0.0161927\n[844]\tvalid_0's l2: 0.0161785\n[845]\tvalid_0's l2: 0.0161636\n[846]\tvalid_0's l2: 0.0161575\n[847]\tvalid_0's l2: 0.0161535\n[848]\tvalid_0's l2: 0.0161527\n[849]\tvalid_0's l2: 0.0161461\n[850]\tvalid_0's l2: 0.016167\n[851]\tvalid_0's l2: 0.0161545\n[852]\tvalid_0's l2: 0.0161551\n[853]\tvalid_0's l2: 0.0161549\n[854]\tvalid_0's l2: 0.0161549\n[855]\tvalid_0's l2: 0.0161543\n[856]\tvalid_0's l2: 0.0161485\n[857]\tvalid_0's l2: 0.0161407\n[858]\tvalid_0's l2: 0.0161322\n[859]\tvalid_0's l2: 0.0161322\n[860]\tvalid_0's l2: 0.0161213\n[861]\tvalid_0's l2: 0.0161212\n[862]\tvalid_0's l2: 0.0161126\n[863]\tvalid_0's l2: 0.0161125\n[864]\tvalid_0's l2: 0.0161121\n[865]\tvalid_0's l2: 0.0161091\n[866]\tvalid_0's l2: 0.0161074\n[867]\tvalid_0's l2: 0.0161057\n[868]\tvalid_0's l2: 0.0160962\n[869]\tvalid_0's l2: 0.0160981\n[870]\tvalid_0's l2: 0.0160981\n[871]\tvalid_0's l2: 0.0160977\n[872]\tvalid_0's l2: 0.0160937\n[873]\tvalid_0's l2: 0.0160927\n[874]\tvalid_0's l2: 0.0160859\n[875]\tvalid_0's l2: 0.0160849\n[876]\tvalid_0's l2: 0.0160785\n[877]\tvalid_0's l2: 0.0160775\n[878]\tvalid_0's l2: 0.0160776\n[879]\tvalid_0's l2: 0.0160768\n[880]\tvalid_0's l2: 0.0160446\n[881]\tvalid_0's l2: 0.016037\n[882]\tvalid_0's l2: 0.0160332\n[883]\tvalid_0's l2: 0.0160261\n[884]\tvalid_0's l2: 0.0160216\n[885]\tvalid_0's l2: 0.016022\n[886]\tvalid_0's l2: 0.0160075\n[887]\tvalid_0's l2: 0.0160041\n[888]\tvalid_0's l2: 0.0160024\n[889]\tvalid_0's l2: 0.0160026\n[890]\tvalid_0's l2: 0.0160016\n[891]\tvalid_0's l2: 0.0159986\n[892]\tvalid_0's l2: 0.0160013\n[893]\tvalid_0's l2: 0.0159394\n[894]\tvalid_0's l2: 0.0159374\n[895]\tvalid_0's l2: 0.0159369\n[896]\tvalid_0's l2: 0.0159108\n[897]\tvalid_0's l2: 0.0159064\n[898]\tvalid_0's l2: 0.0159005\n[899]\tvalid_0's l2: 0.0159009\n[900]\tvalid_0's l2: 0.0159005\n[901]\tvalid_0's l2: 0.0158902\n[902]\tvalid_0's l2: 0.0158879\n[903]\tvalid_0's l2: 0.0158828\n[904]\tvalid_0's l2: 0.0158831\n[905]\tvalid_0's l2: 0.0158788\n[906]\tvalid_0's l2: 0.0158767\n[907]\tvalid_0's l2: 0.0158771\n[908]\tvalid_0's l2: 0.0158737\n[909]\tvalid_0's l2: 0.0158741\n[910]\tvalid_0's l2: 0.0158666\n[911]\tvalid_0's l2: 0.0158663\n[912]\tvalid_0's l2: 0.0158663\n[913]\tvalid_0's l2: 0.0158666\n[914]\tvalid_0's l2: 0.0158641\n[915]\tvalid_0's l2: 0.0158583\n[916]\tvalid_0's l2: 0.0158583\n[917]\tvalid_0's l2: 0.0158561\n[918]\tvalid_0's l2: 0.0158556\n[919]\tvalid_0's l2: 0.0158516\n[920]\tvalid_0's l2: 0.0158524\n[921]\tvalid_0's l2: 0.0158416\n[922]\tvalid_0's l2: 0.0158398\n[923]\tvalid_0's l2: 0.0158358\n[924]\tvalid_0's l2: 0.0158352\n[925]\tvalid_0's l2: 0.0158269\n[926]\tvalid_0's l2: 0.0158265\n[927]\tvalid_0's l2: 0.0158379\n[928]\tvalid_0's l2: 0.0158587\n[929]\tvalid_0's l2: 0.015858\n[930]\tvalid_0's l2: 0.0158531\n[931]\tvalid_0's l2: 0.015852\n[932]\tvalid_0's l2: 0.0158491\n[933]\tvalid_0's l2: 0.0158495\n[934]\tvalid_0's l2: 0.0158364\n[935]\tvalid_0's l2: 0.0158169\n[936]\tvalid_0's l2: 0.0158148\n[937]\tvalid_0's l2: 0.015815\n[938]\tvalid_0's l2: 0.015808\n[939]\tvalid_0's l2: 0.015807\n[940]\tvalid_0's l2: 0.0157803\n[941]\tvalid_0's l2: 0.015774\n[942]\tvalid_0's l2: 0.0157717\n[943]\tvalid_0's l2: 0.0157674\n[944]\tvalid_0's l2: 0.0157641\n[945]\tvalid_0's l2: 0.0157547\n[946]\tvalid_0's l2: 0.0157544\n[947]\tvalid_0's l2: 0.0157583\n[948]\tvalid_0's l2: 0.015758\n[949]\tvalid_0's l2: 0.0157584\n[950]\tvalid_0's l2: 0.0157544\n[951]\tvalid_0's l2: 0.0157425\n[952]\tvalid_0's l2: 0.0157207\n[953]\tvalid_0's l2: 0.0157169\n[954]\tvalid_0's l2: 0.0157044\n[955]\tvalid_0's l2: 0.0157037\n[956]\tvalid_0's l2: 0.0156964\n[957]\tvalid_0's l2: 0.0156926\n[958]\tvalid_0's l2: 0.0156865\n[959]\tvalid_0's l2: 0.0156787\n[960]\tvalid_0's l2: 0.0156774\n[961]\tvalid_0's l2: 0.0156388\n[962]\tvalid_0's l2: 0.0156385\n[963]\tvalid_0's l2: 0.0156315\n[964]\tvalid_0's l2: 0.0156311\n[965]\tvalid_0's l2: 0.0156141\n[966]\tvalid_0's l2: 0.0156126\n[967]\tvalid_0's l2: 0.01561\n[968]\tvalid_0's l2: 0.0156099\n[969]\tvalid_0's l2: 0.0156089\n[970]\tvalid_0's l2: 0.0155926\n[971]\tvalid_0's l2: 0.0155919\n[972]\tvalid_0's l2: 0.0155886\n[973]\tvalid_0's l2: 0.0155682\n[974]\tvalid_0's l2: 0.0155678\n[975]\tvalid_0's l2: 0.0155585\n[976]\tvalid_0's l2: 0.015512\n[977]\tvalid_0's l2: 0.0155096\n[978]\tvalid_0's l2: 0.0155096\n[979]\tvalid_0's l2: 0.0155097\n[980]\tvalid_0's l2: 0.0155033\n[981]\tvalid_0's l2: 0.0154978\n[982]\tvalid_0's l2: 0.0154928\n[983]\tvalid_0's l2: 0.0154928\n[984]\tvalid_0's l2: 0.0154895\n[985]\tvalid_0's l2: 0.0154902\n[986]\tvalid_0's l2: 0.0154907\n[987]\tvalid_0's l2: 0.0154844\n[988]\tvalid_0's l2: 0.0154882\n[989]\tvalid_0's l2: 0.0154879\n[990]\tvalid_0's l2: 0.0154838\n[991]\tvalid_0's l2: 0.0154841\n[992]\tvalid_0's l2: 0.0154824\n[993]\tvalid_0's l2: 0.0154808\n[994]\tvalid_0's l2: 0.0154796\n[995]\tvalid_0's l2: 0.01548\n[996]\tvalid_0's l2: 0.0154757\n[997]\tvalid_0's l2: 0.0154757\n[998]\tvalid_0's l2: 0.0154685\n[999]\tvalid_0's l2: 0.0154661\n[1000]\tvalid_0's l2: 0.0154652\nElapsed time for fitting LightGBM model: 21.25 s\nEarly stopping performed. Best iteration: 1000\nElapsed time for fitting LightGBM model: 26.95 s\npath: predictions/pointpredictions_lightgbm.csv\ndirname: predictions\nfilename: pointpredictions_lightgbm.csv\nartifact_path: predictions\npath: /tmp/tmpvseqmbbr/pointpredictions_lightgbm.csv\ntmp_path: /tmp/tmpvseqmbbr/pointpredictions_lightgbm.csv\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["lightgbm_params = {\n","    \"boosting_type\": 'gbdt',\n","    \"objective\": 'regression',\n","    \"n_jobs\": -1, \n","    \"min_split_gain\": 0.0,\n","    \"min_data_in_leaf\": 1,\n","    \"max_bin\": 1024,\n","    \"num_leaves\": 64, \n","    \"max_depth\": -1,\n","    \"learning_rate\": 0.1,\n","    \"n_estimators\": 1000,\n","    \"feature_fraction\": 0.7,\n","    \"bagging_fraction\": 0.7,\n","    \"bagging_freq\": 1, \n","    \"seed\": 1,\n","    \"lambda\": 1,\n","}\n","\n","early_stopping_round = 20\n","\n","start_time = time.perf_counter()\n","    \n","# fitting model on train set with early stopping on valid set\n","lightgbm_reg = LightGBM(vectorizer_with_nan, target_transformer=target_transformer)\n","lightgbm_fit_params = {**lightgbm_params, \"early_stopping_round\": early_stopping_round}\n","lightgbm_reg.fit(train_val_df, TARGET, X_val=valid_df, y_val=valid_df[TARGET], params=lightgbm_fit_params, verbose=True)\n","lightgbm_best_iteration = lightgbm_reg.best_iteration\n","print(\"Early stopping performed. Best iteration:\", lightgbm_best_iteration)\n","\n","# fitting model on train+val set with best_iteration\n","lightgbm_full_train_reg = LightGBM(vectorizer_with_nan, target_transformer=target_transformer)\n","lightgbm_full_train_params = {**lightgbm_params, \"n_estimators\": lightgbm_best_iteration}\n","lightgbm_full_train_reg.fit(train_df, TARGET, params=lightgbm_full_train_params, verbose=True)\n","\n","# predicting on test set with our fully trained model\n","lightgbm_pred = lightgbm_full_train_reg.predict(test_df)\n","lightgbm_metrics = lightgbm_full_train_reg.metrics(test_df[TARGET], lightgbm_pred)\n","\n","end_time = time.perf_counter()\n","full_time = np.round(end_time - start_time, 2)\n","lightgbm_metrics['time'] = full_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lightgbm_metrics"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a975650a-0147-4726-8e4e-5711c3920a8f","showTitle":false,"title":""}},"source":["## 3.2 Bootstrapping (Data Sampling)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fdf6e5dd-e9a9-418b-82bd-a6886a4a42d8","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["d84b31d6869042bead2e53169b7939b1\n","resampling i: 1\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 20.53 s\n","resampling i: 2\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.93 s\n","resampling i: 3\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.12 s\n","resampling i: 4\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.48 s\n","resampling i: 5\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.2 s\n","resampling i: 6\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.5 s\n","resampling i: 7\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.46 s\n","resampling i: 8\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 30.21 s\n","resampling i: 9\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.22 s\n","resampling i: 10\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 30.66 s\n","resampling i: 11\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.7 s\n","resampling i: 12\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.12 s\n","resampling i: 13\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 26.81 s\n","resampling i: 14\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.18 s\n","resampling i: 15\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 31.21 s\n","resampling i: 16\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 30.41 s\n","resampling i: 17\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.53 s\n","resampling i: 18\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.47 s\n","resampling i: 19\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.36 s\n","resampling i: 20\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.44 s\n","resampling i: 21\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.61 s\n","resampling i: 22\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.27 s\n","resampling i: 23\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.17 s\n","resampling i: 24\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.54 s\n","resampling i: 25\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.05 s\n","resampling i: 26\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.62 s\n","resampling i: 27\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.6 s\n","resampling i: 28\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.78 s\n","resampling i: 29\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.48 s\n","resampling i: 30\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.82 s\n","resampling i: 31\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 30.21 s\n","resampling i: 32\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.8 s\n","resampling i: 33\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.16 s\n","resampling i: 34\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.27 s\n","resampling i: 35\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.67 s\n","resampling i: 36\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.51 s\n","resampling i: 37\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.15 s\n","resampling i: 38\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.25 s\n","resampling i: 39\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fra\n","\n","*** WARNING: max output size exceeded, skipping output. ***\n","\n","nt value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.22 s\n","resampling i: 64\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.78 s\n","resampling i: 65\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.86 s\n","resampling i: 66\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.16 s\n","resampling i: 67\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 26.97 s\n","resampling i: 68\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.98 s\n","resampling i: 69\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.32 s\n","resampling i: 70\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.42 s\n","resampling i: 71\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.48 s\n","resampling i: 72\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 29.17 s\n","resampling i: 73\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.65 s\n","resampling i: 74\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.19 s\n","resampling i: 75\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.21 s\n","resampling i: 76\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.44 s\n","resampling i: 77\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.47 s\n","resampling i: 78\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.65 s\n","resampling i: 79\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.62 s\n","resampling i: 80\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.29 s\n","resampling i: 81\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.62 s\n","resampling i: 82\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.35 s\n","resampling i: 83\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.54 s\n","resampling i: 84\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 30.72 s\n","resampling i: 85\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.54 s\n","resampling i: 86\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 28.9 s\n","resampling i: 87\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.36 s\n","resampling i: 88\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.44 s\n","resampling i: 89\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.55 s\n","resampling i: 90\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.61 s\n","resampling i: 91\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.31 s\n","resampling i: 92\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.61 s\n","resampling i: 93\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.46 s\n","resampling i: 94\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.3 s\n","resampling i: 95\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.14 s\n","resampling i: 96\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.83 s\n","resampling i: 97\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.64 s\n","resampling i: 98\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.82 s\n","resampling i: 99\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.77 s\n","resampling i: 100\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","Elapsed time for fitting LightGBM model: 27.61 s\n","path: predictions/pointpredictions_bootstrapping.csv\n","dirname: predictions\n","filename: pointpredictions_bootstrapping.csv\n","artifact_path: predictions\n","path: /tmp/tmpbw26l43g/pointpredictions_bootstrapping.csv\n","tmp_path: /tmp/tmpbw26l43g/pointpredictions_bootstrapping.csv\n","path: predictions/quantiles_bootstrapping0.1.csv\n","dirname: predictions\n","filename: quantiles_bootstrapping0.1.csv\n","artifact_path: predictions\n","path: /tmp/tmpg0_gv3er/quantiles_bootstrapping0.1.csv\n","tmp_path: /tmp/tmpg0_gv3er/quantiles_bootstrapping0.1.csv\n","path: predictions/quantiles_bootstrapping0.9.csv\n","dirname: predictions\n","filename: quantiles_bootstrapping0.9.csv\n","artifact_path: predictions\n","path: /tmp/tmpfsxz162i/quantiles_bootstrapping0.9.csv\n","tmp_path: /tmp/tmpfsxz162i/quantiles_bootstrapping0.9.csv\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"d84b31d6869042bead2e53169b7939b1\nresampling i: 1\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 20.53 s\nresampling i: 2\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.93 s\nresampling i: 3\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.12 s\nresampling i: 4\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.48 s\nresampling i: 5\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.2 s\nresampling i: 6\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.5 s\nresampling i: 7\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.46 s\nresampling i: 8\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 30.21 s\nresampling i: 9\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.22 s\nresampling i: 10\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 30.66 s\nresampling i: 11\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.7 s\nresampling i: 12\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.12 s\nresampling i: 13\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 26.81 s\nresampling i: 14\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.18 s\nresampling i: 15\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 31.21 s\nresampling i: 16\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 30.41 s\nresampling i: 17\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.53 s\nresampling i: 18\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.47 s\nresampling i: 19\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.36 s\nresampling i: 20\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.44 s\nresampling i: 21\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.61 s\nresampling i: 22\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.27 s\nresampling i: 23\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.17 s\nresampling i: 24\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.54 s\nresampling i: 25\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.05 s\nresampling i: 26\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.62 s\nresampling i: 27\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.6 s\nresampling i: 28\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.78 s\nresampling i: 29\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.48 s\nresampling i: 30\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.82 s\nresampling i: 31\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 30.21 s\nresampling i: 32\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.8 s\nresampling i: 33\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.16 s\nresampling i: 34\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.27 s\nresampling i: 35\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.67 s\nresampling i: 36\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.51 s\nresampling i: 37\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.15 s\nresampling i: 38\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.25 s\nresampling i: 39\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fra\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nnt value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.22 s\nresampling i: 64\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.78 s\nresampling i: 65\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.86 s\nresampling i: 66\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.16 s\nresampling i: 67\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 26.97 s\nresampling i: 68\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.98 s\nresampling i: 69\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.32 s\nresampling i: 70\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.42 s\nresampling i: 71\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.48 s\nresampling i: 72\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 29.17 s\nresampling i: 73\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.65 s\nresampling i: 74\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.19 s\nresampling i: 75\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.21 s\nresampling i: 76\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.44 s\nresampling i: 77\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.47 s\nresampling i: 78\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.65 s\nresampling i: 79\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.62 s\nresampling i: 80\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.29 s\nresampling i: 81\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.62 s\nresampling i: 82\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.35 s\nresampling i: 83\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.54 s\nresampling i: 84\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 30.72 s\nresampling i: 85\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.54 s\nresampling i: 86\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 28.9 s\nresampling i: 87\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.36 s\nresampling i: 88\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.44 s\nresampling i: 89\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.55 s\nresampling i: 90\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.61 s\nresampling i: 91\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.31 s\nresampling i: 92\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.61 s\nresampling i: 93\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.46 s\nresampling i: 94\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.3 s\nresampling i: 95\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.14 s\nresampling i: 96\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.83 s\nresampling i: 97\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.64 s\nresampling i: 98\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.82 s\nresampling i: 99\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.77 s\nresampling i: 100\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nElapsed time for fitting LightGBM model: 27.61 s\npath: predictions/pointpredictions_bootstrapping.csv\ndirname: predictions\nfilename: pointpredictions_bootstrapping.csv\nartifact_path: predictions\npath: /tmp/tmpbw26l43g/pointpredictions_bootstrapping.csv\ntmp_path: /tmp/tmpbw26l43g/pointpredictions_bootstrapping.csv\npath: predictions/quantiles_bootstrapping0.1.csv\ndirname: predictions\nfilename: quantiles_bootstrapping0.1.csv\nartifact_path: predictions\npath: /tmp/tmpg0_gv3er/quantiles_bootstrapping0.1.csv\ntmp_path: /tmp/tmpg0_gv3er/quantiles_bootstrapping0.1.csv\npath: predictions/quantiles_bootstrapping0.9.csv\ndirname: predictions\nfilename: quantiles_bootstrapping0.9.csv\nartifact_path: predictions\npath: /tmp/tmpfsxz162i/quantiles_bootstrapping0.9.csv\ntmp_path: /tmp/tmpfsxz162i/quantiles_bootstrapping0.9.csv\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["# fitting model with parameters obtained from early stopping with train_val set\n","n_resamples = 100\n","case_resampling_results = np.zeros(shape = (len(test_df), n_resamples))\n","\n","start_time = time.perf_counter()\n","\n","train_temp = train_df.reset_index(drop=True).copy()\n","for i in range(n_resamples):\n","    print(f\"resampling i: {i+1}\")\n","\n","    if i == 0:\n","        train_resampled = train_temp\n","    else:\n","        train_resampled = train_temp.iloc[random.choices(list(range(len(train_temp))), k = len(train_temp))].reset_index(drop=True)\n","\n","    lightgbm_full_train_params = {**lightgbm_params, \"n_estimators\": lightgbm_best_iteration}\n","    lgb_model = LightGBM(vectorizer_with_nan, target_transformer=target_transformer)\n","    lgb_model.fit(train_resampled, TARGET, params=lightgbm_full_train_params, verbose=True)\n","\n","    # predicting on test set with our fully trained model\n","    predictions = lgb_model.predict(test_df)\n","    case_resampling_results[:, i] = predictions[PredEnum.POINT_ESTIMATES]\n","\n","samples = case_resampling_results\n","quantiles = np.concatenate((np.quantile(case_resampling_results, q = 0.1, axis = 1)[:,np.newaxis], np.quantile(case_resampling_results, q = 0.9, axis = 1)[:,np.newaxis]), axis = 1)\n","point_pred = case_resampling_results.mean(axis = 1)\n","    \n","bootstrap_metrics = {}\n","bootstrap_metrics['rmse'] = Model.rmse(test_df[TARGET], point_pred)\n","bootstrap_metrics['rmspe'] = Model.rmspe(test_df[TARGET], point_pred)\n","bootstrap_metrics['avg_interval_length'] = Model.avg_interval_length(quantiles)\n","bootstrap_metrics['sharpness'] = Model.avg_interval_length(quantiles)\n","bootstrap_metrics['coverage'] = Model.coverage(test_df[TARGET], quantiles)\n","bootstrap_metrics['crps'] = Model.crps(test_df[TARGET], case_resampling_results)\n","bootstrap_metrics['nll_from_samples'] = Model.neg_log_likelihood_with_kde(np.array(test_df[TARGET]), case_resampling_results)\n","\n","end_time = time.perf_counter()\n","full_time = np.round(end_time - start_time, 2)\n","bootstrap_metrics['time'] = full_time"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b57ab47e-e7eb-40fa-bdb6-b7df15e580af","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[26]: {'rmse': 962.3463783666828,\n"," 'rmspe': 0.12462515780224556,\n"," 'avg_interval_length': 1258.5053691365442,\n"," 'sharpness': 1258.5053691365442,\n"," 'coverage': 0.5626580071484614,\n"," 'crps': 492.8727313829994,\n"," 'nll_from_samples': 10.453515299090732,\n"," 'time': 21.25}"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[26]: {'rmse': 962.3463783666828,\n 'rmspe': 0.12462515780224556,\n 'avg_interval_length': 1258.5053691365442,\n 'sharpness': 1258.5053691365442,\n 'coverage': 0.5626580071484614,\n 'crps': 492.8727313829994,\n 'nll_from_samples': 10.453515299090732,\n 'time': 21.25}","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["bootstrap_metrics"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"635ce0e3-7863-4d4b-a1eb-e6015b865f7f","showTitle":false,"title":""}},"source":["## 3.3 Quantile Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"29d45bc2-da82-49ee-b0b7-382b820f9e28","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["eaf1526970d94c5c9134213a2337fa94\n","[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n","[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[1]\tvalid_0's quantile: 0.073891\n","[2]\tvalid_0's quantile: 0.0720418\n","[3]\tvalid_0's quantile: 0.070186\n","[4]\tvalid_0's quantile: 0.0687681\n","[5]\tvalid_0's quantile: 0.0676141\n","[6]\tvalid_0's quantile: 0.0663061\n","[7]\tvalid_0's quantile: 0.0651943\n","[8]\tvalid_0's quantile: 0.0641024\n","[9]\tvalid_0's quantile: 0.0631393\n","[10]\tvalid_0's quantile: 0.0622466\n","[11]\tvalid_0's quantile: 0.0614045\n","[12]\tvalid_0's quantile: 0.0606769\n","[13]\tvalid_0's quantile: 0.0600169\n","[14]\tvalid_0's quantile: 0.0595511\n","[15]\tvalid_0's quantile: 0.0589853\n","[16]\tvalid_0's quantile: 0.0583849\n","[17]\tvalid_0's quantile: 0.0578707\n","[18]\tvalid_0's quantile: 0.0570253\n","[19]\tvalid_0's quantile: 0.0565005\n","[20]\tvalid_0's quantile: 0.0561017\n","[21]\tvalid_0's quantile: 0.0556577\n","[22]\tvalid_0's quantile: 0.0552979\n","[23]\tvalid_0's quantile: 0.0548717\n","[24]\tvalid_0's quantile: 0.0542067\n","[25]\tvalid_0's quantile: 0.0538729\n","[26]\tvalid_0's quantile: 0.0534963\n","[27]\tvalid_0's quantile: 0.0530041\n","[28]\tvalid_0's quantile: 0.0528372\n","[29]\tvalid_0's quantile: 0.0523372\n","[30]\tvalid_0's quantile: 0.0520879\n","[31]\tvalid_0's quantile: 0.0522527\n","[32]\tvalid_0's quantile: 0.0516622\n","[33]\tvalid_0's quantile: 0.0512615\n","[34]\tvalid_0's quantile: 0.0509556\n","[35]\tvalid_0's quantile: 0.050403\n","[36]\tvalid_0's quantile: 0.0501964\n","[37]\tvalid_0's quantile: 0.0500022\n","[38]\tvalid_0's quantile: 0.0498239\n","[39]\tvalid_0's quantile: 0.0494407\n","[40]\tvalid_0's quantile: 0.0492253\n","[41]\tvalid_0's quantile: 0.048794\n","[42]\tvalid_0's quantile: 0.0485901\n","[43]\tvalid_0's quantile: 0.0482876\n","[44]\tvalid_0's quantile: 0.0479052\n","[45]\tvalid_0's quantile: 0.0477117\n","[46]\tvalid_0's quantile: 0.0474616\n","[47]\tvalid_0's quantile: 0.047209\n","[48]\tvalid_0's quantile: 0.047107\n","[49]\tvalid_0's quantile: 0.0467512\n","[50]\tvalid_0's quantile: 0.046531\n","[51]\tvalid_0's quantile: 0.046254\n","[52]\tvalid_0's quantile: 0.0467046\n","[53]\tvalid_0's quantile: 0.0465384\n","[54]\tvalid_0's quantile: 0.0462431\n","[55]\tvalid_0's quantile: 0.0459602\n","[56]\tvalid_0's quantile: 0.0458257\n","[57]\tvalid_0's quantile: 0.0454786\n","[58]\tvalid_0's quantile: 0.0451537\n","[59]\tvalid_0's quantile: 0.0449963\n","[60]\tvalid_0's quantile: 0.0447554\n","[61]\tvalid_0's quantile: 0.0446315\n","[62]\tvalid_0's quantile: 0.0444068\n","[63]\tvalid_0's quantile: 0.0442008\n","[64]\tvalid_0's quantile: 0.0439336\n","[65]\tvalid_0's quantile: 0.0437424\n","[66]\tvalid_0's quantile: 0.0436687\n","[67]\tvalid_0's quantile: 0.0435794\n","[68]\tvalid_0's quantile: 0.0433433\n","[69]\tvalid_0's quantile: 0.04311\n","[70]\tvalid_0's quantile: 0.0429656\n","[71]\tvalid_0's quantile: 0.0428179\n","[72]\tvalid_0's quantile: 0.0426097\n","[73]\tvalid_0's quantile: 0.0424776\n","[74]\tvalid_0's quantile: 0.0426511\n","[75]\tvalid_0's quantile: 0.0425522\n","[76]\tvalid_0's quantile: 0.042321\n","[77]\tvalid_0's quantile: 0.0422217\n","[78]\tvalid_0's quantile: 0.0421359\n","[79]\tvalid_0's quantile: 0.0418577\n","[80]\tvalid_0's quantile: 0.0417303\n","[81]\tvalid_0's quantile: 0.0415587\n","[82]\tvalid_0's quantile: 0.0413235\n","[83]\tvalid_0's quantile: 0.0410807\n","[84]\tvalid_0's quantile: 0.0408904\n","[85]\tvalid_0's quantile: 0.0407574\n","[86]\tvalid_0's quantile: 0.0406671\n","[87]\tvalid_0's quantile: 0.0405739\n","[88]\tvalid_0's quantile: 0.040502\n","[89]\tvalid_0's quantile: 0.04031\n","[90]\tvalid_0's quantile: 0.0402571\n","[91]\tvalid_0's quantile: 0.0409969\n","[92]\tvalid_0's quantile: 0.0408049\n","[93]\tvalid_0's quantile: 0.040681\n","[94]\tvalid_0's quantile: 0.0405917\n","[95]\tvalid_0's quantile: 0.0405439\n","[96]\tvalid_0's quantile: 0.0404085\n","[97]\tvalid_0's quantile: 0.0403523\n","[98]\tvalid_0's quantile: 0.0402408\n","[99]\tvalid_0's quantile: 0.0401162\n","[100]\tvalid_0's quantile: 0.0400511\n","[101]\tvalid_0's quantile: 0.0400171\n","[102]\tvalid_0's quantile: 0.0397818\n","[103]\tvalid_0's quantile: 0.039601\n","[104]\tvalid_0's quantile: 0.0394632\n","[105]\tvalid_0's quantile: 0.0393238\n","[106]\tvalid_0's quantile: 0.0392405\n","[107]\tvalid_0's quantile: 0.0392572\n","[108]\tvalid_0's quantile: 0.0391733\n","[109]\tvalid_0's quantile: 0.0390405\n","[110]\tvalid_0's quantile: 0.0390364\n","[111]\tvalid_0's quantile: 0.0389843\n","[112]\tvalid_0's quantile: 0.0389573\n","[113]\tvalid_0's quantile: 0.0388133\n","[114]\tvalid_0's quantile: 0.0386695\n","[115]\tvalid_0's quantile: 0.0385705\n","[116]\tvalid_0's quantile: 0.0386617\n","[117]\tvalid_0's quantile: 0.0385273\n","[118]\tvalid_0's quantile: 0.0384561\n","[119]\tvalid_0's quantile: 0.0385836\n","[120]\tvalid_0's quantile: 0.0385414\n","[121]\tvalid_0's quantile: 0.0384353\n","[122]\tvalid_0's quantile: 0.0382941\n","[123]\tvalid_0's quantile: 0.0381183\n","[124]\tvalid_0's quantile: 0.0379334\n","[125]\tvalid_0's quantile: 0.0378109\n","[126]\tvalid_0's quantile: 0.0377245\n","[127]\tvalid_0's quantile: 0.0377385\n","[128]\tvalid_0's quantile: 0.0375824\n","[129]\tvalid_0's quantile: 0.0374728\n","[130]\tvalid_0's quantile: 0.0373706\n","[131]\tvalid_0's quantile: 0.0373583\n","[132]\tvalid_0's quantile: 0.0372266\n","[133]\tvalid_0's quantile: 0.0371809\n","[134]\tvalid_0's quantile: 0.0370633\n","[135]\tvalid_0's quantile: 0.0369722\n","[136]\tvalid_0's quantile: 0.0368258\n","[137]\tvalid_0's quantile: 0.0367418\n","[138]\tvalid_0's quantile: 0.0366496\n","[139]\tvalid_0's quantile: 0.0365967\n","[140]\tvalid_0's quantile: 0.0365884\n","[141]\tvalid_0's quantile: 0.036606\n","[142]\tvalid_0's quantile: 0.036486\n","[143]\tvalid_0's quantile: 0.0363526\n","[144]\tvalid_0's quantile: 0.0363255\n","[145]\tvalid_0's quantile: 0.036231\n","[146]\tvalid_0's quantile: 0.036176\n","[147]\tvalid_0's quantile: 0.0361026\n","[148]\tvalid_0's quantile: 0.0359914\n","[149]\tvalid_0's quantile: 0.035986\n","[150]\tvalid_0's quantile: 0.0358947\n","[151]\tvalid_0's quantile: 0.0358063\n","[152]\tvalid_0's quantile: 0.0357331\n","[153]\tvalid_0's quantile: 0.0357152\n","[154]\tvalid_0's quantile: 0.035641\n","[155]\tvalid_0's quantile: 0.0355342\n","[156]\tvalid_0's quantile: 0.0354907\n","[157]\tvalid_0's quantile: 0.0353884\n","[158]\tvalid_0's quantile: 0.0353258\n","[159]\tvalid_0's quantile: 0.0352925\n","[160]\tvalid_0's quantile: 0.0352184\n","[161]\tvalid_0's quantile: 0.0351612\n","[162]\tvalid_0's quantile: 0.0350809\n","[163]\tvalid_0's quantile: 0.0349533\n","[164]\tvalid_0's quantile: 0.0348409\n","[165]\tvalid_0's quantile: 0.0347814\n","[166]\tvalid_0's quantile: 0.0347779\n","[167]\tvalid_0's quantile: 0.0347018\n","[168]\tvalid_0's quantile: 0.0346027\n","[169]\tvalid_0's quantile: 0.0344926\n","[170]\tvalid_0's quantile: 0.0344126\n","[171]\tvalid_0's quantile: 0.0344043\n","[172]\tvalid_0's quantile: 0.0343057\n","[173]\tvalid_0's quantile: 0.0342427\n","[174]\tvalid_0's quantile: 0.0341786\n","[175]\tvalid_0's quantile: 0.0340948\n","[176]\tvalid_0's quantile: 0.0339918\n","[177]\tvalid_0's quantile: 0.0338972\n","[178]\tvalid_0's quantile: 0.0338004\n","[179]\tvalid_0's quantile: 0.0337917\n","[180]\tvalid_0's quantile: 0.0336974\n","[181]\tvalid_0's quantile: 0.0336478\n","[182]\tvalid_0's quantile: 0.0336142\n","[183]\tvalid_0's quantile: 0.0335607\n","[184]\tvalid_0's quantile: 0.0334763\n","[185]\tvalid_0's quantile: 0.033439\n","[186]\tvalid_0's quantile: 0.0333801\n","[187]\tvalid_0's quantile: 0.0332805\n","[188]\tvalid_0's quantile: 0.0332075\n","[189]\tvalid_0's quantile: 0.0331175\n","[190]\tvalid_0's quantile: 0.0330757\n","[191]\tvalid_0's quantile: 0.033033\n","[192]\tvalid_0's quantile: 0.0330022\n","[193]\tvalid_0's quantile: 0.0329484\n","[194]\tvalid_0's quantile: 0.0329281\n","[195]\tvalid_0's quantile: 0.0329077\n","[196]\tvalid_0's quantile: 0.0328834\n","[197]\tvalid_0's quantile: 0.0327876\n","[198]\tvalid_0's quantile: 0.0327257\n","[199]\tvalid_0's quantile: 0.032629\n","[200]\tvalid_0's quantile: 0.0326177\n","[201]\tvalid_0's quantile: 0.0325018\n","[202]\tvalid_0's quantile: 0.0324215\n","[203]\tvalid_0's quantile: 0.0323997\n","[204]\tvalid_0's quantile: 0.0323453\n","[205]\tvalid_0's quantile: 0.0322509\n","[206]\tvalid_0's quantile: 0.0321866\n","[207]\tvalid_0's quantile: 0.0321122\n","[208]\tvalid_0's quantile: 0.0319621\n","[209]\tvalid_0's quantile: 0.0319244\n","[210]\tvalid_0's quantile: 0.0318844\n","[211]\tvalid_0's quantile: 0.0318155\n","[212]\tvalid_0's quantile: 0.031784\n","[213]\tvalid_0's quantile: 0.0317764\n","[214]\tvalid_0's quantile: 0.0317476\n","[215]\tvalid_0's quantile: 0.0317491\n","[216]\tvalid_0's quantile: 0.031723\n","[217]\tvalid_0's quantile: 0.0316738\n","[218]\tvalid_0's quantile: 0.0316568\n","[219]\tvalid_0's quantile: 0.031659\n","[220]\tvalid_0's quantile: 0.0316248\n","[221]\tvalid_0's quantile: 0.0315851\n","[222]\tvalid_0's quantile: 0.0315805\n","[223]\tvalid_0's quantile: 0.0314932\n","[224]\tvalid_0's quantile: 0.0314382\n","[225]\tvalid_0's quantile: 0.0313806\n","[226]\tvalid_0's quantile: 0.0313437\n","[227]\tvalid_0's quantile: 0.0312864\n","[228]\tvalid_0's quantile: 0.0312432\n","[229]\tvalid_0's quantile: 0.0312167\n","[230]\tvalid_0's quantile: 0.0312054\n","[231]\tvalid_0's quantile: 0.0311854\n","[232]\tvalid_0's quantile: 0.0311524\n","[233]\tvalid_0's quantile: 0.0311624\n","[234]\tvalid_0's quantile: 0.0311062\n","[235]\tvalid_0's quantile: 0.0310748\n","[236]\tvalid_0's quantile: 0.0310218\n","[237]\tvalid_0's quantile: 0.0309709\n","[238]\tvalid_0's quantile: 0.0309454\n","[239]\tvalid_0's quantile: 0.0309055\n","[240]\tvalid_0's quantile: 0.0308412\n","[241]\tvalid_0's quantile: 0.0308474\n","[242]\tvalid_0's quantile: 0.0308061\n","[243]\tvalid_0's quantile: 0.0307792\n","[244]\tvalid_0's quantile: 0.0307495\n","[245]\tvalid_0's quantile: 0.0306582\n","[246]\tvalid_0's quantile: 0.0306422\n","[247]\tvalid_0's quantile: 0.0306306\n","[248]\tvalid_0's quantile: 0.0306198\n","[249]\tvalid_0's quantile: 0.0305312\n","[250]\tvalid_0's quantile: 0.0304682\n","[251]\tvalid_0's quantile: 0.0304024\n","[252]\tvalid_0's quantile: 0.030536\n","[253]\tvalid_0's quantile: 0.0305114\n","[254]\tvalid_0's quantile: 0.0306332\n","[255]\tvalid_0's quantile: 0.0305724\n","[256]\tvalid_0's quantile: 0.0305314\n","[257]\tvalid_0's quantile: 0.0305205\n","[258]\tvalid_0's quantile: 0.0304829\n","[259]\tvalid_0's quantile: 0.0304241\n","[260]\tvalid_0's quantile: 0.0303803\n","[261]\tvalid_0's quantile: 0.0303712\n","[262]\tvalid_0's quantile: 0.0303395\n","[263]\tvalid_0's quantile: 0.0302879\n","[264]\tvalid_0's quantile: 0.0302506\n","[265]\tvalid_0's quantile: 0.0302151\n","[266]\tvalid_0's quantile: 0.0301813\n","[267]\tvalid_0's quantile: 0.0301483\n","[268]\tvalid_0's quantile: 0.0301321\n","[269]\tvalid_0's quantile: 0.0301142\n","[270]\tvalid_0's quantile: 0.0300921\n","[271]\tvalid_0's quantile: 0.0300446\n","[272]\tvalid_0's quantile: 0.0300189\n","[273]\tvalid_0's quantile: 0.0300083\n","[274]\tvalid_0's quantile: 0.0299985\n","[275]\tvalid_0's quantile: 0.0299636\n","[276]\tvalid_0's quantile: 0.0299477\n","[277]\tvalid_0's quantile: 0.0299243\n","[278]\tvalid_0's quantile: 0.0299099\n","[279]\tvalid_0's quantile: 0.0298651\n","[280]\tvalid_0's quantile: 0.0298031\n","[281]\tvalid_0's quantile: 0.0298159\n","[282]\tvalid_0's quantile: 0.0298065\n","[283]\tvalid_0's quantile: 0.0297815\n","[284]\tvalid_0's quantile: 0.0297712\n","[285]\tvalid_0's quantile: 0.0297565\n","[286]\tvalid_0's quantile: 0.0297444\n","[287]\tvalid_0's quantile: 0.0297389\n","[288]\tvalid_0's quantile: 0.029721\n","[289]\tvalid_0's quantile: 0.0296789\n","[290]\tvalid_0's quantile: 0.0296683\n","[291]\tvalid_0's quantile: 0.0296164\n","[292]\tvalid_0's quantile: 0.029583\n","[293]\tvalid_0's quantile: 0.0295858\n","[294]\tvalid_0's quantile: 0.0295308\n","[295]\tvalid_0's quantile: 0.0295343\n","[296]\tvalid_0's quantile: 0.0294966\n","[297]\tvalid_0's quantile: 0.02948\n","[298]\tvalid_0's quantile: 0.0293955\n","[299]\tvalid_0's quantile: 0.0293583\n","[300]\tvalid_0's quantile: 0.0293369\n","[301]\tvalid_0's quantile: 0.0293234\n","[302]\tvalid_0's quantile: 0.0292998\n","[303]\tvalid_0's quantile: 0.0292805\n","[304]\tvalid_0's quantile: 0.0292674\n","[305]\tvalid_0's quantile: 0.0292624\n","[306]\tvalid_0's quantile: 0.0292567\n","[307]\tvalid_0's quantile: 0.0292206\n","[308]\tvalid_0's quantile: 0.0291784\n","[309]\tvalid_0's quantile: 0.0291582\n","[310]\tvalid_0's quantile: 0.0291116\n","[311]\tvalid_0's quantile: 0.029089\n","[312]\tvalid_0's quantile: 0.0290658\n","[313]\tvalid_0's quantile: 0.0290257\n","[314]\tvalid_0's quantile: 0.0290212\n","[315]\tvalid_0's quantile: 0.0289913\n","[316]\tvalid_0's quantile: 0.0289521\n","[317]\tvalid_0's quantile: 0.0289155\n","[318]\tvalid_0's quantile: 0.0289108\n","[319]\tvalid_0's quantile: 0.0289027\n","[320]\tvalid_0's quantile: 0.02889\n","[321]\tvalid_0's quantile: 0.0288441\n","[322]\tvalid_0's quantile: 0.0288388\n","[323]\tvalid_0's quantile: 0.0288308\n","[324]\tvalid_0's quantile: 0.0288103\n","[325]\tvalid_0's quantile: 0.028798\n","[326]\tvalid_0's quantile: 0.0287608\n","[327]\tvalid_0's quantile: 0.0287513\n","[328]\tvalid_0's quantile: 0.0287372\n","[329]\tvalid_0's quantile: 0.0286951\n","[330]\tvalid_0's quantile: 0.0286743\n","[331]\tvalid_0's quantile: 0.0286413\n","[332]\tvalid_0's quantile: 0.0286462\n","[333]\tvalid_0's quantile: 0.0286318\n","[334]\tvalid_0's quantile: 0.0286089\n","[335]\tvalid_0's quantile: 0.0286119\n","[336]\tvalid_0's quantile: 0.0286002\n","[337]\tvalid_0's quantile: 0.0285818\n","[338]\tvalid_0's quantile: 0.0285331\n","[339]\tvalid_0's quantile: 0.0285268\n","[340]\tvalid_0's quantile: 0.0285312\n","[341]\tvalid_0's quantile: 0.0285058\n","[342]\tvalid_0's quantile: 0.0284675\n","[343]\tvalid_0's quantile: 0.0284229\n","[344]\tvalid_0's quantile: 0.0283913\n","[345]\tvalid_0's quantile: 0.0283487\n","[346]\tvalid_0's quantile: 0.0283297\n","[347]\tvalid_0's quantile: 0.0283205\n","[348]\tvalid_0's quantile: 0.0283269\n","[349]\tvalid_0's quantile: 0.0283038\n","[350]\tvalid_0's quantile: 0.0282757\n","[351]\tvalid_0's quantile: 0.0282539\n","[352]\tvalid_0's quantile: 0.0282378\n","[353]\tvalid_0's quantile: 0.0282313\n","[354]\tvalid_0's quantile: 0.0281972\n","[355]\tvalid_0's quantile: 0.0281981\n","[356]\tvalid_0's quantile: 0.0281765\n","[357]\tvalid_0's quantile: 0.0281647\n","[358]\tvalid_0's quantile: 0.0281464\n","[359]\tvalid_0's quantile: 0.0281484\n","[360]\tvalid_0's quantile: 0.0281143\n","[361]\tvalid_0's quantile: 0.0280854\n","[362]\tvalid_0's quantile: 0.0280726\n","[363]\tvalid_0's quantile: 0.0280627\n","[364]\tvalid_0's quantile: 0.0280557\n","[365]\tvalid_0's quantile: 0.0280443\n","[366]\tvalid_0's quantile: 0.0280357\n","[367]\tvalid_0's quantile: 0.0280271\n","[368]\tvalid_0's quantile: 0.0280086\n","[369]\tvalid_0's quantile: 0.0279976\n","[370]\tvalid_0's quantile: 0.0279935\n","[371]\tvalid_0's quantile: 0.0279904\n","[372]\tvalid_0's quantile: 0.0279903\n","[373]\tvalid_0's quantile: 0.02796\n","[374]\tvalid_0's quantile: 0.0279433\n","[375]\tvalid_0's quantile: 0.0279451\n","[376]\tvalid_0's quantile: 0.0279143\n","[377]\tvalid_0's quantile: 0.0279109\n","[378]\tvalid_0's quantile: 0.0279132\n","[379]\tvalid_0's quantile: 0.0279135\n","[380]\tvalid_0's quantile: 0.0278989\n","[381]\tvalid_0's quantile: 0.0278904\n","[382]\tvalid_0's quantile: 0.0278708\n","[383]\tvalid_0's quantile: 0.0278703\n","[384]\tvalid_0's quantile: 0.0278136\n","[385]\tvalid_0's quantile: 0.027813\n","[386]\tvalid_0's quantile: 0.0277861\n","[387]\tvalid_0's quantile: 0.0277807\n","[388]\tvalid_0's quantile: 0.0277728\n","[389]\tvalid_0's quantile: 0.0277412\n","[390]\tvalid_0's quantile: 0.0277063\n","[391]\tvalid_0's quantile: 0.0276994\n","[392]\tvalid_0's quantile: 0.0276866\n","[393]\tvalid_0's quantile: 0.0276562\n","[394]\tvalid_0's quantile: 0.027648\n","[395]\tvalid_0's quantile: 0.0276464\n","[396]\tvalid_0's quantile: 0.0276232\n","[397]\tvalid_0's quantile: 0.0276226\n","[398]\tvalid_0's quantile: 0.0276159\n","[399]\tvalid_0's quantile: 0.0275998\n","[400]\tvalid_0's quantile: 0.0275777\n","[401]\tvalid_0's quantile: 0.0275739\n","[402]\tvalid_0's quantile: 0.0275733\n","[403]\tvalid_0's quantile: 0.0275581\n","[404]\tvalid_0's quantile: 0.0275342\n","[405]\tvalid_0's quantile: 0.027529\n","[406]\tvalid_0's quantile: 0.0275166\n","[407]\tvalid_0's quantile: 0.0275204\n","[408]\tvalid_0's quantile: 0.0275052\n","[409]\tvalid_0's quantile: 0.0275054\n","[410]\tvalid_0's quantile: 0.0274857\n","[411]\tvalid_0's quantile: 0.027485\n","[412]\tvalid_0's quantile: 0.0274677\n","[413]\tvalid_0's quantile: 0.0274786\n","[414]\tvalid_0's quantile: 0.0274542\n","[415]\tvalid_0's quantile: 0.0274364\n","[416]\tvalid_0's quantile: 0.0274353\n","[417]\tvalid_0's quantile: 0.0274357\n","[418]\tvalid_0's quantile: 0.0274216\n","[419]\tvalid_0's quantile: 0.027422\n","[420]\tvalid_0's quantile: 0.0274074\n","[421]\tvalid_0's quantile: 0.0274027\n","[422]\tvalid_0's quantile: 0.027433\n","[423]\tvalid_0's quantile: 0.0274151\n","[424]\tvalid_0's quantile: 0.0274057\n","[425]\tvalid_0's quantile: 0.0274046\n","[426]\tvalid_0's quantile: 0.0274015\n","[427]\tvalid_0's quantile: 0.0273852\n","[428]\tvalid_0's quantile: 0.0273717\n","[429]\tvalid_0's quantile: 0.0273871\n","[430]\tvalid_0's quantile: 0.0273869\n","[431]\tvalid_0's quantile: 0.027352\n","[432]\tvalid_0's quantile: 0.0273504\n","[433]\tvalid_0's quantile: 0.0273496\n","[434]\tvalid_0's quantile: 0.0273258\n","[435]\tvalid_0's quantile: 0.0273218\n","[436]\tvalid_0's quantile: 0.0273116\n","[437]\tvalid_0's quantile: 0.0273105\n","[438]\tvalid_0's quantile: 0.0273075\n","[439]\tvalid_0's quantile: 0.0272876\n","[440]\tvalid_0's quantile: 0.027286\n","[441]\tvalid_0's quantile: 0.0272781\n","[442]\tvalid_0's quantile: 0.0272753\n","[443]\tvalid_0's quantile: 0.0272674\n","[444]\tvalid_0's quantile: 0.0272599\n","[445]\tvalid_0's quantile: 0.0272604\n","[446]\tvalid_0's quantile: 0.0272472\n","[447]\tvalid_0's quantile: 0.0272209\n","[448]\tvalid_0's quantile: 0.027219\n","[449]\tvalid_0's quantile: 0.0271979\n","[450]\tvalid_0's quantile: 0.0271651\n","[451]\tvalid_0's quantile: 0.0271609\n","[452]\tvalid_0's quantile: 0.0271403\n","[453]\tvalid_0's quantile: 0.0271249\n","[454]\tvalid_0's quantile: 0.0271233\n","[455]\tvalid_0's quantile: 0.0270821\n","[456]\tvalid_0's quantile: 0.0270556\n","[457]\tvalid_0's quantile: 0.0270329\n","[458]\tvalid_0's quantile: 0.0270202\n","[459]\tvalid_0's quantile: 0.0269632\n","[460]\tvalid_0's quantile: 0.026951\n","[461]\tvalid_0's quantile: 0.0269458\n","[462]\tvalid_0's quantile: 0.026947\n","[463]\tvalid_0's quantile: 0.0269426\n","[464]\tvalid_0's quantile: 0.0269291\n","[465]\tvalid_0's quantile: 0.0269072\n","[466]\tvalid_0's quantile: 0.0269013\n","[467]\tvalid_0's quantile: 0.0269\n","[468]\tvalid_0's quantile: 0.0268888\n","[469]\tvalid_0's quantile: 0.0268804\n","[470]\tvalid_0's quantile: 0.0268511\n","[471]\tvalid_0's quantile: 0.0268429\n","[472]\tvalid_0's quantile: 0.0268319\n","[473]\tvalid_0's quantile: 0.0268266\n","[474]\tvalid_0's quantile: 0.0268142\n","[475]\tvalid_0's quantile: 0.0268132\n","[476]\tvalid_0's quantile: 0.0267962\n","[477]\tvalid_0's quantile: 0.0267854\n","[478]\tvalid_0's quantile: 0.0267617\n","[479]\tvalid_0's quantile: 0.0267489\n","[480]\tvalid_0's quantile: 0.0267241\n","[481]\tvalid_0's quantile: 0.0267198\n","[482]\tvalid_0's quantile: 0.0266994\n","[483]\tvalid_0's quantile: 0.0266951\n","[484]\tvalid_0's quantile: 0.0266957\n","[485]\tvalid_0's quantile: 0.0266913\n","[486]\tvalid_0's quantile: 0.0266718\n","[487]\tvalid_0's quantile: 0.0266718\n","[488]\tvalid_0's quantile: 0.0266704\n","[489]\tvalid_0's quantile: 0.0266697\n","[490]\tvalid_0's quantile: 0.0266697\n","[491]\tvalid_0's quantile: 0.0266695\n","[492]\tvalid_0's quantile: 0.0266566\n","[493]\tvalid_0's quantile: 0.0266491\n","[494]\tvalid_0's quantile: 0.02663\n","[495]\tvalid_0's quantile: 0.0266304\n","[496]\tvalid_0's quantile: 0.0266056\n","[497]\tvalid_0's quantile: 0.0265995\n","[498]\tvalid_0's quantile: 0.0265979\n","[499]\tvalid_0's quantile: 0.0265981\n","[500]\tvalid_0's quantile: 0.026595\n","[501]\tvalid_0's quantile: 0.026585\n","[502]\tvalid_0's quantile: 0.0265794\n","[503]\tvalid_0's quantile: 0.0265809\n","[504]\tvalid_0's quantile: 0.026568\n","[505]\tvalid_0's quantile: 0.0265599\n","[506]\tvalid_0's quantile: 0.0265578\n","[507]\tvalid_0's quantile: 0.02654\n","[508]\tvalid_0's quantile: 0.0265371\n","[509]\tvalid_0's quantile: 0.0265187\n","[510]\tvalid_0's quantile: 0.0265182\n","[511]\tvalid_0's quantile: 0.0265158\n","[512]\tvalid_0's quantile: 0.026512\n","[513]\tvalid_0's quantile: 0.026509\n","[514]\tvalid_0's quantile: 0.0265102\n","[515]\tvalid_0's quantile: 0.0265079\n","[516]\tvalid_0's quantile: 0.0265027\n","[517]\tvalid_0's quantile: 0.0265031\n","[518]\tvalid_0's quantile: 0.0264996\n","[519]\tvalid_0's quantile: 0.0264965\n","[520]\tvalid_0's quantile: 0.0264992\n","[521]\tvalid_0's quantile: 0.0264927\n","[522]\tvalid_0's quantile: 0.0264986\n","[523]\tvalid_0's quantile: 0.0264955\n","[524]\tvalid_0's quantile: 0.0264874\n","[525]\tvalid_0's quantile: 0.026476\n","[526]\tvalid_0's quantile: 0.0264733\n","[527]\tvalid_0's quantile: 0.026473\n","[528]\tvalid_0's quantile: 0.0264554\n","[529]\tvalid_0's quantile: 0.0264554\n","[530]\tvalid_0's quantile: 0.0264439\n","[531]\tvalid_0's quantile: 0.0264261\n","[532]\tvalid_0's quantile: 0.0264118\n","[533]\tvalid_0's quantile: 0.0263992\n","[534]\tvalid_0's quantile: 0.0263743\n","[535]\tvalid_0's quantile: 0.0263563\n","[536]\tvalid_0's quantile: 0.0263572\n","[537]\tvalid_0's quantile: 0.0263575\n","[538]\tvalid_0's quantile: 0.026338\n","[539]\tvalid_0's quantile: 0.026333\n","[540]\tvalid_0's quantile: 0.0263296\n","[541]\tvalid_0's quantile: 0.0263212\n","[542]\tvalid_0's quantile: 0.0263109\n","[543]\tvalid_0's quantile: 0.0263111\n","[544]\tvalid_0's quantile: 0.0263057\n","[545]\tvalid_0's quantile: 0.0262954\n","[546]\tvalid_0's quantile: 0.0262926\n","[547]\tvalid_0's quantile: 0.0262908\n","[548]\tvalid_0's quantile: 0.0262802\n","[549]\tvalid_0's quantile: 0.0262619\n","[550]\tvalid_0's quantile: 0.0262617\n","[551]\tvalid_0's quantile: 0.0262576\n","[552]\tvalid_0's quantile: 0.0262493\n","[553]\tvalid_0's quantile: 0.0262427\n","[554]\tvalid_0's quantile: 0.0262355\n","[555]\tvalid_0's quantile: 0.0262357\n","[556]\tvalid_0's quantile: 0.0262386\n","[557]\tvalid_0's quantile: 0.0262255\n","[558]\tvalid_0's quantile: 0.0262032\n","[559]\tvalid_0's quantile: 0.0261887\n","[560]\tvalid_0's quantile: 0.0261817\n","[561]\tvalid_0's quantile: 0.026174\n","[562]\tvalid_0's quantile: 0.0261681\n","[563]\tvalid_0's quantile: 0.0261684\n","[564]\tvalid_0's quantile: 0.0261627\n","[565]\tvalid_0's quantile: 0.0261631\n","[566]\tvalid_0's quantile: 0.0261415\n","[567]\tvalid_0's quantile: 0.0261199\n","[568]\tvalid_0's quantile: 0.0261196\n","[569]\tvalid_0's quantile: 0.026103\n","[570]\tvalid_0's quantile: 0.0261004\n","[571]\tvalid_0's quantile: 0.02608\n","[572]\tvalid_0's quantile: 0.0260584\n","[573]\tvalid_0's quantile: 0.0260491\n","[574]\tvalid_0's quantile: 0.0260355\n","[575]\tvalid_0's quantile: 0.026036\n","[576]\tvalid_0's quantile: 0.0260284\n","[577]\tvalid_0's quantile: 0.0260259\n","[578]\tvalid_0's quantile: 0.0260262\n","[579]\tvalid_0's quantile: 0.0260077\n","[580]\tvalid_0's quantile: 0.0260046\n","[581]\tvalid_0's quantile: 0.0259971\n","[582]\tvalid_0's quantile: 0.0259975\n","[583]\tvalid_0's quantile: 0.0259759\n","[584]\tvalid_0's quantile: 0.0259725\n","[585]\tvalid_0's quantile: 0.025953\n","[586]\tvalid_0's quantile: 0.0259427\n","[587]\tvalid_0's quantile: 0.0259346\n","[588]\tvalid_0's quantile: 0.025928\n","[589]\tvalid_0's quantile: 0.0259263\n","[590]\tvalid_0's quantile: 0.0259184\n","[591]\tvalid_0's quantile: 0.0259168\n","[592]\tvalid_0's quantile: 0.0259171\n","[593]\tvalid_0's quantile: 0.0259093\n","[594]\tvalid_0's quantile: 0.0259055\n","[595]\tvalid_0's quantile: 0.025903\n","[596]\tvalid_0's quantile: 0.0259\n","[597]\tvalid_0's quantile: 0.0258892\n","[598]\tvalid_0's quantile: 0.0258892\n","[599]\tvalid_0's quantile: 0.0258891\n","[600]\tvalid_0's quantile: 0.0258809\n","[601]\tvalid_0's quantile: 0.0258708\n","[602]\tvalid_0's quantile: 0.025877\n","[603]\tvalid_0's quantile: 0.0258879\n","[604]\tvalid_0's quantile: 0.0258756\n","[605]\tvalid_0's quantile: 0.0258712\n","[606]\tvalid_0's quantile: 0.0258534\n","[607]\tvalid_0's quantile: 0.0258663\n","[608]\tvalid_0's quantile: 0.0258465\n","[609]\tvalid_0's quantile: 0.0258473\n","[610]\tvalid_0's quantile: 0.0258435\n","[611]\tvalid_0's quantile: 0.0258316\n","[612]\tvalid_0's quantile: 0.0258302\n","[613]\tvalid_0's quantile: 0.0258205\n","[614]\tvalid_0's quantile: 0.0258186\n","[615]\tvalid_0's quantile: 0.0258196\n","[616]\tvalid_0's quantile: 0.0258099\n","[617]\tvalid_0's quantile: 0.0258169\n","[618]\tvalid_0's quantile: 0.0258137\n","[619]\tvalid_0's quantile: 0.0258139\n","[620]\tvalid_0's quantile: 0.0258108\n","[621]\tvalid_0's quantile: 0.0258055\n","[622]\tvalid_0's quantile: 0.0257793\n","[623]\tvalid_0's quantile: 0.0258098\n","[624]\tvalid_0's quantile: 0.0258075\n","[625]\tvalid_0's quantile: 0.0258072\n","[626]\tvalid_0's quantile: 0.0258059\n","[627]\tvalid_0's quantile: 0.0258166\n","[628]\tvalid_0's quantile: 0.0257997\n","[629]\tvalid_0's quantile: 0.0257842\n","[630]\tvalid_0's quantile: 0.0257755\n","[631]\tvalid_0's quantile: 0.0257703\n","[632]\tvalid_0's quantile: 0.0257706\n","[633]\tvalid_0's quantile: 0.0257702\n","[634]\tvalid_0's quantile: 0.025768\n","[635]\tvalid_0's quantile: 0.0257676\n","[636]\tvalid_0's quantile: 0.0257618\n","[637]\tvalid_0's quantile: 0.0257619\n","[638]\tvalid_0's quantile: 0.025769\n","[639]\tvalid_0's quantile: 0.0257676\n","[640]\tvalid_0's quantile: 0.0257676\n","[641]\tvalid_0's quantile: 0.0257655\n","[642]\tvalid_0's quantile: 0.0257656\n","[643]\tvalid_0's quantile: 0.0257506\n","[644]\tvalid_0's quantile: 0.0257495\n","[645]\tvalid_0's quantile: 0.0257481\n","[646]\tvalid_0's quantile: 0.0257201\n","[647]\tvalid_0's quantile: 0.0257243\n","[648]\tvalid_0's quantile: 0.0257087\n","[649]\tvalid_0's quantile: 0.0257141\n","[650]\tvalid_0's quantile: 0.025697\n","[651]\tvalid_0's quantile: 0.0256947\n","[652]\tvalid_0's quantile: 0.0256951\n","[653]\tvalid_0's quantile: 0.0256815\n","[654]\tvalid_0's quantile: 0.0256686\n","[655]\tvalid_0's quantile: 0.0256688\n","[656]\tvalid_0's quantile: 0.0256697\n","[657]\tvalid_0's quantile: 0.0256698\n","[658]\tvalid_0's quantile: 0.0256596\n","[659]\tvalid_0's quantile: 0.0256603\n","[660]\tvalid_0's quantile: 0.0256583\n","[661]\tvalid_0's quantile: 0.0256591\n","[662]\tvalid_0's quantile: 0.0256613\n","[663]\tvalid_0's quantile: 0.0256551\n","[664]\tvalid_0's quantile: 0.0256549\n","[665]\tvalid_0's quantile: 0.0256839\n","[666]\tvalid_0's quantile: 0.0256699\n","[667]\tvalid_0's quantile: 0.0256667\n","[668]\tvalid_0's quantile: 0.0256585\n","[669]\tvalid_0's quantile: 0.0256607\n","[670]\tvalid_0's quantile: 0.0256399\n","[671]\tvalid_0's quantile: 0.0256401\n","[672]\tvalid_0's quantile: 0.0256398\n","[673]\tvalid_0's quantile: 0.0256402\n","[674]\tvalid_0's quantile: 0.0256364\n","[675]\tvalid_0's quantile: 0.0256334\n","[676]\tvalid_0's quantile: 0.0256283\n","[677]\tvalid_0's quantile: 0.0256256\n","[678]\tvalid_0's quantile: 0.0255948\n","[679]\tvalid_0's quantile: 0.0255806\n","[680]\tvalid_0's quantile: 0.0255776\n","[681]\tvalid_0's quantile: 0.0255764\n","[682]\tvalid_0's quantile: 0.0255676\n","[683]\tvalid_0's q\n","\n","*** WARNING: max output size exceeded, skipping output. ***\n","\n","0.0264371\n","[338]\tvalid_0's quantile: 0.0264167\n","[339]\tvalid_0's quantile: 0.0264068\n","[340]\tvalid_0's quantile: 0.0263993\n","[341]\tvalid_0's quantile: 0.0263813\n","[342]\tvalid_0's quantile: 0.0263623\n","[343]\tvalid_0's quantile: 0.0263623\n","[344]\tvalid_0's quantile: 0.0263513\n","[345]\tvalid_0's quantile: 0.0263322\n","[346]\tvalid_0's quantile: 0.0263276\n","[347]\tvalid_0's quantile: 0.026323\n","[348]\tvalid_0's quantile: 0.0263231\n","[349]\tvalid_0's quantile: 0.0263193\n","[350]\tvalid_0's quantile: 0.0262932\n","[351]\tvalid_0's quantile: 0.0262494\n","[352]\tvalid_0's quantile: 0.0262342\n","[353]\tvalid_0's quantile: 0.0261998\n","[354]\tvalid_0's quantile: 0.0261741\n","[355]\tvalid_0's quantile: 0.0261461\n","[356]\tvalid_0's quantile: 0.0261463\n","[357]\tvalid_0's quantile: 0.0261436\n","[358]\tvalid_0's quantile: 0.0261357\n","[359]\tvalid_0's quantile: 0.0261167\n","[360]\tvalid_0's quantile: 0.0261028\n","[361]\tvalid_0's quantile: 0.0260864\n","[362]\tvalid_0's quantile: 0.0260657\n","[363]\tvalid_0's quantile: 0.0260424\n","[364]\tvalid_0's quantile: 0.0260424\n","[365]\tvalid_0's quantile: 0.0260253\n","[366]\tvalid_0's quantile: 0.026021\n","[367]\tvalid_0's quantile: 0.0259876\n","[368]\tvalid_0's quantile: 0.0259738\n","[369]\tvalid_0's quantile: 0.0259775\n","[370]\tvalid_0's quantile: 0.0259627\n","[371]\tvalid_0's quantile: 0.0259628\n","[372]\tvalid_0's quantile: 0.0259564\n","[373]\tvalid_0's quantile: 0.0259565\n","[374]\tvalid_0's quantile: 0.0259233\n","[375]\tvalid_0's quantile: 0.0259165\n","[376]\tvalid_0's quantile: 0.0258935\n","[377]\tvalid_0's quantile: 0.0258855\n","[378]\tvalid_0's quantile: 0.025869\n","[379]\tvalid_0's quantile: 0.025845\n","[380]\tvalid_0's quantile: 0.025802\n","[381]\tvalid_0's quantile: 0.025774\n","[382]\tvalid_0's quantile: 0.0257681\n","[383]\tvalid_0's quantile: 0.0257191\n","[384]\tvalid_0's quantile: 0.0257192\n","[385]\tvalid_0's quantile: 0.0256757\n","[386]\tvalid_0's quantile: 0.0256557\n","[387]\tvalid_0's quantile: 0.0256556\n","[388]\tvalid_0's quantile: 0.0256471\n","[389]\tvalid_0's quantile: 0.0256325\n","[390]\tvalid_0's quantile: 0.0256125\n","[391]\tvalid_0's quantile: 0.0255941\n","[392]\tvalid_0's quantile: 0.0255559\n","[393]\tvalid_0's quantile: 0.0255553\n","[394]\tvalid_0's quantile: 0.0255347\n","[395]\tvalid_0's quantile: 0.0255111\n","[396]\tvalid_0's quantile: 0.0255007\n","[397]\tvalid_0's quantile: 0.025487\n","[398]\tvalid_0's quantile: 0.0254717\n","[399]\tvalid_0's quantile: 0.0254497\n","[400]\tvalid_0's quantile: 0.0254315\n","[401]\tvalid_0's quantile: 0.0254208\n","[402]\tvalid_0's quantile: 0.02541\n","[403]\tvalid_0's quantile: 0.0254171\n","[404]\tvalid_0's quantile: 0.0254007\n","[405]\tvalid_0's quantile: 0.0253915\n","[406]\tvalid_0's quantile: 0.025359\n","[407]\tvalid_0's quantile: 0.025356\n","[408]\tvalid_0's quantile: 0.025357\n","[409]\tvalid_0's quantile: 0.025342\n","[410]\tvalid_0's quantile: 0.0253345\n","[411]\tvalid_0's quantile: 0.0253332\n","[412]\tvalid_0's quantile: 0.0253215\n","[413]\tvalid_0's quantile: 0.025313\n","[414]\tvalid_0's quantile: 0.0253072\n","[415]\tvalid_0's quantile: 0.025301\n","[416]\tvalid_0's quantile: 0.0252916\n","[417]\tvalid_0's quantile: 0.0252899\n","[418]\tvalid_0's quantile: 0.0252843\n","[419]\tvalid_0's quantile: 0.0252843\n","[420]\tvalid_0's quantile: 0.0252799\n","[421]\tvalid_0's quantile: 0.0252736\n","[422]\tvalid_0's quantile: 0.0252523\n","[423]\tvalid_0's quantile: 0.0252481\n","[424]\tvalid_0's quantile: 0.0252225\n","[425]\tvalid_0's quantile: 0.0251878\n","[426]\tvalid_0's quantile: 0.0251883\n","[427]\tvalid_0's quantile: 0.0251841\n","[428]\tvalid_0's quantile: 0.0251803\n","[429]\tvalid_0's quantile: 0.02516\n","[430]\tvalid_0's quantile: 0.0251492\n","[431]\tvalid_0's quantile: 0.0251418\n","[432]\tvalid_0's quantile: 0.0251403\n","[433]\tvalid_0's quantile: 0.0251376\n","[434]\tvalid_0's quantile: 0.0251115\n","[435]\tvalid_0's quantile: 0.0251016\n","[436]\tvalid_0's quantile: 0.0250784\n","[437]\tvalid_0's quantile: 0.0250689\n","[438]\tvalid_0's quantile: 0.0250656\n","[439]\tvalid_0's quantile: 0.0250535\n","[440]\tvalid_0's quantile: 0.0250536\n","[441]\tvalid_0's quantile: 0.0250488\n","[442]\tvalid_0's quantile: 0.0250278\n","[443]\tvalid_0's quantile: 0.0250097\n","[444]\tvalid_0's quantile: 0.0249836\n","[445]\tvalid_0's quantile: 0.0249824\n","[446]\tvalid_0's quantile: 0.0249771\n","[447]\tvalid_0's quantile: 0.0249726\n","[448]\tvalid_0's quantile: 0.0249669\n","[449]\tvalid_0's quantile: 0.0249398\n","[450]\tvalid_0's quantile: 0.0249118\n","[451]\tvalid_0's quantile: 0.0249063\n","[452]\tvalid_0's quantile: 0.0248959\n","[453]\tvalid_0's quantile: 0.0248775\n","[454]\tvalid_0's quantile: 0.0248657\n","[455]\tvalid_0's quantile: 0.0248522\n","[456]\tvalid_0's quantile: 0.0248526\n","[457]\tvalid_0's quantile: 0.0248558\n","[458]\tvalid_0's quantile: 0.0248521\n","[459]\tvalid_0's quantile: 0.0248483\n","[460]\tvalid_0's quantile: 0.0248477\n","[461]\tvalid_0's quantile: 0.024846\n","[462]\tvalid_0's quantile: 0.0248386\n","[463]\tvalid_0's quantile: 0.0248312\n","[464]\tvalid_0's quantile: 0.0248283\n","[465]\tvalid_0's quantile: 0.024823\n","[466]\tvalid_0's quantile: 0.0248101\n","[467]\tvalid_0's quantile: 0.0248087\n","[468]\tvalid_0's quantile: 0.0248086\n","[469]\tvalid_0's quantile: 0.0248182\n","[470]\tvalid_0's quantile: 0.0248186\n","[471]\tvalid_0's quantile: 0.0248149\n","[472]\tvalid_0's quantile: 0.024778\n","[473]\tvalid_0's quantile: 0.0247759\n","[474]\tvalid_0's quantile: 0.0247751\n","[475]\tvalid_0's quantile: 0.0247739\n","[476]\tvalid_0's quantile: 0.0247684\n","[477]\tvalid_0's quantile: 0.0247606\n","[478]\tvalid_0's quantile: 0.024754\n","[479]\tvalid_0's quantile: 0.0247457\n","[480]\tvalid_0's quantile: 0.0247297\n","[481]\tvalid_0's quantile: 0.0247167\n","[482]\tvalid_0's quantile: 0.0246874\n","[483]\tvalid_0's quantile: 0.0246875\n","[484]\tvalid_0's quantile: 0.02468\n","[485]\tvalid_0's quantile: 0.0246724\n","[486]\tvalid_0's quantile: 0.024662\n","[487]\tvalid_0's quantile: 0.0246629\n","[488]\tvalid_0's quantile: 0.0246497\n","[489]\tvalid_0's quantile: 0.0246415\n","[490]\tvalid_0's quantile: 0.0246387\n","[491]\tvalid_0's quantile: 0.0246325\n","[492]\tvalid_0's quantile: 0.0246238\n","[493]\tvalid_0's quantile: 0.0246058\n","[494]\tvalid_0's quantile: 0.0246039\n","[495]\tvalid_0's quantile: 0.0245708\n","[496]\tvalid_0's quantile: 0.0245671\n","[497]\tvalid_0's quantile: 0.0245416\n","[498]\tvalid_0's quantile: 0.0245333\n","[499]\tvalid_0's quantile: 0.0245305\n","[500]\tvalid_0's quantile: 0.0245295\n","[501]\tvalid_0's quantile: 0.0245297\n","[502]\tvalid_0's quantile: 0.0245142\n","[503]\tvalid_0's quantile: 0.0245007\n","[504]\tvalid_0's quantile: 0.0244838\n","[505]\tvalid_0's quantile: 0.0244739\n","[506]\tvalid_0's quantile: 0.0244598\n","[507]\tvalid_0's quantile: 0.0244398\n","[508]\tvalid_0's quantile: 0.0244397\n","[509]\tvalid_0's quantile: 0.0244188\n","[510]\tvalid_0's quantile: 0.024413\n","[511]\tvalid_0's quantile: 0.0243969\n","[512]\tvalid_0's quantile: 0.0243859\n","[513]\tvalid_0's quantile: 0.0243749\n","[514]\tvalid_0's quantile: 0.0243682\n","[515]\tvalid_0's quantile: 0.0243664\n","[516]\tvalid_0's quantile: 0.0243664\n","[517]\tvalid_0's quantile: 0.0243606\n","[518]\tvalid_0's quantile: 0.0243543\n","[519]\tvalid_0's quantile: 0.0243518\n","[520]\tvalid_0's quantile: 0.0243398\n","[521]\tvalid_0's quantile: 0.0243366\n","[522]\tvalid_0's quantile: 0.0243237\n","[523]\tvalid_0's quantile: 0.0243069\n","[524]\tvalid_0's quantile: 0.0242924\n","[525]\tvalid_0's quantile: 0.0242923\n","[526]\tvalid_0's quantile: 0.0242804\n","[527]\tvalid_0's quantile: 0.0242687\n","[528]\tvalid_0's quantile: 0.0242528\n","[529]\tvalid_0's quantile: 0.0242498\n","[530]\tvalid_0's quantile: 0.02422\n","[531]\tvalid_0's quantile: 0.0242117\n","[532]\tvalid_0's quantile: 0.024202\n","[533]\tvalid_0's quantile: 0.0241977\n","[534]\tvalid_0's quantile: 0.0241923\n","[535]\tvalid_0's quantile: 0.0241789\n","[536]\tvalid_0's quantile: 0.024168\n","[537]\tvalid_0's quantile: 0.0241612\n","[538]\tvalid_0's quantile: 0.0241512\n","[539]\tvalid_0's quantile: 0.0241439\n","[540]\tvalid_0's quantile: 0.0241367\n","[541]\tvalid_0's quantile: 0.0241366\n","[542]\tvalid_0's quantile: 0.0241358\n","[543]\tvalid_0's quantile: 0.0241195\n","[544]\tvalid_0's quantile: 0.0241165\n","[545]\tvalid_0's quantile: 0.024107\n","[546]\tvalid_0's quantile: 0.0241057\n","[547]\tvalid_0's quantile: 0.0240992\n","[548]\tvalid_0's quantile: 0.0240936\n","[549]\tvalid_0's quantile: 0.0240875\n","[550]\tvalid_0's quantile: 0.0240834\n","[551]\tvalid_0's quantile: 0.024071\n","[552]\tvalid_0's quantile: 0.024069\n","[553]\tvalid_0's quantile: 0.0240614\n","[554]\tvalid_0's quantile: 0.0240537\n","[555]\tvalid_0's quantile: 0.0240537\n","[556]\tvalid_0's quantile: 0.0240454\n","[557]\tvalid_0's quantile: 0.0240362\n","[558]\tvalid_0's quantile: 0.0240316\n","[559]\tvalid_0's quantile: 0.0240213\n","[560]\tvalid_0's quantile: 0.0240221\n","[561]\tvalid_0's quantile: 0.0240202\n","[562]\tvalid_0's quantile: 0.0240048\n","[563]\tvalid_0's quantile: 0.0239995\n","[564]\tvalid_0's quantile: 0.0239884\n","[565]\tvalid_0's quantile: 0.0239848\n","[566]\tvalid_0's quantile: 0.0239833\n","[567]\tvalid_0's quantile: 0.0239729\n","[568]\tvalid_0's quantile: 0.023973\n","[569]\tvalid_0's quantile: 0.0239664\n","[570]\tvalid_0's quantile: 0.0239522\n","[571]\tvalid_0's quantile: 0.0239425\n","[572]\tvalid_0's quantile: 0.0239407\n","[573]\tvalid_0's quantile: 0.0239234\n","[574]\tvalid_0's quantile: 0.0239204\n","[575]\tvalid_0's quantile: 0.0239128\n","[576]\tvalid_0's quantile: 0.0238956\n","[577]\tvalid_0's quantile: 0.0238815\n","[578]\tvalid_0's quantile: 0.0238809\n","[579]\tvalid_0's quantile: 0.0238797\n","[580]\tvalid_0's quantile: 0.0238649\n","[581]\tvalid_0's quantile: 0.0238646\n","[582]\tvalid_0's quantile: 0.0238531\n","[583]\tvalid_0's quantile: 0.0238439\n","[584]\tvalid_0's quantile: 0.0238378\n","[585]\tvalid_0's quantile: 0.0238372\n","[586]\tvalid_0's quantile: 0.0238192\n","[587]\tvalid_0's quantile: 0.0238013\n","[588]\tvalid_0's quantile: 0.0238001\n","[589]\tvalid_0's quantile: 0.0237943\n","[590]\tvalid_0's quantile: 0.0237943\n","[591]\tvalid_0's quantile: 0.0237919\n","[592]\tvalid_0's quantile: 0.0237912\n","[593]\tvalid_0's quantile: 0.0237718\n","[594]\tvalid_0's quantile: 0.0237654\n","[595]\tvalid_0's quantile: 0.0237588\n","[596]\tvalid_0's quantile: 0.0237573\n","[597]\tvalid_0's quantile: 0.0237537\n","[598]\tvalid_0's quantile: 0.0237393\n","[599]\tvalid_0's quantile: 0.0237336\n","[600]\tvalid_0's quantile: 0.023715\n","[601]\tvalid_0's quantile: 0.0237116\n","[602]\tvalid_0's quantile: 0.023716\n","[603]\tvalid_0's quantile: 0.0237214\n","[604]\tvalid_0's quantile: 0.0237189\n","[605]\tvalid_0's quantile: 0.0237171\n","[606]\tvalid_0's quantile: 0.0237163\n","[607]\tvalid_0's quantile: 0.023711\n","[608]\tvalid_0's quantile: 0.0237072\n","[609]\tvalid_0's quantile: 0.0237046\n","[610]\tvalid_0's quantile: 0.0237024\n","[611]\tvalid_0's quantile: 0.0236933\n","[612]\tvalid_0's quantile: 0.0236932\n","[613]\tvalid_0's quantile: 0.0236903\n","[614]\tvalid_0's quantile: 0.0236911\n","[615]\tvalid_0's quantile: 0.0237481\n","[616]\tvalid_0's quantile: 0.0237482\n","[617]\tvalid_0's quantile: 0.0237482\n","[618]\tvalid_0's quantile: 0.0237483\n","[619]\tvalid_0's quantile: 0.0237421\n","[620]\tvalid_0's quantile: 0.0237421\n","[621]\tvalid_0's quantile: 0.0237264\n","[622]\tvalid_0's quantile: 0.0237265\n","[623]\tvalid_0's quantile: 0.0237246\n","[624]\tvalid_0's quantile: 0.0237135\n","[625]\tvalid_0's quantile: 0.0236984\n","[626]\tvalid_0's quantile: 0.0236973\n","[627]\tvalid_0's quantile: 0.0236891\n","[628]\tvalid_0's quantile: 0.0236869\n","[629]\tvalid_0's quantile: 0.0236863\n","[630]\tvalid_0's quantile: 0.0236562\n","[631]\tvalid_0's quantile: 0.0236566\n","[632]\tvalid_0's quantile: 0.0236564\n","[633]\tvalid_0's quantile: 0.0236515\n","[634]\tvalid_0's quantile: 0.0236516\n","[635]\tvalid_0's quantile: 0.0236509\n","[636]\tvalid_0's quantile: 0.0236528\n","[637]\tvalid_0's quantile: 0.0236521\n","[638]\tvalid_0's quantile: 0.0236685\n","[639]\tvalid_0's quantile: 0.0236685\n","[640]\tvalid_0's quantile: 0.0236654\n","[641]\tvalid_0's quantile: 0.0236549\n","[642]\tvalid_0's quantile: 0.0236418\n","[643]\tvalid_0's quantile: 0.0236376\n","[644]\tvalid_0's quantile: 0.0236104\n","[645]\tvalid_0's quantile: 0.0235984\n","[646]\tvalid_0's quantile: 0.0235904\n","[647]\tvalid_0's quantile: 0.0235905\n","[648]\tvalid_0's quantile: 0.0235906\n","[649]\tvalid_0's quantile: 0.0235852\n","[650]\tvalid_0's quantile: 0.0235811\n","[651]\tvalid_0's quantile: 0.0235794\n","[652]\tvalid_0's quantile: 0.0235653\n","[653]\tvalid_0's quantile: 0.0235563\n","[654]\tvalid_0's quantile: 0.0235468\n","[655]\tvalid_0's quantile: 0.0235317\n","[656]\tvalid_0's quantile: 0.0235321\n","[657]\tvalid_0's quantile: 0.0235198\n","[658]\tvalid_0's quantile: 0.023495\n","[659]\tvalid_0's quantile: 0.0234886\n","[660]\tvalid_0's quantile: 0.0234886\n","[661]\tvalid_0's quantile: 0.0234811\n","[662]\tvalid_0's quantile: 0.0234687\n","[663]\tvalid_0's quantile: 0.0234557\n","[664]\tvalid_0's quantile: 0.023449\n","[665]\tvalid_0's quantile: 0.023448\n","[666]\tvalid_0's quantile: 0.0234479\n","[667]\tvalid_0's quantile: 0.023431\n","[668]\tvalid_0's quantile: 0.0234272\n","[669]\tvalid_0's quantile: 0.0234349\n","[670]\tvalid_0's quantile: 0.0234317\n","[671]\tvalid_0's quantile: 0.0234262\n","[672]\tvalid_0's quantile: 0.0234237\n","[673]\tvalid_0's quantile: 0.0234156\n","[674]\tvalid_0's quantile: 0.0234151\n","[675]\tvalid_0's quantile: 0.0234106\n","[676]\tvalid_0's quantile: 0.0234051\n","[677]\tvalid_0's quantile: 0.0234038\n","[678]\tvalid_0's quantile: 0.0233911\n","[679]\tvalid_0's quantile: 0.0233889\n","[680]\tvalid_0's quantile: 0.0233826\n","[681]\tvalid_0's quantile: 0.0233733\n","[682]\tvalid_0's quantile: 0.023356\n","[683]\tvalid_0's quantile: 0.0233499\n","[684]\tvalid_0's quantile: 0.0233358\n","[685]\tvalid_0's quantile: 0.0233333\n","[686]\tvalid_0's quantile: 0.0233298\n","[687]\tvalid_0's quantile: 0.023325\n","[688]\tvalid_0's quantile: 0.0233193\n","[689]\tvalid_0's quantile: 0.0233194\n","[690]\tvalid_0's quantile: 0.0233295\n","[691]\tvalid_0's quantile: 0.0233264\n","[692]\tvalid_0's quantile: 0.0233224\n","[693]\tvalid_0's quantile: 0.0233104\n","[694]\tvalid_0's quantile: 0.0233103\n","[695]\tvalid_0's quantile: 0.0233067\n","[696]\tvalid_0's quantile: 0.0233037\n","[697]\tvalid_0's quantile: 0.0233046\n","[698]\tvalid_0's quantile: 0.0233007\n","[699]\tvalid_0's quantile: 0.0232968\n","[700]\tvalid_0's quantile: 0.0232867\n","[701]\tvalid_0's quantile: 0.0232834\n","[702]\tvalid_0's quantile: 0.0232803\n","[703]\tvalid_0's quantile: 0.0232729\n","[704]\tvalid_0's quantile: 0.0232732\n","[705]\tvalid_0's quantile: 0.0232676\n","[706]\tvalid_0's quantile: 0.0232653\n","[707]\tvalid_0's quantile: 0.0232641\n","[708]\tvalid_0's quantile: 0.0232639\n","[709]\tvalid_0's quantile: 0.023264\n","[710]\tvalid_0's quantile: 0.0232547\n","[711]\tvalid_0's quantile: 0.0232518\n","[712]\tvalid_0's quantile: 0.0232486\n","[713]\tvalid_0's quantile: 0.0232457\n","[714]\tvalid_0's quantile: 0.023241\n","[715]\tvalid_0's quantile: 0.0232373\n","[716]\tvalid_0's quantile: 0.0232312\n","[717]\tvalid_0's quantile: 0.0232194\n","[718]\tvalid_0's quantile: 0.0232095\n","[719]\tvalid_0's quantile: 0.0232096\n","[720]\tvalid_0's quantile: 0.0232123\n","[721]\tvalid_0's quantile: 0.0232052\n","[722]\tvalid_0's quantile: 0.0231994\n","[723]\tvalid_0's quantile: 0.0232001\n","[724]\tvalid_0's quantile: 0.0231977\n","[725]\tvalid_0's quantile: 0.023193\n","[726]\tvalid_0's quantile: 0.0231929\n","[727]\tvalid_0's quantile: 0.0231884\n","[728]\tvalid_0's quantile: 0.0231828\n","[729]\tvalid_0's quantile: 0.0231766\n","[730]\tvalid_0's quantile: 0.0231758\n","[731]\tvalid_0's quantile: 0.0231681\n","[732]\tvalid_0's quantile: 0.0231666\n","[733]\tvalid_0's quantile: 0.0231611\n","[734]\tvalid_0's quantile: 0.0231586\n","[735]\tvalid_0's quantile: 0.0231301\n","[736]\tvalid_0's quantile: 0.0231263\n","[737]\tvalid_0's quantile: 0.0231222\n","[738]\tvalid_0's quantile: 0.0231183\n","[739]\tvalid_0's quantile: 0.0231132\n","[740]\tvalid_0's quantile: 0.0231108\n","[741]\tvalid_0's quantile: 0.0231064\n","[742]\tvalid_0's quantile: 0.023105\n","[743]\tvalid_0's quantile: 0.023105\n","[744]\tvalid_0's quantile: 0.0231008\n","[745]\tvalid_0's quantile: 0.023101\n","[746]\tvalid_0's quantile: 0.0230962\n","[747]\tvalid_0's quantile: 0.0230928\n","[748]\tvalid_0's quantile: 0.0230901\n","[749]\tvalid_0's quantile: 0.0230891\n","[750]\tvalid_0's quantile: 0.0230788\n","[751]\tvalid_0's quantile: 0.0230737\n","[752]\tvalid_0's quantile: 0.0230701\n","[753]\tvalid_0's quantile: 0.023059\n","[754]\tvalid_0's quantile: 0.023055\n","[755]\tvalid_0's quantile: 0.0230478\n","[756]\tvalid_0's quantile: 0.0230444\n","[757]\tvalid_0's quantile: 0.0230339\n","[758]\tvalid_0's quantile: 0.0230339\n","[759]\tvalid_0's quantile: 0.0230269\n","[760]\tvalid_0's quantile: 0.0230259\n","[761]\tvalid_0's quantile: 0.0230224\n","[762]\tvalid_0's quantile: 0.0230213\n","[763]\tvalid_0's quantile: 0.0230206\n","[764]\tvalid_0's quantile: 0.02302\n","[765]\tvalid_0's quantile: 0.0230183\n","[766]\tvalid_0's quantile: 0.0230131\n","[767]\tvalid_0's quantile: 0.0230015\n","[768]\tvalid_0's quantile: 0.023\n","[769]\tvalid_0's quantile: 0.0229982\n","[770]\tvalid_0's quantile: 0.0229955\n","[771]\tvalid_0's quantile: 0.022988\n","[772]\tvalid_0's quantile: 0.022988\n","[773]\tvalid_0's quantile: 0.0229784\n","[774]\tvalid_0's quantile: 0.0229759\n","[775]\tvalid_0's quantile: 0.022973\n","[776]\tvalid_0's quantile: 0.0229597\n","[777]\tvalid_0's quantile: 0.0229577\n","[778]\tvalid_0's quantile: 0.0229412\n","[779]\tvalid_0's quantile: 0.0229311\n","[780]\tvalid_0's quantile: 0.0229303\n","[781]\tvalid_0's quantile: 0.0229187\n","[782]\tvalid_0's quantile: 0.0229168\n","[783]\tvalid_0's quantile: 0.0229031\n","[784]\tvalid_0's quantile: 0.0228765\n","[785]\tvalid_0's quantile: 0.0228604\n","[786]\tvalid_0's quantile: 0.0228586\n","[787]\tvalid_0's quantile: 0.0228587\n","[788]\tvalid_0's quantile: 0.0228426\n","[789]\tvalid_0's quantile: 0.0228383\n","[790]\tvalid_0's quantile: 0.0228389\n","[791]\tvalid_0's quantile: 0.0228338\n","[792]\tvalid_0's quantile: 0.0228245\n","[793]\tvalid_0's quantile: 0.0228248\n","[794]\tvalid_0's quantile: 0.0228023\n","[795]\tvalid_0's quantile: 0.0228018\n","[796]\tvalid_0's quantile: 0.0227818\n","[797]\tvalid_0's quantile: 0.0227735\n","[798]\tvalid_0's quantile: 0.0227727\n","[799]\tvalid_0's quantile: 0.0227711\n","[800]\tvalid_0's quantile: 0.022768\n","[801]\tvalid_0's quantile: 0.0227487\n","[802]\tvalid_0's quantile: 0.0227472\n","[803]\tvalid_0's quantile: 0.0227239\n","[804]\tvalid_0's quantile: 0.0227182\n","[805]\tvalid_0's quantile: 0.0227132\n","[806]\tvalid_0's quantile: 0.0227028\n","[807]\tvalid_0's quantile: 0.0226984\n","[808]\tvalid_0's quantile: 0.0226986\n","[809]\tvalid_0's quantile: 0.0226968\n","[810]\tvalid_0's quantile: 0.022693\n","[811]\tvalid_0's quantile: 0.0226916\n","[812]\tvalid_0's quantile: 0.0226939\n","[813]\tvalid_0's quantile: 0.0226731\n","[814]\tvalid_0's quantile: 0.0226667\n","[815]\tvalid_0's quantile: 0.0226644\n","[816]\tvalid_0's quantile: 0.0226631\n","[817]\tvalid_0's quantile: 0.0226587\n","[818]\tvalid_0's quantile: 0.0226515\n","[819]\tvalid_0's quantile: 0.0226521\n","[820]\tvalid_0's quantile: 0.0226522\n","[821]\tvalid_0's quantile: 0.0226273\n","[822]\tvalid_0's quantile: 0.0226264\n","[823]\tvalid_0's quantile: 0.0226244\n","[824]\tvalid_0's quantile: 0.0226053\n","[825]\tvalid_0's quantile: 0.0226037\n","[826]\tvalid_0's quantile: 0.0225761\n","[827]\tvalid_0's quantile: 0.0225622\n","[828]\tvalid_0's quantile: 0.0225643\n","[829]\tvalid_0's quantile: 0.0225495\n","[830]\tvalid_0's quantile: 0.0225278\n","[831]\tvalid_0's quantile: 0.0225122\n","[832]\tvalid_0's quantile: 0.0225036\n","[833]\tvalid_0's quantile: 0.022504\n","[834]\tvalid_0's quantile: 0.0224914\n","[835]\tvalid_0's quantile: 0.0224789\n","[836]\tvalid_0's quantile: 0.0224792\n","[837]\tvalid_0's quantile: 0.0224666\n","[838]\tvalid_0's quantile: 0.0224605\n","[839]\tvalid_0's quantile: 0.0224386\n","[840]\tvalid_0's quantile: 0.0224381\n","[841]\tvalid_0's quantile: 0.0224382\n","[842]\tvalid_0's quantile: 0.0224387\n","[843]\tvalid_0's quantile: 0.0224349\n","[844]\tvalid_0's quantile: 0.02243\n","[845]\tvalid_0's quantile: 0.02243\n","[846]\tvalid_0's quantile: 0.0224292\n","[847]\tvalid_0's quantile: 0.0224264\n","[848]\tvalid_0's quantile: 0.0224186\n","[849]\tvalid_0's quantile: 0.0224104\n","[850]\tvalid_0's quantile: 0.0224085\n","[851]\tvalid_0's quantile: 0.0223973\n","[852]\tvalid_0's quantile: 0.0223943\n","[853]\tvalid_0's quantile: 0.0223946\n","[854]\tvalid_0's quantile: 0.0223876\n","[855]\tvalid_0's quantile: 0.0223855\n","[856]\tvalid_0's quantile: 0.0223855\n","[857]\tvalid_0's quantile: 0.022382\n","[858]\tvalid_0's quantile: 0.0223804\n","[859]\tvalid_0's quantile: 0.0223791\n","[860]\tvalid_0's quantile: 0.0223768\n","[861]\tvalid_0's quantile: 0.0223793\n","[862]\tvalid_0's quantile: 0.0223739\n","[863]\tvalid_0's quantile: 0.0223657\n","[864]\tvalid_0's quantile: 0.0223677\n","[865]\tvalid_0's quantile: 0.0223665\n","[866]\tvalid_0's quantile: 0.0223652\n","[867]\tvalid_0's quantile: 0.022365\n","[868]\tvalid_0's quantile: 0.0223651\n","[869]\tvalid_0's quantile: 0.0223646\n","[870]\tvalid_0's quantile: 0.0223605\n","[871]\tvalid_0's quantile: 0.0223583\n","[872]\tvalid_0's quantile: 0.0223544\n","[873]\tvalid_0's quantile: 0.0223497\n","[874]\tvalid_0's quantile: 0.0223489\n","[875]\tvalid_0's quantile: 0.022349\n","[876]\tvalid_0's quantile: 0.0223467\n","[877]\tvalid_0's quantile: 0.0223458\n","[878]\tvalid_0's quantile: 0.022348\n","[879]\tvalid_0's quantile: 0.0223429\n","[880]\tvalid_0's quantile: 0.0223423\n","[881]\tvalid_0's quantile: 0.022342\n","[882]\tvalid_0's quantile: 0.0223381\n","[883]\tvalid_0's quantile: 0.0223378\n","[884]\tvalid_0's quantile: 0.0223324\n","[885]\tvalid_0's quantile: 0.0223281\n","[886]\tvalid_0's quantile: 0.0223283\n","[887]\tvalid_0's quantile: 0.0223235\n","[888]\tvalid_0's quantile: 0.0223235\n","[889]\tvalid_0's quantile: 0.0223794\n","[890]\tvalid_0's quantile: 0.0223605\n","[891]\tvalid_0's quantile: 0.022359\n","[892]\tvalid_0's quantile: 0.0223559\n","[893]\tvalid_0's quantile: 0.0223496\n","[894]\tvalid_0's quantile: 0.0223496\n","[895]\tvalid_0's quantile: 0.022349\n","[896]\tvalid_0's quantile: 0.0223471\n","[897]\tvalid_0's quantile: 0.0223404\n","[898]\tvalid_0's quantile: 0.0222334\n","[899]\tvalid_0's quantile: 0.0222343\n","[900]\tvalid_0's quantile: 0.022235\n","[901]\tvalid_0's quantile: 0.0222309\n","[902]\tvalid_0's quantile: 0.0222297\n","[903]\tvalid_0's quantile: 0.0222279\n","[904]\tvalid_0's quantile: 0.0222271\n","[905]\tvalid_0's quantile: 0.0222202\n","[906]\tvalid_0's quantile: 0.0222198\n","[907]\tvalid_0's quantile: 0.0222198\n","[908]\tvalid_0's quantile: 0.0222204\n","[909]\tvalid_0's quantile: 0.0222189\n","[910]\tvalid_0's quantile: 0.0222174\n","[911]\tvalid_0's quantile: 0.0222168\n","[912]\tvalid_0's quantile: 0.022217\n","[913]\tvalid_0's quantile: 0.022215\n","[914]\tvalid_0's quantile: 0.0222042\n","[915]\tvalid_0's quantile: 0.0222028\n","[916]\tvalid_0's quantile: 0.0221994\n","[917]\tvalid_0's quantile: 0.0221967\n","[918]\tvalid_0's quantile: 0.0221911\n","[919]\tvalid_0's quantile: 0.0221895\n","[920]\tvalid_0's quantile: 0.0221876\n","[921]\tvalid_0's quantile: 0.022189\n","[922]\tvalid_0's quantile: 0.0221897\n","[923]\tvalid_0's quantile: 0.0221897\n","[924]\tvalid_0's quantile: 0.0221888\n","[925]\tvalid_0's quantile: 0.022187\n","[926]\tvalid_0's quantile: 0.0221846\n","[927]\tvalid_0's quantile: 0.0221847\n","[928]\tvalid_0's quantile: 0.0221732\n","[929]\tvalid_0's quantile: 0.0221696\n","[930]\tvalid_0's quantile: 0.022163\n","[931]\tvalid_0's quantile: 0.022163\n","[932]\tvalid_0's quantile: 0.0221572\n","[933]\tvalid_0's quantile: 0.0221547\n","[934]\tvalid_0's quantile: 0.022148\n","[935]\tvalid_0's quantile: 0.0221394\n","[936]\tvalid_0's quantile: 0.0221367\n","[937]\tvalid_0's quantile: 0.0221331\n","[938]\tvalid_0's quantile: 0.0221296\n","[939]\tvalid_0's quantile: 0.0221222\n","[940]\tvalid_0's quantile: 0.0221218\n","[941]\tvalid_0's quantile: 0.0221219\n","[942]\tvalid_0's quantile: 0.022121\n","[943]\tvalid_0's quantile: 0.0221224\n","[944]\tvalid_0's quantile: 0.0221202\n","[945]\tvalid_0's quantile: 0.0221187\n","[946]\tvalid_0's quantile: 0.0221113\n","[947]\tvalid_0's quantile: 0.0221115\n","[948]\tvalid_0's quantile: 0.0221123\n","[949]\tvalid_0's quantile: 0.0221102\n","[950]\tvalid_0's quantile: 0.022107\n","[951]\tvalid_0's quantile: 0.0221071\n","[952]\tvalid_0's quantile: 0.0221047\n","[953]\tvalid_0's quantile: 0.0221068\n","[954]\tvalid_0's quantile: 0.0221048\n","[955]\tvalid_0's quantile: 0.0221049\n","[956]\tvalid_0's quantile: 0.0221049\n","[957]\tvalid_0's quantile: 0.0221049\n","[958]\tvalid_0's quantile: 0.0221072\n","[959]\tvalid_0's quantile: 0.022104\n","[960]\tvalid_0's quantile: 0.022102\n","[961]\tvalid_0's quantile: 0.0221005\n","[962]\tvalid_0's quantile: 0.022102\n","[963]\tvalid_0's quantile: 0.0220983\n","[964]\tvalid_0's quantile: 0.022098\n","[965]\tvalid_0's quantile: 0.0220947\n","[966]\tvalid_0's quantile: 0.0220945\n","[967]\tvalid_0's quantile: 0.0220896\n","[968]\tvalid_0's quantile: 0.0220905\n","[969]\tvalid_0's quantile: 0.0220873\n","[970]\tvalid_0's quantile: 0.0220883\n","[971]\tvalid_0's quantile: 0.0220874\n","[972]\tvalid_0's quantile: 0.022087\n","[973]\tvalid_0's quantile: 0.0220842\n","[974]\tvalid_0's quantile: 0.0220805\n","[975]\tvalid_0's quantile: 0.0220805\n","[976]\tvalid_0's quantile: 0.022078\n","[977]\tvalid_0's quantile: 0.0220767\n","[978]\tvalid_0's quantile: 0.0220768\n","[979]\tvalid_0's quantile: 0.0220757\n","[980]\tvalid_0's quantile: 0.0220729\n","[981]\tvalid_0's quantile: 0.0220694\n","[982]\tvalid_0's quantile: 0.0220692\n","[983]\tvalid_0's quantile: 0.0220705\n","[984]\tvalid_0's quantile: 0.0220624\n","[985]\tvalid_0's quantile: 0.0220634\n","[986]\tvalid_0's quantile: 0.0220631\n","[987]\tvalid_0's quantile: 0.0220621\n","[988]\tvalid_0's quantile: 0.0220622\n","[989]\tvalid_0's quantile: 0.0220615\n","[990]\tvalid_0's quantile: 0.0220576\n","[991]\tvalid_0's quantile: 0.0220573\n","[992]\tvalid_0's quantile: 0.0220572\n","[993]\tvalid_0's quantile: 0.0220596\n","[994]\tvalid_0's quantile: 0.0220581\n","[995]\tvalid_0's quantile: 0.0220571\n","[996]\tvalid_0's quantile: 0.0220553\n","[997]\tvalid_0's quantile: 0.0220553\n","[998]\tvalid_0's quantile: 0.0220506\n","[999]\tvalid_0's quantile: 0.0220505\n","[1000]\tvalid_0's quantile: 0.0220513\n","Elapsed time for fitting LightGBMQuantileRegressor model: 60.58 s\n","Early stopping performed. Best iteration: 866\n","Elapsed time for fitting LightGBMQuantileRegressor model: 64.33 s\n","path: predictions/pointpredictions_lightgbm_quant.csv\n","dirname: predictions\n","filename: pointpredictions_lightgbm_quant.csv\n","artifact_path: predictions\n","path: /tmp/tmpfb8not0c/pointpredictions_lightgbm_quant.csv\n","tmp_path: /tmp/tmpfb8not0c/pointpredictions_lightgbm_quant.csv\n","path: predictions/quantiles_lightgbm_quant0.1.csv\n","dirname: predictions\n","filename: quantiles_lightgbm_quant0.1.csv\n","artifact_path: predictions\n","path: /tmp/tmpq3tymchr/quantiles_lightgbm_quant0.1.csv\n","tmp_path: /tmp/tmpq3tymchr/quantiles_lightgbm_quant0.1.csv\n","path: predictions/quantiles_lightgbm_quant0.5.csv\n","dirname: predictions\n","filename: quantiles_lightgbm_quant0.5.csv\n","artifact_path: predictions\n","path: /tmp/tmp6igjb6um/quantiles_lightgbm_quant0.5.csv\n","tmp_path: /tmp/tmp6igjb6um/quantiles_lightgbm_quant0.5.csv\n","path: predictions/quantiles_lightgbm_quant0.9.csv\n","dirname: predictions\n","filename: quantiles_lightgbm_quant0.9.csv\n","artifact_path: predictions\n","path: /tmp/tmpglp5rtg3/quantiles_lightgbm_quant0.9.csv\n","tmp_path: /tmp/tmpglp5rtg3/quantiles_lightgbm_quant0.9.csv\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"eaf1526970d94c5c9134213a2337fa94\n[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[1]\tvalid_0's quantile: 0.073891\n[2]\tvalid_0's quantile: 0.0720418\n[3]\tvalid_0's quantile: 0.070186\n[4]\tvalid_0's quantile: 0.0687681\n[5]\tvalid_0's quantile: 0.0676141\n[6]\tvalid_0's quantile: 0.0663061\n[7]\tvalid_0's quantile: 0.0651943\n[8]\tvalid_0's quantile: 0.0641024\n[9]\tvalid_0's quantile: 0.0631393\n[10]\tvalid_0's quantile: 0.0622466\n[11]\tvalid_0's quantile: 0.0614045\n[12]\tvalid_0's quantile: 0.0606769\n[13]\tvalid_0's quantile: 0.0600169\n[14]\tvalid_0's quantile: 0.0595511\n[15]\tvalid_0's quantile: 0.0589853\n[16]\tvalid_0's quantile: 0.0583849\n[17]\tvalid_0's quantile: 0.0578707\n[18]\tvalid_0's quantile: 0.0570253\n[19]\tvalid_0's quantile: 0.0565005\n[20]\tvalid_0's quantile: 0.0561017\n[21]\tvalid_0's quantile: 0.0556577\n[22]\tvalid_0's quantile: 0.0552979\n[23]\tvalid_0's quantile: 0.0548717\n[24]\tvalid_0's quantile: 0.0542067\n[25]\tvalid_0's quantile: 0.0538729\n[26]\tvalid_0's quantile: 0.0534963\n[27]\tvalid_0's quantile: 0.0530041\n[28]\tvalid_0's quantile: 0.0528372\n[29]\tvalid_0's quantile: 0.0523372\n[30]\tvalid_0's quantile: 0.0520879\n[31]\tvalid_0's quantile: 0.0522527\n[32]\tvalid_0's quantile: 0.0516622\n[33]\tvalid_0's quantile: 0.0512615\n[34]\tvalid_0's quantile: 0.0509556\n[35]\tvalid_0's quantile: 0.050403\n[36]\tvalid_0's quantile: 0.0501964\n[37]\tvalid_0's quantile: 0.0500022\n[38]\tvalid_0's quantile: 0.0498239\n[39]\tvalid_0's quantile: 0.0494407\n[40]\tvalid_0's quantile: 0.0492253\n[41]\tvalid_0's quantile: 0.048794\n[42]\tvalid_0's quantile: 0.0485901\n[43]\tvalid_0's quantile: 0.0482876\n[44]\tvalid_0's quantile: 0.0479052\n[45]\tvalid_0's quantile: 0.0477117\n[46]\tvalid_0's quantile: 0.0474616\n[47]\tvalid_0's quantile: 0.047209\n[48]\tvalid_0's quantile: 0.047107\n[49]\tvalid_0's quantile: 0.0467512\n[50]\tvalid_0's quantile: 0.046531\n[51]\tvalid_0's quantile: 0.046254\n[52]\tvalid_0's quantile: 0.0467046\n[53]\tvalid_0's quantile: 0.0465384\n[54]\tvalid_0's quantile: 0.0462431\n[55]\tvalid_0's quantile: 0.0459602\n[56]\tvalid_0's quantile: 0.0458257\n[57]\tvalid_0's quantile: 0.0454786\n[58]\tvalid_0's quantile: 0.0451537\n[59]\tvalid_0's quantile: 0.0449963\n[60]\tvalid_0's quantile: 0.0447554\n[61]\tvalid_0's quantile: 0.0446315\n[62]\tvalid_0's quantile: 0.0444068\n[63]\tvalid_0's quantile: 0.0442008\n[64]\tvalid_0's quantile: 0.0439336\n[65]\tvalid_0's quantile: 0.0437424\n[66]\tvalid_0's quantile: 0.0436687\n[67]\tvalid_0's quantile: 0.0435794\n[68]\tvalid_0's quantile: 0.0433433\n[69]\tvalid_0's quantile: 0.04311\n[70]\tvalid_0's quantile: 0.0429656\n[71]\tvalid_0's quantile: 0.0428179\n[72]\tvalid_0's quantile: 0.0426097\n[73]\tvalid_0's quantile: 0.0424776\n[74]\tvalid_0's quantile: 0.0426511\n[75]\tvalid_0's quantile: 0.0425522\n[76]\tvalid_0's quantile: 0.042321\n[77]\tvalid_0's quantile: 0.0422217\n[78]\tvalid_0's quantile: 0.0421359\n[79]\tvalid_0's quantile: 0.0418577\n[80]\tvalid_0's quantile: 0.0417303\n[81]\tvalid_0's quantile: 0.0415587\n[82]\tvalid_0's quantile: 0.0413235\n[83]\tvalid_0's quantile: 0.0410807\n[84]\tvalid_0's quantile: 0.0408904\n[85]\tvalid_0's quantile: 0.0407574\n[86]\tvalid_0's quantile: 0.0406671\n[87]\tvalid_0's quantile: 0.0405739\n[88]\tvalid_0's quantile: 0.040502\n[89]\tvalid_0's quantile: 0.04031\n[90]\tvalid_0's quantile: 0.0402571\n[91]\tvalid_0's quantile: 0.0409969\n[92]\tvalid_0's quantile: 0.0408049\n[93]\tvalid_0's quantile: 0.040681\n[94]\tvalid_0's quantile: 0.0405917\n[95]\tvalid_0's quantile: 0.0405439\n[96]\tvalid_0's quantile: 0.0404085\n[97]\tvalid_0's quantile: 0.0403523\n[98]\tvalid_0's quantile: 0.0402408\n[99]\tvalid_0's quantile: 0.0401162\n[100]\tvalid_0's quantile: 0.0400511\n[101]\tvalid_0's quantile: 0.0400171\n[102]\tvalid_0's quantile: 0.0397818\n[103]\tvalid_0's quantile: 0.039601\n[104]\tvalid_0's quantile: 0.0394632\n[105]\tvalid_0's quantile: 0.0393238\n[106]\tvalid_0's quantile: 0.0392405\n[107]\tvalid_0's quantile: 0.0392572\n[108]\tvalid_0's quantile: 0.0391733\n[109]\tvalid_0's quantile: 0.0390405\n[110]\tvalid_0's quantile: 0.0390364\n[111]\tvalid_0's quantile: 0.0389843\n[112]\tvalid_0's quantile: 0.0389573\n[113]\tvalid_0's quantile: 0.0388133\n[114]\tvalid_0's quantile: 0.0386695\n[115]\tvalid_0's quantile: 0.0385705\n[116]\tvalid_0's quantile: 0.0386617\n[117]\tvalid_0's quantile: 0.0385273\n[118]\tvalid_0's quantile: 0.0384561\n[119]\tvalid_0's quantile: 0.0385836\n[120]\tvalid_0's quantile: 0.0385414\n[121]\tvalid_0's quantile: 0.0384353\n[122]\tvalid_0's quantile: 0.0382941\n[123]\tvalid_0's quantile: 0.0381183\n[124]\tvalid_0's quantile: 0.0379334\n[125]\tvalid_0's quantile: 0.0378109\n[126]\tvalid_0's quantile: 0.0377245\n[127]\tvalid_0's quantile: 0.0377385\n[128]\tvalid_0's quantile: 0.0375824\n[129]\tvalid_0's quantile: 0.0374728\n[130]\tvalid_0's quantile: 0.0373706\n[131]\tvalid_0's quantile: 0.0373583\n[132]\tvalid_0's quantile: 0.0372266\n[133]\tvalid_0's quantile: 0.0371809\n[134]\tvalid_0's quantile: 0.0370633\n[135]\tvalid_0's quantile: 0.0369722\n[136]\tvalid_0's quantile: 0.0368258\n[137]\tvalid_0's quantile: 0.0367418\n[138]\tvalid_0's quantile: 0.0366496\n[139]\tvalid_0's quantile: 0.0365967\n[140]\tvalid_0's quantile: 0.0365884\n[141]\tvalid_0's quantile: 0.036606\n[142]\tvalid_0's quantile: 0.036486\n[143]\tvalid_0's quantile: 0.0363526\n[144]\tvalid_0's quantile: 0.0363255\n[145]\tvalid_0's quantile: 0.036231\n[146]\tvalid_0's quantile: 0.036176\n[147]\tvalid_0's quantile: 0.0361026\n[148]\tvalid_0's quantile: 0.0359914\n[149]\tvalid_0's quantile: 0.035986\n[150]\tvalid_0's quantile: 0.0358947\n[151]\tvalid_0's quantile: 0.0358063\n[152]\tvalid_0's quantile: 0.0357331\n[153]\tvalid_0's quantile: 0.0357152\n[154]\tvalid_0's quantile: 0.035641\n[155]\tvalid_0's quantile: 0.0355342\n[156]\tvalid_0's quantile: 0.0354907\n[157]\tvalid_0's quantile: 0.0353884\n[158]\tvalid_0's quantile: 0.0353258\n[159]\tvalid_0's quantile: 0.0352925\n[160]\tvalid_0's quantile: 0.0352184\n[161]\tvalid_0's quantile: 0.0351612\n[162]\tvalid_0's quantile: 0.0350809\n[163]\tvalid_0's quantile: 0.0349533\n[164]\tvalid_0's quantile: 0.0348409\n[165]\tvalid_0's quantile: 0.0347814\n[166]\tvalid_0's quantile: 0.0347779\n[167]\tvalid_0's quantile: 0.0347018\n[168]\tvalid_0's quantile: 0.0346027\n[169]\tvalid_0's quantile: 0.0344926\n[170]\tvalid_0's quantile: 0.0344126\n[171]\tvalid_0's quantile: 0.0344043\n[172]\tvalid_0's quantile: 0.0343057\n[173]\tvalid_0's quantile: 0.0342427\n[174]\tvalid_0's quantile: 0.0341786\n[175]\tvalid_0's quantile: 0.0340948\n[176]\tvalid_0's quantile: 0.0339918\n[177]\tvalid_0's quantile: 0.0338972\n[178]\tvalid_0's quantile: 0.0338004\n[179]\tvalid_0's quantile: 0.0337917\n[180]\tvalid_0's quantile: 0.0336974\n[181]\tvalid_0's quantile: 0.0336478\n[182]\tvalid_0's quantile: 0.0336142\n[183]\tvalid_0's quantile: 0.0335607\n[184]\tvalid_0's quantile: 0.0334763\n[185]\tvalid_0's quantile: 0.033439\n[186]\tvalid_0's quantile: 0.0333801\n[187]\tvalid_0's quantile: 0.0332805\n[188]\tvalid_0's quantile: 0.0332075\n[189]\tvalid_0's quantile: 0.0331175\n[190]\tvalid_0's quantile: 0.0330757\n[191]\tvalid_0's quantile: 0.033033\n[192]\tvalid_0's quantile: 0.0330022\n[193]\tvalid_0's quantile: 0.0329484\n[194]\tvalid_0's quantile: 0.0329281\n[195]\tvalid_0's quantile: 0.0329077\n[196]\tvalid_0's quantile: 0.0328834\n[197]\tvalid_0's quantile: 0.0327876\n[198]\tvalid_0's quantile: 0.0327257\n[199]\tvalid_0's quantile: 0.032629\n[200]\tvalid_0's quantile: 0.0326177\n[201]\tvalid_0's quantile: 0.0325018\n[202]\tvalid_0's quantile: 0.0324215\n[203]\tvalid_0's quantile: 0.0323997\n[204]\tvalid_0's quantile: 0.0323453\n[205]\tvalid_0's quantile: 0.0322509\n[206]\tvalid_0's quantile: 0.0321866\n[207]\tvalid_0's quantile: 0.0321122\n[208]\tvalid_0's quantile: 0.0319621\n[209]\tvalid_0's quantile: 0.0319244\n[210]\tvalid_0's quantile: 0.0318844\n[211]\tvalid_0's quantile: 0.0318155\n[212]\tvalid_0's quantile: 0.031784\n[213]\tvalid_0's quantile: 0.0317764\n[214]\tvalid_0's quantile: 0.0317476\n[215]\tvalid_0's quantile: 0.0317491\n[216]\tvalid_0's quantile: 0.031723\n[217]\tvalid_0's quantile: 0.0316738\n[218]\tvalid_0's quantile: 0.0316568\n[219]\tvalid_0's quantile: 0.031659\n[220]\tvalid_0's quantile: 0.0316248\n[221]\tvalid_0's quantile: 0.0315851\n[222]\tvalid_0's quantile: 0.0315805\n[223]\tvalid_0's quantile: 0.0314932\n[224]\tvalid_0's quantile: 0.0314382\n[225]\tvalid_0's quantile: 0.0313806\n[226]\tvalid_0's quantile: 0.0313437\n[227]\tvalid_0's quantile: 0.0312864\n[228]\tvalid_0's quantile: 0.0312432\n[229]\tvalid_0's quantile: 0.0312167\n[230]\tvalid_0's quantile: 0.0312054\n[231]\tvalid_0's quantile: 0.0311854\n[232]\tvalid_0's quantile: 0.0311524\n[233]\tvalid_0's quantile: 0.0311624\n[234]\tvalid_0's quantile: 0.0311062\n[235]\tvalid_0's quantile: 0.0310748\n[236]\tvalid_0's quantile: 0.0310218\n[237]\tvalid_0's quantile: 0.0309709\n[238]\tvalid_0's quantile: 0.0309454\n[239]\tvalid_0's quantile: 0.0309055\n[240]\tvalid_0's quantile: 0.0308412\n[241]\tvalid_0's quantile: 0.0308474\n[242]\tvalid_0's quantile: 0.0308061\n[243]\tvalid_0's quantile: 0.0307792\n[244]\tvalid_0's quantile: 0.0307495\n[245]\tvalid_0's quantile: 0.0306582\n[246]\tvalid_0's quantile: 0.0306422\n[247]\tvalid_0's quantile: 0.0306306\n[248]\tvalid_0's quantile: 0.0306198\n[249]\tvalid_0's quantile: 0.0305312\n[250]\tvalid_0's quantile: 0.0304682\n[251]\tvalid_0's quantile: 0.0304024\n[252]\tvalid_0's quantile: 0.030536\n[253]\tvalid_0's quantile: 0.0305114\n[254]\tvalid_0's quantile: 0.0306332\n[255]\tvalid_0's quantile: 0.0305724\n[256]\tvalid_0's quantile: 0.0305314\n[257]\tvalid_0's quantile: 0.0305205\n[258]\tvalid_0's quantile: 0.0304829\n[259]\tvalid_0's quantile: 0.0304241\n[260]\tvalid_0's quantile: 0.0303803\n[261]\tvalid_0's quantile: 0.0303712\n[262]\tvalid_0's quantile: 0.0303395\n[263]\tvalid_0's quantile: 0.0302879\n[264]\tvalid_0's quantile: 0.0302506\n[265]\tvalid_0's quantile: 0.0302151\n[266]\tvalid_0's quantile: 0.0301813\n[267]\tvalid_0's quantile: 0.0301483\n[268]\tvalid_0's quantile: 0.0301321\n[269]\tvalid_0's quantile: 0.0301142\n[270]\tvalid_0's quantile: 0.0300921\n[271]\tvalid_0's quantile: 0.0300446\n[272]\tvalid_0's quantile: 0.0300189\n[273]\tvalid_0's quantile: 0.0300083\n[274]\tvalid_0's quantile: 0.0299985\n[275]\tvalid_0's quantile: 0.0299636\n[276]\tvalid_0's quantile: 0.0299477\n[277]\tvalid_0's quantile: 0.0299243\n[278]\tvalid_0's quantile: 0.0299099\n[279]\tvalid_0's quantile: 0.0298651\n[280]\tvalid_0's quantile: 0.0298031\n[281]\tvalid_0's quantile: 0.0298159\n[282]\tvalid_0's quantile: 0.0298065\n[283]\tvalid_0's quantile: 0.0297815\n[284]\tvalid_0's quantile: 0.0297712\n[285]\tvalid_0's quantile: 0.0297565\n[286]\tvalid_0's quantile: 0.0297444\n[287]\tvalid_0's quantile: 0.0297389\n[288]\tvalid_0's quantile: 0.029721\n[289]\tvalid_0's quantile: 0.0296789\n[290]\tvalid_0's quantile: 0.0296683\n[291]\tvalid_0's quantile: 0.0296164\n[292]\tvalid_0's quantile: 0.029583\n[293]\tvalid_0's quantile: 0.0295858\n[294]\tvalid_0's quantile: 0.0295308\n[295]\tvalid_0's quantile: 0.0295343\n[296]\tvalid_0's quantile: 0.0294966\n[297]\tvalid_0's quantile: 0.02948\n[298]\tvalid_0's quantile: 0.0293955\n[299]\tvalid_0's quantile: 0.0293583\n[300]\tvalid_0's quantile: 0.0293369\n[301]\tvalid_0's quantile: 0.0293234\n[302]\tvalid_0's quantile: 0.0292998\n[303]\tvalid_0's quantile: 0.0292805\n[304]\tvalid_0's quantile: 0.0292674\n[305]\tvalid_0's quantile: 0.0292624\n[306]\tvalid_0's quantile: 0.0292567\n[307]\tvalid_0's quantile: 0.0292206\n[308]\tvalid_0's quantile: 0.0291784\n[309]\tvalid_0's quantile: 0.0291582\n[310]\tvalid_0's quantile: 0.0291116\n[311]\tvalid_0's quantile: 0.029089\n[312]\tvalid_0's quantile: 0.0290658\n[313]\tvalid_0's quantile: 0.0290257\n[314]\tvalid_0's quantile: 0.0290212\n[315]\tvalid_0's quantile: 0.0289913\n[316]\tvalid_0's quantile: 0.0289521\n[317]\tvalid_0's quantile: 0.0289155\n[318]\tvalid_0's quantile: 0.0289108\n[319]\tvalid_0's quantile: 0.0289027\n[320]\tvalid_0's quantile: 0.02889\n[321]\tvalid_0's quantile: 0.0288441\n[322]\tvalid_0's quantile: 0.0288388\n[323]\tvalid_0's quantile: 0.0288308\n[324]\tvalid_0's quantile: 0.0288103\n[325]\tvalid_0's quantile: 0.028798\n[326]\tvalid_0's quantile: 0.0287608\n[327]\tvalid_0's quantile: 0.0287513\n[328]\tvalid_0's quantile: 0.0287372\n[329]\tvalid_0's quantile: 0.0286951\n[330]\tvalid_0's quantile: 0.0286743\n[331]\tvalid_0's quantile: 0.0286413\n[332]\tvalid_0's quantile: 0.0286462\n[333]\tvalid_0's quantile: 0.0286318\n[334]\tvalid_0's quantile: 0.0286089\n[335]\tvalid_0's quantile: 0.0286119\n[336]\tvalid_0's quantile: 0.0286002\n[337]\tvalid_0's quantile: 0.0285818\n[338]\tvalid_0's quantile: 0.0285331\n[339]\tvalid_0's quantile: 0.0285268\n[340]\tvalid_0's quantile: 0.0285312\n[341]\tvalid_0's quantile: 0.0285058\n[342]\tvalid_0's quantile: 0.0284675\n[343]\tvalid_0's quantile: 0.0284229\n[344]\tvalid_0's quantile: 0.0283913\n[345]\tvalid_0's quantile: 0.0283487\n[346]\tvalid_0's quantile: 0.0283297\n[347]\tvalid_0's quantile: 0.0283205\n[348]\tvalid_0's quantile: 0.0283269\n[349]\tvalid_0's quantile: 0.0283038\n[350]\tvalid_0's quantile: 0.0282757\n[351]\tvalid_0's quantile: 0.0282539\n[352]\tvalid_0's quantile: 0.0282378\n[353]\tvalid_0's quantile: 0.0282313\n[354]\tvalid_0's quantile: 0.0281972\n[355]\tvalid_0's quantile: 0.0281981\n[356]\tvalid_0's quantile: 0.0281765\n[357]\tvalid_0's quantile: 0.0281647\n[358]\tvalid_0's quantile: 0.0281464\n[359]\tvalid_0's quantile: 0.0281484\n[360]\tvalid_0's quantile: 0.0281143\n[361]\tvalid_0's quantile: 0.0280854\n[362]\tvalid_0's quantile: 0.0280726\n[363]\tvalid_0's quantile: 0.0280627\n[364]\tvalid_0's quantile: 0.0280557\n[365]\tvalid_0's quantile: 0.0280443\n[366]\tvalid_0's quantile: 0.0280357\n[367]\tvalid_0's quantile: 0.0280271\n[368]\tvalid_0's quantile: 0.0280086\n[369]\tvalid_0's quantile: 0.0279976\n[370]\tvalid_0's quantile: 0.0279935\n[371]\tvalid_0's quantile: 0.0279904\n[372]\tvalid_0's quantile: 0.0279903\n[373]\tvalid_0's quantile: 0.02796\n[374]\tvalid_0's quantile: 0.0279433\n[375]\tvalid_0's quantile: 0.0279451\n[376]\tvalid_0's quantile: 0.0279143\n[377]\tvalid_0's quantile: 0.0279109\n[378]\tvalid_0's quantile: 0.0279132\n[379]\tvalid_0's quantile: 0.0279135\n[380]\tvalid_0's quantile: 0.0278989\n[381]\tvalid_0's quantile: 0.0278904\n[382]\tvalid_0's quantile: 0.0278708\n[383]\tvalid_0's quantile: 0.0278703\n[384]\tvalid_0's quantile: 0.0278136\n[385]\tvalid_0's quantile: 0.027813\n[386]\tvalid_0's quantile: 0.0277861\n[387]\tvalid_0's quantile: 0.0277807\n[388]\tvalid_0's quantile: 0.0277728\n[389]\tvalid_0's quantile: 0.0277412\n[390]\tvalid_0's quantile: 0.0277063\n[391]\tvalid_0's quantile: 0.0276994\n[392]\tvalid_0's quantile: 0.0276866\n[393]\tvalid_0's quantile: 0.0276562\n[394]\tvalid_0's quantile: 0.027648\n[395]\tvalid_0's quantile: 0.0276464\n[396]\tvalid_0's quantile: 0.0276232\n[397]\tvalid_0's quantile: 0.0276226\n[398]\tvalid_0's quantile: 0.0276159\n[399]\tvalid_0's quantile: 0.0275998\n[400]\tvalid_0's quantile: 0.0275777\n[401]\tvalid_0's quantile: 0.0275739\n[402]\tvalid_0's quantile: 0.0275733\n[403]\tvalid_0's quantile: 0.0275581\n[404]\tvalid_0's quantile: 0.0275342\n[405]\tvalid_0's quantile: 0.027529\n[406]\tvalid_0's quantile: 0.0275166\n[407]\tvalid_0's quantile: 0.0275204\n[408]\tvalid_0's quantile: 0.0275052\n[409]\tvalid_0's quantile: 0.0275054\n[410]\tvalid_0's quantile: 0.0274857\n[411]\tvalid_0's quantile: 0.027485\n[412]\tvalid_0's quantile: 0.0274677\n[413]\tvalid_0's quantile: 0.0274786\n[414]\tvalid_0's quantile: 0.0274542\n[415]\tvalid_0's quantile: 0.0274364\n[416]\tvalid_0's quantile: 0.0274353\n[417]\tvalid_0's quantile: 0.0274357\n[418]\tvalid_0's quantile: 0.0274216\n[419]\tvalid_0's quantile: 0.027422\n[420]\tvalid_0's quantile: 0.0274074\n[421]\tvalid_0's quantile: 0.0274027\n[422]\tvalid_0's quantile: 0.027433\n[423]\tvalid_0's quantile: 0.0274151\n[424]\tvalid_0's quantile: 0.0274057\n[425]\tvalid_0's quantile: 0.0274046\n[426]\tvalid_0's quantile: 0.0274015\n[427]\tvalid_0's quantile: 0.0273852\n[428]\tvalid_0's quantile: 0.0273717\n[429]\tvalid_0's quantile: 0.0273871\n[430]\tvalid_0's quantile: 0.0273869\n[431]\tvalid_0's quantile: 0.027352\n[432]\tvalid_0's quantile: 0.0273504\n[433]\tvalid_0's quantile: 0.0273496\n[434]\tvalid_0's quantile: 0.0273258\n[435]\tvalid_0's quantile: 0.0273218\n[436]\tvalid_0's quantile: 0.0273116\n[437]\tvalid_0's quantile: 0.0273105\n[438]\tvalid_0's quantile: 0.0273075\n[439]\tvalid_0's quantile: 0.0272876\n[440]\tvalid_0's quantile: 0.027286\n[441]\tvalid_0's quantile: 0.0272781\n[442]\tvalid_0's quantile: 0.0272753\n[443]\tvalid_0's quantile: 0.0272674\n[444]\tvalid_0's quantile: 0.0272599\n[445]\tvalid_0's quantile: 0.0272604\n[446]\tvalid_0's quantile: 0.0272472\n[447]\tvalid_0's quantile: 0.0272209\n[448]\tvalid_0's quantile: 0.027219\n[449]\tvalid_0's quantile: 0.0271979\n[450]\tvalid_0's quantile: 0.0271651\n[451]\tvalid_0's quantile: 0.0271609\n[452]\tvalid_0's quantile: 0.0271403\n[453]\tvalid_0's quantile: 0.0271249\n[454]\tvalid_0's quantile: 0.0271233\n[455]\tvalid_0's quantile: 0.0270821\n[456]\tvalid_0's quantile: 0.0270556\n[457]\tvalid_0's quantile: 0.0270329\n[458]\tvalid_0's quantile: 0.0270202\n[459]\tvalid_0's quantile: 0.0269632\n[460]\tvalid_0's quantile: 0.026951\n[461]\tvalid_0's quantile: 0.0269458\n[462]\tvalid_0's quantile: 0.026947\n[463]\tvalid_0's quantile: 0.0269426\n[464]\tvalid_0's quantile: 0.0269291\n[465]\tvalid_0's quantile: 0.0269072\n[466]\tvalid_0's quantile: 0.0269013\n[467]\tvalid_0's quantile: 0.0269\n[468]\tvalid_0's quantile: 0.0268888\n[469]\tvalid_0's quantile: 0.0268804\n[470]\tvalid_0's quantile: 0.0268511\n[471]\tvalid_0's quantile: 0.0268429\n[472]\tvalid_0's quantile: 0.0268319\n[473]\tvalid_0's quantile: 0.0268266\n[474]\tvalid_0's quantile: 0.0268142\n[475]\tvalid_0's quantile: 0.0268132\n[476]\tvalid_0's quantile: 0.0267962\n[477]\tvalid_0's quantile: 0.0267854\n[478]\tvalid_0's quantile: 0.0267617\n[479]\tvalid_0's quantile: 0.0267489\n[480]\tvalid_0's quantile: 0.0267241\n[481]\tvalid_0's quantile: 0.0267198\n[482]\tvalid_0's quantile: 0.0266994\n[483]\tvalid_0's quantile: 0.0266951\n[484]\tvalid_0's quantile: 0.0266957\n[485]\tvalid_0's quantile: 0.0266913\n[486]\tvalid_0's quantile: 0.0266718\n[487]\tvalid_0's quantile: 0.0266718\n[488]\tvalid_0's quantile: 0.0266704\n[489]\tvalid_0's quantile: 0.0266697\n[490]\tvalid_0's quantile: 0.0266697\n[491]\tvalid_0's quantile: 0.0266695\n[492]\tvalid_0's quantile: 0.0266566\n[493]\tvalid_0's quantile: 0.0266491\n[494]\tvalid_0's quantile: 0.02663\n[495]\tvalid_0's quantile: 0.0266304\n[496]\tvalid_0's quantile: 0.0266056\n[497]\tvalid_0's quantile: 0.0265995\n[498]\tvalid_0's quantile: 0.0265979\n[499]\tvalid_0's quantile: 0.0265981\n[500]\tvalid_0's quantile: 0.026595\n[501]\tvalid_0's quantile: 0.026585\n[502]\tvalid_0's quantile: 0.0265794\n[503]\tvalid_0's quantile: 0.0265809\n[504]\tvalid_0's quantile: 0.026568\n[505]\tvalid_0's quantile: 0.0265599\n[506]\tvalid_0's quantile: 0.0265578\n[507]\tvalid_0's quantile: 0.02654\n[508]\tvalid_0's quantile: 0.0265371\n[509]\tvalid_0's quantile: 0.0265187\n[510]\tvalid_0's quantile: 0.0265182\n[511]\tvalid_0's quantile: 0.0265158\n[512]\tvalid_0's quantile: 0.026512\n[513]\tvalid_0's quantile: 0.026509\n[514]\tvalid_0's quantile: 0.0265102\n[515]\tvalid_0's quantile: 0.0265079\n[516]\tvalid_0's quantile: 0.0265027\n[517]\tvalid_0's quantile: 0.0265031\n[518]\tvalid_0's quantile: 0.0264996\n[519]\tvalid_0's quantile: 0.0264965\n[520]\tvalid_0's quantile: 0.0264992\n[521]\tvalid_0's quantile: 0.0264927\n[522]\tvalid_0's quantile: 0.0264986\n[523]\tvalid_0's quantile: 0.0264955\n[524]\tvalid_0's quantile: 0.0264874\n[525]\tvalid_0's quantile: 0.026476\n[526]\tvalid_0's quantile: 0.0264733\n[527]\tvalid_0's quantile: 0.026473\n[528]\tvalid_0's quantile: 0.0264554\n[529]\tvalid_0's quantile: 0.0264554\n[530]\tvalid_0's quantile: 0.0264439\n[531]\tvalid_0's quantile: 0.0264261\n[532]\tvalid_0's quantile: 0.0264118\n[533]\tvalid_0's quantile: 0.0263992\n[534]\tvalid_0's quantile: 0.0263743\n[535]\tvalid_0's quantile: 0.0263563\n[536]\tvalid_0's quantile: 0.0263572\n[537]\tvalid_0's quantile: 0.0263575\n[538]\tvalid_0's quantile: 0.026338\n[539]\tvalid_0's quantile: 0.026333\n[540]\tvalid_0's quantile: 0.0263296\n[541]\tvalid_0's quantile: 0.0263212\n[542]\tvalid_0's quantile: 0.0263109\n[543]\tvalid_0's quantile: 0.0263111\n[544]\tvalid_0's quantile: 0.0263057\n[545]\tvalid_0's quantile: 0.0262954\n[546]\tvalid_0's quantile: 0.0262926\n[547]\tvalid_0's quantile: 0.0262908\n[548]\tvalid_0's quantile: 0.0262802\n[549]\tvalid_0's quantile: 0.0262619\n[550]\tvalid_0's quantile: 0.0262617\n[551]\tvalid_0's quantile: 0.0262576\n[552]\tvalid_0's quantile: 0.0262493\n[553]\tvalid_0's quantile: 0.0262427\n[554]\tvalid_0's quantile: 0.0262355\n[555]\tvalid_0's quantile: 0.0262357\n[556]\tvalid_0's quantile: 0.0262386\n[557]\tvalid_0's quantile: 0.0262255\n[558]\tvalid_0's quantile: 0.0262032\n[559]\tvalid_0's quantile: 0.0261887\n[560]\tvalid_0's quantile: 0.0261817\n[561]\tvalid_0's quantile: 0.026174\n[562]\tvalid_0's quantile: 0.0261681\n[563]\tvalid_0's quantile: 0.0261684\n[564]\tvalid_0's quantile: 0.0261627\n[565]\tvalid_0's quantile: 0.0261631\n[566]\tvalid_0's quantile: 0.0261415\n[567]\tvalid_0's quantile: 0.0261199\n[568]\tvalid_0's quantile: 0.0261196\n[569]\tvalid_0's quantile: 0.026103\n[570]\tvalid_0's quantile: 0.0261004\n[571]\tvalid_0's quantile: 0.02608\n[572]\tvalid_0's quantile: 0.0260584\n[573]\tvalid_0's quantile: 0.0260491\n[574]\tvalid_0's quantile: 0.0260355\n[575]\tvalid_0's quantile: 0.026036\n[576]\tvalid_0's quantile: 0.0260284\n[577]\tvalid_0's quantile: 0.0260259\n[578]\tvalid_0's quantile: 0.0260262\n[579]\tvalid_0's quantile: 0.0260077\n[580]\tvalid_0's quantile: 0.0260046\n[581]\tvalid_0's quantile: 0.0259971\n[582]\tvalid_0's quantile: 0.0259975\n[583]\tvalid_0's quantile: 0.0259759\n[584]\tvalid_0's quantile: 0.0259725\n[585]\tvalid_0's quantile: 0.025953\n[586]\tvalid_0's quantile: 0.0259427\n[587]\tvalid_0's quantile: 0.0259346\n[588]\tvalid_0's quantile: 0.025928\n[589]\tvalid_0's quantile: 0.0259263\n[590]\tvalid_0's quantile: 0.0259184\n[591]\tvalid_0's quantile: 0.0259168\n[592]\tvalid_0's quantile: 0.0259171\n[593]\tvalid_0's quantile: 0.0259093\n[594]\tvalid_0's quantile: 0.0259055\n[595]\tvalid_0's quantile: 0.025903\n[596]\tvalid_0's quantile: 0.0259\n[597]\tvalid_0's quantile: 0.0258892\n[598]\tvalid_0's quantile: 0.0258892\n[599]\tvalid_0's quantile: 0.0258891\n[600]\tvalid_0's quantile: 0.0258809\n[601]\tvalid_0's quantile: 0.0258708\n[602]\tvalid_0's quantile: 0.025877\n[603]\tvalid_0's quantile: 0.0258879\n[604]\tvalid_0's quantile: 0.0258756\n[605]\tvalid_0's quantile: 0.0258712\n[606]\tvalid_0's quantile: 0.0258534\n[607]\tvalid_0's quantile: 0.0258663\n[608]\tvalid_0's quantile: 0.0258465\n[609]\tvalid_0's quantile: 0.0258473\n[610]\tvalid_0's quantile: 0.0258435\n[611]\tvalid_0's quantile: 0.0258316\n[612]\tvalid_0's quantile: 0.0258302\n[613]\tvalid_0's quantile: 0.0258205\n[614]\tvalid_0's quantile: 0.0258186\n[615]\tvalid_0's quantile: 0.0258196\n[616]\tvalid_0's quantile: 0.0258099\n[617]\tvalid_0's quantile: 0.0258169\n[618]\tvalid_0's quantile: 0.0258137\n[619]\tvalid_0's quantile: 0.0258139\n[620]\tvalid_0's quantile: 0.0258108\n[621]\tvalid_0's quantile: 0.0258055\n[622]\tvalid_0's quantile: 0.0257793\n[623]\tvalid_0's quantile: 0.0258098\n[624]\tvalid_0's quantile: 0.0258075\n[625]\tvalid_0's quantile: 0.0258072\n[626]\tvalid_0's quantile: 0.0258059\n[627]\tvalid_0's quantile: 0.0258166\n[628]\tvalid_0's quantile: 0.0257997\n[629]\tvalid_0's quantile: 0.0257842\n[630]\tvalid_0's quantile: 0.0257755\n[631]\tvalid_0's quantile: 0.0257703\n[632]\tvalid_0's quantile: 0.0257706\n[633]\tvalid_0's quantile: 0.0257702\n[634]\tvalid_0's quantile: 0.025768\n[635]\tvalid_0's quantile: 0.0257676\n[636]\tvalid_0's quantile: 0.0257618\n[637]\tvalid_0's quantile: 0.0257619\n[638]\tvalid_0's quantile: 0.025769\n[639]\tvalid_0's quantile: 0.0257676\n[640]\tvalid_0's quantile: 0.0257676\n[641]\tvalid_0's quantile: 0.0257655\n[642]\tvalid_0's quantile: 0.0257656\n[643]\tvalid_0's quantile: 0.0257506\n[644]\tvalid_0's quantile: 0.0257495\n[645]\tvalid_0's quantile: 0.0257481\n[646]\tvalid_0's quantile: 0.0257201\n[647]\tvalid_0's quantile: 0.0257243\n[648]\tvalid_0's quantile: 0.0257087\n[649]\tvalid_0's quantile: 0.0257141\n[650]\tvalid_0's quantile: 0.025697\n[651]\tvalid_0's quantile: 0.0256947\n[652]\tvalid_0's quantile: 0.0256951\n[653]\tvalid_0's quantile: 0.0256815\n[654]\tvalid_0's quantile: 0.0256686\n[655]\tvalid_0's quantile: 0.0256688\n[656]\tvalid_0's quantile: 0.0256697\n[657]\tvalid_0's quantile: 0.0256698\n[658]\tvalid_0's quantile: 0.0256596\n[659]\tvalid_0's quantile: 0.0256603\n[660]\tvalid_0's quantile: 0.0256583\n[661]\tvalid_0's quantile: 0.0256591\n[662]\tvalid_0's quantile: 0.0256613\n[663]\tvalid_0's quantile: 0.0256551\n[664]\tvalid_0's quantile: 0.0256549\n[665]\tvalid_0's quantile: 0.0256839\n[666]\tvalid_0's quantile: 0.0256699\n[667]\tvalid_0's quantile: 0.0256667\n[668]\tvalid_0's quantile: 0.0256585\n[669]\tvalid_0's quantile: 0.0256607\n[670]\tvalid_0's quantile: 0.0256399\n[671]\tvalid_0's quantile: 0.0256401\n[672]\tvalid_0's quantile: 0.0256398\n[673]\tvalid_0's quantile: 0.0256402\n[674]\tvalid_0's quantile: 0.0256364\n[675]\tvalid_0's quantile: 0.0256334\n[676]\tvalid_0's quantile: 0.0256283\n[677]\tvalid_0's quantile: 0.0256256\n[678]\tvalid_0's quantile: 0.0255948\n[679]\tvalid_0's quantile: 0.0255806\n[680]\tvalid_0's quantile: 0.0255776\n[681]\tvalid_0's quantile: 0.0255764\n[682]\tvalid_0's quantile: 0.0255676\n[683]\tvalid_0's q\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n0.0264371\n[338]\tvalid_0's quantile: 0.0264167\n[339]\tvalid_0's quantile: 0.0264068\n[340]\tvalid_0's quantile: 0.0263993\n[341]\tvalid_0's quantile: 0.0263813\n[342]\tvalid_0's quantile: 0.0263623\n[343]\tvalid_0's quantile: 0.0263623\n[344]\tvalid_0's quantile: 0.0263513\n[345]\tvalid_0's quantile: 0.0263322\n[346]\tvalid_0's quantile: 0.0263276\n[347]\tvalid_0's quantile: 0.026323\n[348]\tvalid_0's quantile: 0.0263231\n[349]\tvalid_0's quantile: 0.0263193\n[350]\tvalid_0's quantile: 0.0262932\n[351]\tvalid_0's quantile: 0.0262494\n[352]\tvalid_0's quantile: 0.0262342\n[353]\tvalid_0's quantile: 0.0261998\n[354]\tvalid_0's quantile: 0.0261741\n[355]\tvalid_0's quantile: 0.0261461\n[356]\tvalid_0's quantile: 0.0261463\n[357]\tvalid_0's quantile: 0.0261436\n[358]\tvalid_0's quantile: 0.0261357\n[359]\tvalid_0's quantile: 0.0261167\n[360]\tvalid_0's quantile: 0.0261028\n[361]\tvalid_0's quantile: 0.0260864\n[362]\tvalid_0's quantile: 0.0260657\n[363]\tvalid_0's quantile: 0.0260424\n[364]\tvalid_0's quantile: 0.0260424\n[365]\tvalid_0's quantile: 0.0260253\n[366]\tvalid_0's quantile: 0.026021\n[367]\tvalid_0's quantile: 0.0259876\n[368]\tvalid_0's quantile: 0.0259738\n[369]\tvalid_0's quantile: 0.0259775\n[370]\tvalid_0's quantile: 0.0259627\n[371]\tvalid_0's quantile: 0.0259628\n[372]\tvalid_0's quantile: 0.0259564\n[373]\tvalid_0's quantile: 0.0259565\n[374]\tvalid_0's quantile: 0.0259233\n[375]\tvalid_0's quantile: 0.0259165\n[376]\tvalid_0's quantile: 0.0258935\n[377]\tvalid_0's quantile: 0.0258855\n[378]\tvalid_0's quantile: 0.025869\n[379]\tvalid_0's quantile: 0.025845\n[380]\tvalid_0's quantile: 0.025802\n[381]\tvalid_0's quantile: 0.025774\n[382]\tvalid_0's quantile: 0.0257681\n[383]\tvalid_0's quantile: 0.0257191\n[384]\tvalid_0's quantile: 0.0257192\n[385]\tvalid_0's quantile: 0.0256757\n[386]\tvalid_0's quantile: 0.0256557\n[387]\tvalid_0's quantile: 0.0256556\n[388]\tvalid_0's quantile: 0.0256471\n[389]\tvalid_0's quantile: 0.0256325\n[390]\tvalid_0's quantile: 0.0256125\n[391]\tvalid_0's quantile: 0.0255941\n[392]\tvalid_0's quantile: 0.0255559\n[393]\tvalid_0's quantile: 0.0255553\n[394]\tvalid_0's quantile: 0.0255347\n[395]\tvalid_0's quantile: 0.0255111\n[396]\tvalid_0's quantile: 0.0255007\n[397]\tvalid_0's quantile: 0.025487\n[398]\tvalid_0's quantile: 0.0254717\n[399]\tvalid_0's quantile: 0.0254497\n[400]\tvalid_0's quantile: 0.0254315\n[401]\tvalid_0's quantile: 0.0254208\n[402]\tvalid_0's quantile: 0.02541\n[403]\tvalid_0's quantile: 0.0254171\n[404]\tvalid_0's quantile: 0.0254007\n[405]\tvalid_0's quantile: 0.0253915\n[406]\tvalid_0's quantile: 0.025359\n[407]\tvalid_0's quantile: 0.025356\n[408]\tvalid_0's quantile: 0.025357\n[409]\tvalid_0's quantile: 0.025342\n[410]\tvalid_0's quantile: 0.0253345\n[411]\tvalid_0's quantile: 0.0253332\n[412]\tvalid_0's quantile: 0.0253215\n[413]\tvalid_0's quantile: 0.025313\n[414]\tvalid_0's quantile: 0.0253072\n[415]\tvalid_0's quantile: 0.025301\n[416]\tvalid_0's quantile: 0.0252916\n[417]\tvalid_0's quantile: 0.0252899\n[418]\tvalid_0's quantile: 0.0252843\n[419]\tvalid_0's quantile: 0.0252843\n[420]\tvalid_0's quantile: 0.0252799\n[421]\tvalid_0's quantile: 0.0252736\n[422]\tvalid_0's quantile: 0.0252523\n[423]\tvalid_0's quantile: 0.0252481\n[424]\tvalid_0's quantile: 0.0252225\n[425]\tvalid_0's quantile: 0.0251878\n[426]\tvalid_0's quantile: 0.0251883\n[427]\tvalid_0's quantile: 0.0251841\n[428]\tvalid_0's quantile: 0.0251803\n[429]\tvalid_0's quantile: 0.02516\n[430]\tvalid_0's quantile: 0.0251492\n[431]\tvalid_0's quantile: 0.0251418\n[432]\tvalid_0's quantile: 0.0251403\n[433]\tvalid_0's quantile: 0.0251376\n[434]\tvalid_0's quantile: 0.0251115\n[435]\tvalid_0's quantile: 0.0251016\n[436]\tvalid_0's quantile: 0.0250784\n[437]\tvalid_0's quantile: 0.0250689\n[438]\tvalid_0's quantile: 0.0250656\n[439]\tvalid_0's quantile: 0.0250535\n[440]\tvalid_0's quantile: 0.0250536\n[441]\tvalid_0's quantile: 0.0250488\n[442]\tvalid_0's quantile: 0.0250278\n[443]\tvalid_0's quantile: 0.0250097\n[444]\tvalid_0's quantile: 0.0249836\n[445]\tvalid_0's quantile: 0.0249824\n[446]\tvalid_0's quantile: 0.0249771\n[447]\tvalid_0's quantile: 0.0249726\n[448]\tvalid_0's quantile: 0.0249669\n[449]\tvalid_0's quantile: 0.0249398\n[450]\tvalid_0's quantile: 0.0249118\n[451]\tvalid_0's quantile: 0.0249063\n[452]\tvalid_0's quantile: 0.0248959\n[453]\tvalid_0's quantile: 0.0248775\n[454]\tvalid_0's quantile: 0.0248657\n[455]\tvalid_0's quantile: 0.0248522\n[456]\tvalid_0's quantile: 0.0248526\n[457]\tvalid_0's quantile: 0.0248558\n[458]\tvalid_0's quantile: 0.0248521\n[459]\tvalid_0's quantile: 0.0248483\n[460]\tvalid_0's quantile: 0.0248477\n[461]\tvalid_0's quantile: 0.024846\n[462]\tvalid_0's quantile: 0.0248386\n[463]\tvalid_0's quantile: 0.0248312\n[464]\tvalid_0's quantile: 0.0248283\n[465]\tvalid_0's quantile: 0.024823\n[466]\tvalid_0's quantile: 0.0248101\n[467]\tvalid_0's quantile: 0.0248087\n[468]\tvalid_0's quantile: 0.0248086\n[469]\tvalid_0's quantile: 0.0248182\n[470]\tvalid_0's quantile: 0.0248186\n[471]\tvalid_0's quantile: 0.0248149\n[472]\tvalid_0's quantile: 0.024778\n[473]\tvalid_0's quantile: 0.0247759\n[474]\tvalid_0's quantile: 0.0247751\n[475]\tvalid_0's quantile: 0.0247739\n[476]\tvalid_0's quantile: 0.0247684\n[477]\tvalid_0's quantile: 0.0247606\n[478]\tvalid_0's quantile: 0.024754\n[479]\tvalid_0's quantile: 0.0247457\n[480]\tvalid_0's quantile: 0.0247297\n[481]\tvalid_0's quantile: 0.0247167\n[482]\tvalid_0's quantile: 0.0246874\n[483]\tvalid_0's quantile: 0.0246875\n[484]\tvalid_0's quantile: 0.02468\n[485]\tvalid_0's quantile: 0.0246724\n[486]\tvalid_0's quantile: 0.024662\n[487]\tvalid_0's quantile: 0.0246629\n[488]\tvalid_0's quantile: 0.0246497\n[489]\tvalid_0's quantile: 0.0246415\n[490]\tvalid_0's quantile: 0.0246387\n[491]\tvalid_0's quantile: 0.0246325\n[492]\tvalid_0's quantile: 0.0246238\n[493]\tvalid_0's quantile: 0.0246058\n[494]\tvalid_0's quantile: 0.0246039\n[495]\tvalid_0's quantile: 0.0245708\n[496]\tvalid_0's quantile: 0.0245671\n[497]\tvalid_0's quantile: 0.0245416\n[498]\tvalid_0's quantile: 0.0245333\n[499]\tvalid_0's quantile: 0.0245305\n[500]\tvalid_0's quantile: 0.0245295\n[501]\tvalid_0's quantile: 0.0245297\n[502]\tvalid_0's quantile: 0.0245142\n[503]\tvalid_0's quantile: 0.0245007\n[504]\tvalid_0's quantile: 0.0244838\n[505]\tvalid_0's quantile: 0.0244739\n[506]\tvalid_0's quantile: 0.0244598\n[507]\tvalid_0's quantile: 0.0244398\n[508]\tvalid_0's quantile: 0.0244397\n[509]\tvalid_0's quantile: 0.0244188\n[510]\tvalid_0's quantile: 0.024413\n[511]\tvalid_0's quantile: 0.0243969\n[512]\tvalid_0's quantile: 0.0243859\n[513]\tvalid_0's quantile: 0.0243749\n[514]\tvalid_0's quantile: 0.0243682\n[515]\tvalid_0's quantile: 0.0243664\n[516]\tvalid_0's quantile: 0.0243664\n[517]\tvalid_0's quantile: 0.0243606\n[518]\tvalid_0's quantile: 0.0243543\n[519]\tvalid_0's quantile: 0.0243518\n[520]\tvalid_0's quantile: 0.0243398\n[521]\tvalid_0's quantile: 0.0243366\n[522]\tvalid_0's quantile: 0.0243237\n[523]\tvalid_0's quantile: 0.0243069\n[524]\tvalid_0's quantile: 0.0242924\n[525]\tvalid_0's quantile: 0.0242923\n[526]\tvalid_0's quantile: 0.0242804\n[527]\tvalid_0's quantile: 0.0242687\n[528]\tvalid_0's quantile: 0.0242528\n[529]\tvalid_0's quantile: 0.0242498\n[530]\tvalid_0's quantile: 0.02422\n[531]\tvalid_0's quantile: 0.0242117\n[532]\tvalid_0's quantile: 0.024202\n[533]\tvalid_0's quantile: 0.0241977\n[534]\tvalid_0's quantile: 0.0241923\n[535]\tvalid_0's quantile: 0.0241789\n[536]\tvalid_0's quantile: 0.024168\n[537]\tvalid_0's quantile: 0.0241612\n[538]\tvalid_0's quantile: 0.0241512\n[539]\tvalid_0's quantile: 0.0241439\n[540]\tvalid_0's quantile: 0.0241367\n[541]\tvalid_0's quantile: 0.0241366\n[542]\tvalid_0's quantile: 0.0241358\n[543]\tvalid_0's quantile: 0.0241195\n[544]\tvalid_0's quantile: 0.0241165\n[545]\tvalid_0's quantile: 0.024107\n[546]\tvalid_0's quantile: 0.0241057\n[547]\tvalid_0's quantile: 0.0240992\n[548]\tvalid_0's quantile: 0.0240936\n[549]\tvalid_0's quantile: 0.0240875\n[550]\tvalid_0's quantile: 0.0240834\n[551]\tvalid_0's quantile: 0.024071\n[552]\tvalid_0's quantile: 0.024069\n[553]\tvalid_0's quantile: 0.0240614\n[554]\tvalid_0's quantile: 0.0240537\n[555]\tvalid_0's quantile: 0.0240537\n[556]\tvalid_0's quantile: 0.0240454\n[557]\tvalid_0's quantile: 0.0240362\n[558]\tvalid_0's quantile: 0.0240316\n[559]\tvalid_0's quantile: 0.0240213\n[560]\tvalid_0's quantile: 0.0240221\n[561]\tvalid_0's quantile: 0.0240202\n[562]\tvalid_0's quantile: 0.0240048\n[563]\tvalid_0's quantile: 0.0239995\n[564]\tvalid_0's quantile: 0.0239884\n[565]\tvalid_0's quantile: 0.0239848\n[566]\tvalid_0's quantile: 0.0239833\n[567]\tvalid_0's quantile: 0.0239729\n[568]\tvalid_0's quantile: 0.023973\n[569]\tvalid_0's quantile: 0.0239664\n[570]\tvalid_0's quantile: 0.0239522\n[571]\tvalid_0's quantile: 0.0239425\n[572]\tvalid_0's quantile: 0.0239407\n[573]\tvalid_0's quantile: 0.0239234\n[574]\tvalid_0's quantile: 0.0239204\n[575]\tvalid_0's quantile: 0.0239128\n[576]\tvalid_0's quantile: 0.0238956\n[577]\tvalid_0's quantile: 0.0238815\n[578]\tvalid_0's quantile: 0.0238809\n[579]\tvalid_0's quantile: 0.0238797\n[580]\tvalid_0's quantile: 0.0238649\n[581]\tvalid_0's quantile: 0.0238646\n[582]\tvalid_0's quantile: 0.0238531\n[583]\tvalid_0's quantile: 0.0238439\n[584]\tvalid_0's quantile: 0.0238378\n[585]\tvalid_0's quantile: 0.0238372\n[586]\tvalid_0's quantile: 0.0238192\n[587]\tvalid_0's quantile: 0.0238013\n[588]\tvalid_0's quantile: 0.0238001\n[589]\tvalid_0's quantile: 0.0237943\n[590]\tvalid_0's quantile: 0.0237943\n[591]\tvalid_0's quantile: 0.0237919\n[592]\tvalid_0's quantile: 0.0237912\n[593]\tvalid_0's quantile: 0.0237718\n[594]\tvalid_0's quantile: 0.0237654\n[595]\tvalid_0's quantile: 0.0237588\n[596]\tvalid_0's quantile: 0.0237573\n[597]\tvalid_0's quantile: 0.0237537\n[598]\tvalid_0's quantile: 0.0237393\n[599]\tvalid_0's quantile: 0.0237336\n[600]\tvalid_0's quantile: 0.023715\n[601]\tvalid_0's quantile: 0.0237116\n[602]\tvalid_0's quantile: 0.023716\n[603]\tvalid_0's quantile: 0.0237214\n[604]\tvalid_0's quantile: 0.0237189\n[605]\tvalid_0's quantile: 0.0237171\n[606]\tvalid_0's quantile: 0.0237163\n[607]\tvalid_0's quantile: 0.023711\n[608]\tvalid_0's quantile: 0.0237072\n[609]\tvalid_0's quantile: 0.0237046\n[610]\tvalid_0's quantile: 0.0237024\n[611]\tvalid_0's quantile: 0.0236933\n[612]\tvalid_0's quantile: 0.0236932\n[613]\tvalid_0's quantile: 0.0236903\n[614]\tvalid_0's quantile: 0.0236911\n[615]\tvalid_0's quantile: 0.0237481\n[616]\tvalid_0's quantile: 0.0237482\n[617]\tvalid_0's quantile: 0.0237482\n[618]\tvalid_0's quantile: 0.0237483\n[619]\tvalid_0's quantile: 0.0237421\n[620]\tvalid_0's quantile: 0.0237421\n[621]\tvalid_0's quantile: 0.0237264\n[622]\tvalid_0's quantile: 0.0237265\n[623]\tvalid_0's quantile: 0.0237246\n[624]\tvalid_0's quantile: 0.0237135\n[625]\tvalid_0's quantile: 0.0236984\n[626]\tvalid_0's quantile: 0.0236973\n[627]\tvalid_0's quantile: 0.0236891\n[628]\tvalid_0's quantile: 0.0236869\n[629]\tvalid_0's quantile: 0.0236863\n[630]\tvalid_0's quantile: 0.0236562\n[631]\tvalid_0's quantile: 0.0236566\n[632]\tvalid_0's quantile: 0.0236564\n[633]\tvalid_0's quantile: 0.0236515\n[634]\tvalid_0's quantile: 0.0236516\n[635]\tvalid_0's quantile: 0.0236509\n[636]\tvalid_0's quantile: 0.0236528\n[637]\tvalid_0's quantile: 0.0236521\n[638]\tvalid_0's quantile: 0.0236685\n[639]\tvalid_0's quantile: 0.0236685\n[640]\tvalid_0's quantile: 0.0236654\n[641]\tvalid_0's quantile: 0.0236549\n[642]\tvalid_0's quantile: 0.0236418\n[643]\tvalid_0's quantile: 0.0236376\n[644]\tvalid_0's quantile: 0.0236104\n[645]\tvalid_0's quantile: 0.0235984\n[646]\tvalid_0's quantile: 0.0235904\n[647]\tvalid_0's quantile: 0.0235905\n[648]\tvalid_0's quantile: 0.0235906\n[649]\tvalid_0's quantile: 0.0235852\n[650]\tvalid_0's quantile: 0.0235811\n[651]\tvalid_0's quantile: 0.0235794\n[652]\tvalid_0's quantile: 0.0235653\n[653]\tvalid_0's quantile: 0.0235563\n[654]\tvalid_0's quantile: 0.0235468\n[655]\tvalid_0's quantile: 0.0235317\n[656]\tvalid_0's quantile: 0.0235321\n[657]\tvalid_0's quantile: 0.0235198\n[658]\tvalid_0's quantile: 0.023495\n[659]\tvalid_0's quantile: 0.0234886\n[660]\tvalid_0's quantile: 0.0234886\n[661]\tvalid_0's quantile: 0.0234811\n[662]\tvalid_0's quantile: 0.0234687\n[663]\tvalid_0's quantile: 0.0234557\n[664]\tvalid_0's quantile: 0.023449\n[665]\tvalid_0's quantile: 0.023448\n[666]\tvalid_0's quantile: 0.0234479\n[667]\tvalid_0's quantile: 0.023431\n[668]\tvalid_0's quantile: 0.0234272\n[669]\tvalid_0's quantile: 0.0234349\n[670]\tvalid_0's quantile: 0.0234317\n[671]\tvalid_0's quantile: 0.0234262\n[672]\tvalid_0's quantile: 0.0234237\n[673]\tvalid_0's quantile: 0.0234156\n[674]\tvalid_0's quantile: 0.0234151\n[675]\tvalid_0's quantile: 0.0234106\n[676]\tvalid_0's quantile: 0.0234051\n[677]\tvalid_0's quantile: 0.0234038\n[678]\tvalid_0's quantile: 0.0233911\n[679]\tvalid_0's quantile: 0.0233889\n[680]\tvalid_0's quantile: 0.0233826\n[681]\tvalid_0's quantile: 0.0233733\n[682]\tvalid_0's quantile: 0.023356\n[683]\tvalid_0's quantile: 0.0233499\n[684]\tvalid_0's quantile: 0.0233358\n[685]\tvalid_0's quantile: 0.0233333\n[686]\tvalid_0's quantile: 0.0233298\n[687]\tvalid_0's quantile: 0.023325\n[688]\tvalid_0's quantile: 0.0233193\n[689]\tvalid_0's quantile: 0.0233194\n[690]\tvalid_0's quantile: 0.0233295\n[691]\tvalid_0's quantile: 0.0233264\n[692]\tvalid_0's quantile: 0.0233224\n[693]\tvalid_0's quantile: 0.0233104\n[694]\tvalid_0's quantile: 0.0233103\n[695]\tvalid_0's quantile: 0.0233067\n[696]\tvalid_0's quantile: 0.0233037\n[697]\tvalid_0's quantile: 0.0233046\n[698]\tvalid_0's quantile: 0.0233007\n[699]\tvalid_0's quantile: 0.0232968\n[700]\tvalid_0's quantile: 0.0232867\n[701]\tvalid_0's quantile: 0.0232834\n[702]\tvalid_0's quantile: 0.0232803\n[703]\tvalid_0's quantile: 0.0232729\n[704]\tvalid_0's quantile: 0.0232732\n[705]\tvalid_0's quantile: 0.0232676\n[706]\tvalid_0's quantile: 0.0232653\n[707]\tvalid_0's quantile: 0.0232641\n[708]\tvalid_0's quantile: 0.0232639\n[709]\tvalid_0's quantile: 0.023264\n[710]\tvalid_0's quantile: 0.0232547\n[711]\tvalid_0's quantile: 0.0232518\n[712]\tvalid_0's quantile: 0.0232486\n[713]\tvalid_0's quantile: 0.0232457\n[714]\tvalid_0's quantile: 0.023241\n[715]\tvalid_0's quantile: 0.0232373\n[716]\tvalid_0's quantile: 0.0232312\n[717]\tvalid_0's quantile: 0.0232194\n[718]\tvalid_0's quantile: 0.0232095\n[719]\tvalid_0's quantile: 0.0232096\n[720]\tvalid_0's quantile: 0.0232123\n[721]\tvalid_0's quantile: 0.0232052\n[722]\tvalid_0's quantile: 0.0231994\n[723]\tvalid_0's quantile: 0.0232001\n[724]\tvalid_0's quantile: 0.0231977\n[725]\tvalid_0's quantile: 0.023193\n[726]\tvalid_0's quantile: 0.0231929\n[727]\tvalid_0's quantile: 0.0231884\n[728]\tvalid_0's quantile: 0.0231828\n[729]\tvalid_0's quantile: 0.0231766\n[730]\tvalid_0's quantile: 0.0231758\n[731]\tvalid_0's quantile: 0.0231681\n[732]\tvalid_0's quantile: 0.0231666\n[733]\tvalid_0's quantile: 0.0231611\n[734]\tvalid_0's quantile: 0.0231586\n[735]\tvalid_0's quantile: 0.0231301\n[736]\tvalid_0's quantile: 0.0231263\n[737]\tvalid_0's quantile: 0.0231222\n[738]\tvalid_0's quantile: 0.0231183\n[739]\tvalid_0's quantile: 0.0231132\n[740]\tvalid_0's quantile: 0.0231108\n[741]\tvalid_0's quantile: 0.0231064\n[742]\tvalid_0's quantile: 0.023105\n[743]\tvalid_0's quantile: 0.023105\n[744]\tvalid_0's quantile: 0.0231008\n[745]\tvalid_0's quantile: 0.023101\n[746]\tvalid_0's quantile: 0.0230962\n[747]\tvalid_0's quantile: 0.0230928\n[748]\tvalid_0's quantile: 0.0230901\n[749]\tvalid_0's quantile: 0.0230891\n[750]\tvalid_0's quantile: 0.0230788\n[751]\tvalid_0's quantile: 0.0230737\n[752]\tvalid_0's quantile: 0.0230701\n[753]\tvalid_0's quantile: 0.023059\n[754]\tvalid_0's quantile: 0.023055\n[755]\tvalid_0's quantile: 0.0230478\n[756]\tvalid_0's quantile: 0.0230444\n[757]\tvalid_0's quantile: 0.0230339\n[758]\tvalid_0's quantile: 0.0230339\n[759]\tvalid_0's quantile: 0.0230269\n[760]\tvalid_0's quantile: 0.0230259\n[761]\tvalid_0's quantile: 0.0230224\n[762]\tvalid_0's quantile: 0.0230213\n[763]\tvalid_0's quantile: 0.0230206\n[764]\tvalid_0's quantile: 0.02302\n[765]\tvalid_0's quantile: 0.0230183\n[766]\tvalid_0's quantile: 0.0230131\n[767]\tvalid_0's quantile: 0.0230015\n[768]\tvalid_0's quantile: 0.023\n[769]\tvalid_0's quantile: 0.0229982\n[770]\tvalid_0's quantile: 0.0229955\n[771]\tvalid_0's quantile: 0.022988\n[772]\tvalid_0's quantile: 0.022988\n[773]\tvalid_0's quantile: 0.0229784\n[774]\tvalid_0's quantile: 0.0229759\n[775]\tvalid_0's quantile: 0.022973\n[776]\tvalid_0's quantile: 0.0229597\n[777]\tvalid_0's quantile: 0.0229577\n[778]\tvalid_0's quantile: 0.0229412\n[779]\tvalid_0's quantile: 0.0229311\n[780]\tvalid_0's quantile: 0.0229303\n[781]\tvalid_0's quantile: 0.0229187\n[782]\tvalid_0's quantile: 0.0229168\n[783]\tvalid_0's quantile: 0.0229031\n[784]\tvalid_0's quantile: 0.0228765\n[785]\tvalid_0's quantile: 0.0228604\n[786]\tvalid_0's quantile: 0.0228586\n[787]\tvalid_0's quantile: 0.0228587\n[788]\tvalid_0's quantile: 0.0228426\n[789]\tvalid_0's quantile: 0.0228383\n[790]\tvalid_0's quantile: 0.0228389\n[791]\tvalid_0's quantile: 0.0228338\n[792]\tvalid_0's quantile: 0.0228245\n[793]\tvalid_0's quantile: 0.0228248\n[794]\tvalid_0's quantile: 0.0228023\n[795]\tvalid_0's quantile: 0.0228018\n[796]\tvalid_0's quantile: 0.0227818\n[797]\tvalid_0's quantile: 0.0227735\n[798]\tvalid_0's quantile: 0.0227727\n[799]\tvalid_0's quantile: 0.0227711\n[800]\tvalid_0's quantile: 0.022768\n[801]\tvalid_0's quantile: 0.0227487\n[802]\tvalid_0's quantile: 0.0227472\n[803]\tvalid_0's quantile: 0.0227239\n[804]\tvalid_0's quantile: 0.0227182\n[805]\tvalid_0's quantile: 0.0227132\n[806]\tvalid_0's quantile: 0.0227028\n[807]\tvalid_0's quantile: 0.0226984\n[808]\tvalid_0's quantile: 0.0226986\n[809]\tvalid_0's quantile: 0.0226968\n[810]\tvalid_0's quantile: 0.022693\n[811]\tvalid_0's quantile: 0.0226916\n[812]\tvalid_0's quantile: 0.0226939\n[813]\tvalid_0's quantile: 0.0226731\n[814]\tvalid_0's quantile: 0.0226667\n[815]\tvalid_0's quantile: 0.0226644\n[816]\tvalid_0's quantile: 0.0226631\n[817]\tvalid_0's quantile: 0.0226587\n[818]\tvalid_0's quantile: 0.0226515\n[819]\tvalid_0's quantile: 0.0226521\n[820]\tvalid_0's quantile: 0.0226522\n[821]\tvalid_0's quantile: 0.0226273\n[822]\tvalid_0's quantile: 0.0226264\n[823]\tvalid_0's quantile: 0.0226244\n[824]\tvalid_0's quantile: 0.0226053\n[825]\tvalid_0's quantile: 0.0226037\n[826]\tvalid_0's quantile: 0.0225761\n[827]\tvalid_0's quantile: 0.0225622\n[828]\tvalid_0's quantile: 0.0225643\n[829]\tvalid_0's quantile: 0.0225495\n[830]\tvalid_0's quantile: 0.0225278\n[831]\tvalid_0's quantile: 0.0225122\n[832]\tvalid_0's quantile: 0.0225036\n[833]\tvalid_0's quantile: 0.022504\n[834]\tvalid_0's quantile: 0.0224914\n[835]\tvalid_0's quantile: 0.0224789\n[836]\tvalid_0's quantile: 0.0224792\n[837]\tvalid_0's quantile: 0.0224666\n[838]\tvalid_0's quantile: 0.0224605\n[839]\tvalid_0's quantile: 0.0224386\n[840]\tvalid_0's quantile: 0.0224381\n[841]\tvalid_0's quantile: 0.0224382\n[842]\tvalid_0's quantile: 0.0224387\n[843]\tvalid_0's quantile: 0.0224349\n[844]\tvalid_0's quantile: 0.02243\n[845]\tvalid_0's quantile: 0.02243\n[846]\tvalid_0's quantile: 0.0224292\n[847]\tvalid_0's quantile: 0.0224264\n[848]\tvalid_0's quantile: 0.0224186\n[849]\tvalid_0's quantile: 0.0224104\n[850]\tvalid_0's quantile: 0.0224085\n[851]\tvalid_0's quantile: 0.0223973\n[852]\tvalid_0's quantile: 0.0223943\n[853]\tvalid_0's quantile: 0.0223946\n[854]\tvalid_0's quantile: 0.0223876\n[855]\tvalid_0's quantile: 0.0223855\n[856]\tvalid_0's quantile: 0.0223855\n[857]\tvalid_0's quantile: 0.022382\n[858]\tvalid_0's quantile: 0.0223804\n[859]\tvalid_0's quantile: 0.0223791\n[860]\tvalid_0's quantile: 0.0223768\n[861]\tvalid_0's quantile: 0.0223793\n[862]\tvalid_0's quantile: 0.0223739\n[863]\tvalid_0's quantile: 0.0223657\n[864]\tvalid_0's quantile: 0.0223677\n[865]\tvalid_0's quantile: 0.0223665\n[866]\tvalid_0's quantile: 0.0223652\n[867]\tvalid_0's quantile: 0.022365\n[868]\tvalid_0's quantile: 0.0223651\n[869]\tvalid_0's quantile: 0.0223646\n[870]\tvalid_0's quantile: 0.0223605\n[871]\tvalid_0's quantile: 0.0223583\n[872]\tvalid_0's quantile: 0.0223544\n[873]\tvalid_0's quantile: 0.0223497\n[874]\tvalid_0's quantile: 0.0223489\n[875]\tvalid_0's quantile: 0.022349\n[876]\tvalid_0's quantile: 0.0223467\n[877]\tvalid_0's quantile: 0.0223458\n[878]\tvalid_0's quantile: 0.022348\n[879]\tvalid_0's quantile: 0.0223429\n[880]\tvalid_0's quantile: 0.0223423\n[881]\tvalid_0's quantile: 0.022342\n[882]\tvalid_0's quantile: 0.0223381\n[883]\tvalid_0's quantile: 0.0223378\n[884]\tvalid_0's quantile: 0.0223324\n[885]\tvalid_0's quantile: 0.0223281\n[886]\tvalid_0's quantile: 0.0223283\n[887]\tvalid_0's quantile: 0.0223235\n[888]\tvalid_0's quantile: 0.0223235\n[889]\tvalid_0's quantile: 0.0223794\n[890]\tvalid_0's quantile: 0.0223605\n[891]\tvalid_0's quantile: 0.022359\n[892]\tvalid_0's quantile: 0.0223559\n[893]\tvalid_0's quantile: 0.0223496\n[894]\tvalid_0's quantile: 0.0223496\n[895]\tvalid_0's quantile: 0.022349\n[896]\tvalid_0's quantile: 0.0223471\n[897]\tvalid_0's quantile: 0.0223404\n[898]\tvalid_0's quantile: 0.0222334\n[899]\tvalid_0's quantile: 0.0222343\n[900]\tvalid_0's quantile: 0.022235\n[901]\tvalid_0's quantile: 0.0222309\n[902]\tvalid_0's quantile: 0.0222297\n[903]\tvalid_0's quantile: 0.0222279\n[904]\tvalid_0's quantile: 0.0222271\n[905]\tvalid_0's quantile: 0.0222202\n[906]\tvalid_0's quantile: 0.0222198\n[907]\tvalid_0's quantile: 0.0222198\n[908]\tvalid_0's quantile: 0.0222204\n[909]\tvalid_0's quantile: 0.0222189\n[910]\tvalid_0's quantile: 0.0222174\n[911]\tvalid_0's quantile: 0.0222168\n[912]\tvalid_0's quantile: 0.022217\n[913]\tvalid_0's quantile: 0.022215\n[914]\tvalid_0's quantile: 0.0222042\n[915]\tvalid_0's quantile: 0.0222028\n[916]\tvalid_0's quantile: 0.0221994\n[917]\tvalid_0's quantile: 0.0221967\n[918]\tvalid_0's quantile: 0.0221911\n[919]\tvalid_0's quantile: 0.0221895\n[920]\tvalid_0's quantile: 0.0221876\n[921]\tvalid_0's quantile: 0.022189\n[922]\tvalid_0's quantile: 0.0221897\n[923]\tvalid_0's quantile: 0.0221897\n[924]\tvalid_0's quantile: 0.0221888\n[925]\tvalid_0's quantile: 0.022187\n[926]\tvalid_0's quantile: 0.0221846\n[927]\tvalid_0's quantile: 0.0221847\n[928]\tvalid_0's quantile: 0.0221732\n[929]\tvalid_0's quantile: 0.0221696\n[930]\tvalid_0's quantile: 0.022163\n[931]\tvalid_0's quantile: 0.022163\n[932]\tvalid_0's quantile: 0.0221572\n[933]\tvalid_0's quantile: 0.0221547\n[934]\tvalid_0's quantile: 0.022148\n[935]\tvalid_0's quantile: 0.0221394\n[936]\tvalid_0's quantile: 0.0221367\n[937]\tvalid_0's quantile: 0.0221331\n[938]\tvalid_0's quantile: 0.0221296\n[939]\tvalid_0's quantile: 0.0221222\n[940]\tvalid_0's quantile: 0.0221218\n[941]\tvalid_0's quantile: 0.0221219\n[942]\tvalid_0's quantile: 0.022121\n[943]\tvalid_0's quantile: 0.0221224\n[944]\tvalid_0's quantile: 0.0221202\n[945]\tvalid_0's quantile: 0.0221187\n[946]\tvalid_0's quantile: 0.0221113\n[947]\tvalid_0's quantile: 0.0221115\n[948]\tvalid_0's quantile: 0.0221123\n[949]\tvalid_0's quantile: 0.0221102\n[950]\tvalid_0's quantile: 0.022107\n[951]\tvalid_0's quantile: 0.0221071\n[952]\tvalid_0's quantile: 0.0221047\n[953]\tvalid_0's quantile: 0.0221068\n[954]\tvalid_0's quantile: 0.0221048\n[955]\tvalid_0's quantile: 0.0221049\n[956]\tvalid_0's quantile: 0.0221049\n[957]\tvalid_0's quantile: 0.0221049\n[958]\tvalid_0's quantile: 0.0221072\n[959]\tvalid_0's quantile: 0.022104\n[960]\tvalid_0's quantile: 0.022102\n[961]\tvalid_0's quantile: 0.0221005\n[962]\tvalid_0's quantile: 0.022102\n[963]\tvalid_0's quantile: 0.0220983\n[964]\tvalid_0's quantile: 0.022098\n[965]\tvalid_0's quantile: 0.0220947\n[966]\tvalid_0's quantile: 0.0220945\n[967]\tvalid_0's quantile: 0.0220896\n[968]\tvalid_0's quantile: 0.0220905\n[969]\tvalid_0's quantile: 0.0220873\n[970]\tvalid_0's quantile: 0.0220883\n[971]\tvalid_0's quantile: 0.0220874\n[972]\tvalid_0's quantile: 0.022087\n[973]\tvalid_0's quantile: 0.0220842\n[974]\tvalid_0's quantile: 0.0220805\n[975]\tvalid_0's quantile: 0.0220805\n[976]\tvalid_0's quantile: 0.022078\n[977]\tvalid_0's quantile: 0.0220767\n[978]\tvalid_0's quantile: 0.0220768\n[979]\tvalid_0's quantile: 0.0220757\n[980]\tvalid_0's quantile: 0.0220729\n[981]\tvalid_0's quantile: 0.0220694\n[982]\tvalid_0's quantile: 0.0220692\n[983]\tvalid_0's quantile: 0.0220705\n[984]\tvalid_0's quantile: 0.0220624\n[985]\tvalid_0's quantile: 0.0220634\n[986]\tvalid_0's quantile: 0.0220631\n[987]\tvalid_0's quantile: 0.0220621\n[988]\tvalid_0's quantile: 0.0220622\n[989]\tvalid_0's quantile: 0.0220615\n[990]\tvalid_0's quantile: 0.0220576\n[991]\tvalid_0's quantile: 0.0220573\n[992]\tvalid_0's quantile: 0.0220572\n[993]\tvalid_0's quantile: 0.0220596\n[994]\tvalid_0's quantile: 0.0220581\n[995]\tvalid_0's quantile: 0.0220571\n[996]\tvalid_0's quantile: 0.0220553\n[997]\tvalid_0's quantile: 0.0220553\n[998]\tvalid_0's quantile: 0.0220506\n[999]\tvalid_0's quantile: 0.0220505\n[1000]\tvalid_0's quantile: 0.0220513\nElapsed time for fitting LightGBMQuantileRegressor model: 60.58 s\nEarly stopping performed. Best iteration: 866\nElapsed time for fitting LightGBMQuantileRegressor model: 64.33 s\npath: predictions/pointpredictions_lightgbm_quant.csv\ndirname: predictions\nfilename: pointpredictions_lightgbm_quant.csv\nartifact_path: predictions\npath: /tmp/tmpfb8not0c/pointpredictions_lightgbm_quant.csv\ntmp_path: /tmp/tmpfb8not0c/pointpredictions_lightgbm_quant.csv\npath: predictions/quantiles_lightgbm_quant0.1.csv\ndirname: predictions\nfilename: quantiles_lightgbm_quant0.1.csv\nartifact_path: predictions\npath: /tmp/tmpq3tymchr/quantiles_lightgbm_quant0.1.csv\ntmp_path: /tmp/tmpq3tymchr/quantiles_lightgbm_quant0.1.csv\npath: predictions/quantiles_lightgbm_quant0.5.csv\ndirname: predictions\nfilename: quantiles_lightgbm_quant0.5.csv\nartifact_path: predictions\npath: /tmp/tmp6igjb6um/quantiles_lightgbm_quant0.5.csv\ntmp_path: /tmp/tmp6igjb6um/quantiles_lightgbm_quant0.5.csv\npath: predictions/quantiles_lightgbm_quant0.9.csv\ndirname: predictions\nfilename: quantiles_lightgbm_quant0.9.csv\nartifact_path: predictions\npath: /tmp/tmpglp5rtg3/quantiles_lightgbm_quant0.9.csv\ntmp_path: /tmp/tmpglp5rtg3/quantiles_lightgbm_quant0.9.csv\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["lightgbm_quant_params = {\n","    \"boosting_type\": 'gbdt',\n","    \"objective\": 'quantile',\n","    \"n_jobs\": -1, \n","    \"min_split_gain\": 0.0,\n","    \"min_data_in_leaf\": 1,\n","    \"max_bin\": 1024,\n","    \"num_leaves\": 64, \n","    \"max_depth\": -1,\n","    \"learning_rate\": 0.1,\n","    \"n_estimators\": 1000,\n","    \"feature_fraction\": 0.7,\n","    \"bagging_fraction\": 0.7,\n","    \"seed\": 1,\n","    \"lambda\": 1,\n","    \"bagging_freq\": 1,  \n","}\n","\n","early_stopping_round = 20\n","quantiles = [0.1, 0.5, 0.9]\n","\n","start_time = time.perf_counter()\n","    \n","# fitting model on train set with early stopping on valid set\n","lightgbm_quant_reg = LightGBMQuantileRegressor(vectorizer_with_nan, target_transformer=target_transformer)\n","lightgbm_quant_fit_params = {**lightgbm_quant_params, \"early_stopping_round\": early_stopping_round}\n","lightgbm_quant_reg.fit(train_val_df, TARGET, X_val=valid_df, y_val=valid_df[TARGET], params=lightgbm_quant_fit_params, quantiles=quantiles, verbose=True)\n","lightgbm_quant_best_iteration = int(np.mean(list(lightgbm_quant_reg.best_iterations.values())))\n","print(\"Early stopping performed. Best iteration:\", lightgbm_quant_best_iteration)\n","\n","    # fitting model on train+val set with best_iteration\n","lightgbm_full_train_quant_reg = LightGBMQuantileRegressor(vectorizer_with_nan, target_transformer=target_transformer)\n","lightgbm_quant_full_train_params = {**lightgbm_quant_params, \"n_estimators\": lightgbm_quant_best_iteration}\n","lightgbm_full_train_quant_reg.fit(train_df, TARGET, params=lightgbm_quant_full_train_params, quantiles=quantiles, verbose=True)\n","lightgbm_full_train_quant_reg.best_iterations = lightgbm_quant_reg.best_iterations\n","\n","# predicting on test set with our fully trained model\n","lightgbm_quant_pred = lightgbm_full_train_quant_reg.predict(test_df)\n","lightgbm_quant_metrics = lightgbm_full_train_quant_reg.metrics(test_df[TARGET], lightgbm_quant_pred, confidence_interval_quantiles=[0.1, 0.9])\n","\n","end_time = time.perf_counter()\n","full_time = np.round(end_time - start_time, 2)\n","lightgbm_quant_metrics['time'] = full_time"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c9afabd2-d967-44d8-9787-369d0d880610","showTitle":false,"title":""}},"source":["## 3.4 NGBoost"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.4.1 NGBoost with NLL"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"53a6eec4-21ca-4afc-aaf8-d5c7f92b76b0","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["8682d5d5845b4100a66206c9ee356542\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[iter 0] loss=0.5667 val_loss=0.5267 scale=1.0000 norm=0.6523\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Cur\n","\n","*** WARNING: max output size exceeded, skipping output. ***\n","\n","alue: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n","[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n","[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n","Elapsed time for fitting NGBoost model: 1185.85 s\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/lidl_x_tum_uncertainty_estimation/uncertainty_estimation_models.py:552: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  nll_temp = torch.tensor([-dist[i].log_prob(torch.tensor(y_test[i])) for i in range(len(dist))])\n","path: predictions/pointpredictions_ngboost.csv\n","dirname: predictions\n","filename: pointpredictions_ngboost.csv\n","artifact_path: predictions\n","path: /tmp/tmp9whtxew8/pointpredictions_ngboost.csv\n","tmp_path: /tmp/tmp9whtxew8/pointpredictions_ngboost.csv\n","path: predictions/quantiles_ngboost0.05.csv\n","dirname: predictions\n","filename: quantiles_ngboost0.05.csv\n","artifact_path: predictions\n","path: /tmp/tmp8gkt1z81/quantiles_ngboost0.05.csv\n","tmp_path: /tmp/tmp8gkt1z81/quantiles_ngboost0.05.csv\n","path: predictions/quantiles_ngboost0.1.csv\n","dirname: predictions\n","filename: quantiles_ngboost0.1.csv\n","artifact_path: predictions\n","path: /tmp/tmp1n6cgds9/quantiles_ngboost0.1.csv\n","tmp_path: /tmp/tmp1n6cgds9/quantiles_ngboost0.1.csv\n","path: predictions/quantiles_ngboost0.5.csv\n","dirname: predictions\n","filename: quantiles_ngboost0.5.csv\n","artifact_path: predictions\n","path: /tmp/tmpsompqzp6/quantiles_ngboost0.5.csv\n","tmp_path: /tmp/tmpsompqzp6/quantiles_ngboost0.5.csv\n","path: predictions/quantiles_ngboost0.9.csv\n","dirname: predictions\n","filename: quantiles_ngboost0.9.csv\n","artifact_path: predictions\n","path: /tmp/tmpy_ez3vs6/quantiles_ngboost0.9.csv\n","tmp_path: /tmp/tmpy_ez3vs6/quantiles_ngboost0.9.csv\n","path: predictions/quantiles_ngboost0.95.csv\n","dirname: predictions\n","filename: quantiles_ngboost0.95.csv\n","artifact_path: predictions\n","path: /tmp/tmpmiy70hvt/quantiles_ngboost0.95.csv\n","tmp_path: /tmp/tmpmiy70hvt/quantiles_ngboost0.95.csv\n","path: predictions/samples_ngboost.csv\n","dirname: predictions\n","filename: samples_ngboost.csv\n","artifact_path: predictions\n","path: /tmp/tmpbb2pfjcx/samples_ngboost.csv\n","tmp_path: /tmp/tmpbb2pfjcx/samples_ngboost.csv\n","path: predictions/distparams_ngboostloc.csv\n","dirname: predictions\n","filename: distparams_ngboostloc.csv\n","artifact_path: predictions\n","path: /tmp/tmp6tbc11xb/distparams_ngboostloc.csv\n","tmp_path: /tmp/tmp6tbc11xb/distparams_ngboostloc.csv\n","path: predictions/distparams_ngboostscale.csv\n","dirname: predictions\n","filename: distparams_ngboostscale.csv\n","artifact_path: predictions\n","path: /tmp/tmpn_uhu5i7/distparams_ngboostscale.csv\n","tmp_path: /tmp/tmpn_uhu5i7/distparams_ngboostscale.csv\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"8682d5d5845b4100a66206c9ee356542\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[iter 0] loss=0.5667 val_loss=0.5267 scale=1.0000 norm=0.6523\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Cur\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nalue: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\n[LightGBM] [Warning] boosting is set=rf, boosting_type=gbdt will be ignored. Current value: boosting=rf\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] bagging_fraction is set=0.99, subsample=1.0 will be ignored. Current value: bagging_fraction=0.99\nElapsed time for fitting NGBoost model: 1185.85 s\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/lidl_x_tum_uncertainty_estimation/uncertainty_estimation_models.py:552: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  nll_temp = torch.tensor([-dist[i].log_prob(torch.tensor(y_test[i])) for i in range(len(dist))])\npath: predictions/pointpredictions_ngboost.csv\ndirname: predictions\nfilename: pointpredictions_ngboost.csv\nartifact_path: predictions\npath: /tmp/tmp9whtxew8/pointpredictions_ngboost.csv\ntmp_path: /tmp/tmp9whtxew8/pointpredictions_ngboost.csv\npath: predictions/quantiles_ngboost0.05.csv\ndirname: predictions\nfilename: quantiles_ngboost0.05.csv\nartifact_path: predictions\npath: /tmp/tmp8gkt1z81/quantiles_ngboost0.05.csv\ntmp_path: /tmp/tmp8gkt1z81/quantiles_ngboost0.05.csv\npath: predictions/quantiles_ngboost0.1.csv\ndirname: predictions\nfilename: quantiles_ngboost0.1.csv\nartifact_path: predictions\npath: /tmp/tmp1n6cgds9/quantiles_ngboost0.1.csv\ntmp_path: /tmp/tmp1n6cgds9/quantiles_ngboost0.1.csv\npath: predictions/quantiles_ngboost0.5.csv\ndirname: predictions\nfilename: quantiles_ngboost0.5.csv\nartifact_path: predictions\npath: /tmp/tmpsompqzp6/quantiles_ngboost0.5.csv\ntmp_path: /tmp/tmpsompqzp6/quantiles_ngboost0.5.csv\npath: predictions/quantiles_ngboost0.9.csv\ndirname: predictions\nfilename: quantiles_ngboost0.9.csv\nartifact_path: predictions\npath: /tmp/tmpy_ez3vs6/quantiles_ngboost0.9.csv\ntmp_path: /tmp/tmpy_ez3vs6/quantiles_ngboost0.9.csv\npath: predictions/quantiles_ngboost0.95.csv\ndirname: predictions\nfilename: quantiles_ngboost0.95.csv\nartifact_path: predictions\npath: /tmp/tmpmiy70hvt/quantiles_ngboost0.95.csv\ntmp_path: /tmp/tmpmiy70hvt/quantiles_ngboost0.95.csv\npath: predictions/samples_ngboost.csv\ndirname: predictions\nfilename: samples_ngboost.csv\nartifact_path: predictions\npath: /tmp/tmpbb2pfjcx/samples_ngboost.csv\ntmp_path: /tmp/tmpbb2pfjcx/samples_ngboost.csv\npath: predictions/distparams_ngboostloc.csv\ndirname: predictions\nfilename: distparams_ngboostloc.csv\nartifact_path: predictions\npath: /tmp/tmp6tbc11xb/distparams_ngboostloc.csv\ntmp_path: /tmp/tmp6tbc11xb/distparams_ngboostloc.csv\npath: predictions/distparams_ngboostscale.csv\ndirname: predictions\nfilename: distparams_ngboostscale.csv\nartifact_path: predictions\npath: /tmp/tmpn_uhu5i7/distparams_ngboostscale.csv\ntmp_path: /tmp/tmpn_uhu5i7/distparams_ngboostscale.csv\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["ngboost_base_params = {\n","    'boosting': 'rf',\n","    'n_estimators': 1,\n","    'bagging_fraction': 0.99,\n","    'bagging_freq': 1 \n","}\n","\n","learner = LGBMRegressor(**ngboost_base_params)\n","\n","ngboost_nll_params = {'Score':LogScore, \n","            'Base':learner, \n","            'natural_gradient':True,\n","            \"learning_rate\": 0.1,\n","            \"n_estimators\": 1000,\n","            \"col_sample\": 0.7, \n","            \"minibatch_frac\": 0.7, \n","            \"random_state\": 1, \n","                 } \n","\n","early_stopping_round = 20\n","quantiles = [0.05, 0.1, 0.5, 0.9, 0.95]\n","\n","start_time = time.perf_counter()\n","    \n","# fitting model on train set with early stopping on valid set\n","ngboost_nll_early_stopping_params = {**ngboost_nll_params, \"early_stopping_rounds\": early_stopping_round}\n","ngboost_nll_reg = NGBoost(vectorizer_without_nan, target_transformer=target_transformer, distribution=DistEnum.NORMAL, **ngboost_nll_early_stopping_params)\n","ngboost_nll_reg.fit(train_val_df, TARGET, X_val=valid_df, y_val=np.array(valid_df[TARGET]), verbose=True)\n","ngboost_nll_best_iteration = ngboost_nll_reg.best_iteration\n","print(\"Early stopping performed. Best iteration:\", ngboost_nll_best_iteration)\n","\n","# fitting model on train+val set with best_iteration\n","ngboost_nll_full_train_params = {**ngboost_nll_params, \"n_estimators\": ngboost_nll_best_iteration}\n","ngboost_nll_full_train_reg = NGBoost(vectorizer_without_nan, target_transformer=target_transformer, distribution=DistEnum.NORMAL, **ngboost_nll_full_train_params)\n","ngboost_nll_full_train_reg.fit(train_df, TARGET, verbose=True)\n","    \n","# predicting on test set with our fully trained model\n","ngboost_nll_pred = ngboost_nll_full_train_reg.predict(test_df, quantiles=quantiles, prediction_types=[PredEnum.POINT_ESTIMATES, PredEnum.QUANTILES, PredEnum.SAMPLES, PredEnum.DISTRIBUTION_PARAMS], sample_size=400)\n","\n","ngboost_nll_metrics = ngboost_nll_full_train_reg.metrics(np.array(test_df[TARGET]), ngboost_nll_pred, confidence_interval_quantiles=[0.1,0.9])\n","\n","end_time = time.perf_counter()\n","full_time = np.round(end_time - start_time, 2)\n","ngboost_nll_metrics['time'] = full_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ngboost_nll_metrics"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.4.2 NGBoost with CRPS"]},{"cell_type":"code","execution_count":1,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ae707eb1-e7ea-4add-9fbf-4b3bd1f3b5cb","showTitle":false,"title":""}},"outputs":[{"ename":"NameError","evalue":"name 'LGBMRegressor' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m ngboost_base_params \u001b[39m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mboosting\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mrf\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbagging_fraction\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.99\u001b[39m, \n\u001b[0;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbagging_freq\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m\n\u001b[0;32m      6\u001b[0m }\n\u001b[1;32m----> 8\u001b[0m learner \u001b[39m=\u001b[39m LGBMRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mngboost_base_params)\n\u001b[0;32m     10\u001b[0m ngboost_crps_params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m: CRPScore,\n\u001b[0;32m     11\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mBase\u001b[39m\u001b[39m'\u001b[39m:learner, \n\u001b[0;32m     12\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mnatural_gradient\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrandom_state\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m, \n\u001b[0;32m     18\u001b[0m                  }\n\u001b[0;32m     20\u001b[0m early_stopping_round \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n","\u001b[1;31mNameError\u001b[0m: name 'LGBMRegressor' is not defined"]}],"source":["ngboost_base_params = {\n","    'boosting': 'rf',\n","    'n_estimators': 1,\n","    'bagging_fraction': 0.99, \n","    'bagging_freq': 1\n","}\n","\n","learner = LGBMRegressor(**ngboost_base_params)\n","\n","ngboost_crps_params = {'Score': CRPScore,\n","            'Base':learner, \n","            'natural_gradient':True,\n","            \"learning_rate\": 0.1,\n","            \"n_estimators\": 1000,\n","            \"col_sample\": 0.7, \n","            \"minibatch_frac\": 0.7,    \n","            \"random_state\": 1, \n","                 }\n","\n","early_stopping_round = 20\n","quantiles = [0.05, 0.1, 0.5, 0.9, 0.95]\n","\n","start_time = time.perf_counter()\n","    \n","# fitting model on train set with early stopping on valid set\n","ngboost_crps_early_stopping_params = {**ngboost_crps_params, \"early_stopping_rounds\": early_stopping_round}\n","ngboost_crps_reg = NGBoost(vectorizer_without_nan, target_transformer=target_transformer, distribution=DistEnum.NORMAL, **ngboost_crps_early_stopping_params)\n","ngboost_crps_reg.fit(train_val_df, TARGET, X_val=valid_df, y_val=np.array(valid_df[TARGET]), verbose=True)\n","ngboost_crps_best_iteration = ngboost_crps_reg.best_iteration\n","print(\"Early stopping performed. Best iteration:\", ngboost_crps_best_iteration)\n","\n","# fitting model on train+val set with best_iteration\n","ngboost_crps_full_train_params = {**ngboost_crps_params, \"n_estimators\": ngboost_crps_best_iteration}\n","ngboost_crps_full_train_reg = NGBoost(vectorizer_without_nan, target_transformer=target_transformer, distribution=DistEnum.NORMAL, **ngboost_crps_full_train_params)\n","ngboost_crps_full_train_reg.fit(train_df, TARGET, verbose=True)\n","\n","# predicting on test set with our fully trained model\n","ngboost_crps_pred = ngboost_crps_full_train_reg.predict(test_df, quantiles=quantiles, prediction_types=[PredEnum.POINT_ESTIMATES, PredEnum.QUANTILES, PredEnum.SAMPLES, PredEnum.DISTRIBUTION_PARAMS], sample_size=400)\n","\n","ngboost_crps_metrics = ngboost_crps_full_train_reg.metrics(np.array(test_df[TARGET]), ngboost_crps_pred, confidence_interval_quantiles=[0.1,0.9])\n","\n","end_time = time.perf_counter()\n","full_time = np.round(end_time - start_time, 2)\n","ngboost_crps_metrics['time'] = full_time"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bee68a57-ad43-45c3-b4db-2f00cd20139f","showTitle":false,"title":""}},"source":["## 3.5 PGBM"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.5.1 PGBM normal"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0e5f09d2-3454-4ff3-b0ee-ae15f545c6db","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["6be1ae84f6cf442db89f8b3ab5eb5287\n","Training on GPU\n","Estimator 0/1000, Train metric: 0.4159, Validation metric: 0.4036\n","Estimator 1/1000, Train metric: 0.4064, Validation metric: 0.3944\n","Estimator 2/1000, Train metric: 0.3979, Validation metric: 0.3861\n","Estimator 3/1000, Train metric: 0.3906, Validation metric: 0.3795\n","Estimator 4/1000, Train metric: 0.3868, Validation metric: 0.3764\n","Estimator 5/1000, Train metric: 0.3807, Validation metric: 0.3705\n","Estimator 6/1000, Train metric: 0.3765, Validation metric: 0.3670\n","Estimator 7/1000, Train metric: 0.3710, Validation metric: 0.3618\n","Estimator 8/1000, Train metric: 0.3656, Validation metric: 0.3570\n","Estimator 9/1000, Train metric: 0.3606, Validation metric: 0.3524\n","Estimator 10/1000, Train metric: 0.3566, Validation metric: 0.3489\n","Estimator 11/1000, Train metric: 0.3530, Validation metric: 0.3461\n","Estimator 12/1000, Train metric: 0.3502, Validation metric: 0.3444\n","Estimator 13/1000, Train metric: 0.3486, Validation metric: 0.3436\n","Estimator 14/1000, Train metric: 0.3454, Validation metric: 0.3403\n","Estimator 15/1000, Train metric: 0.3428, Validation metric: 0.3380\n","Estimator 16/1000, Train metric: 0.3408, Validation metric: 0.3365\n","Estimator 17/1000, Train metric: 0.3380, Validation metric: 0.3340\n","Estimator 18/1000, Train metric: 0.3367, Validation metric: 0.3330\n","Estimator 19/1000, Train metric: 0.3355, Validation metric: 0.3322\n","Estimator 20/1000, Train metric: 0.3332, Validation metric: 0.3302\n","Estimator 21/1000, Train metric: 0.3314, Validation metric: 0.3285\n","Estimator 22/1000, Train metric: 0.3300, Validation metric: 0.3272\n","Estimator 23/1000, Train metric: 0.3284, Validation metric: 0.3263\n","Estimator 24/1000, Train metric: 0.3267, Validation metric: 0.3251\n","Estimator 25/1000, Train metric: 0.3253, Validation metric: 0.3237\n","Estimator 26/1000, Train metric: 0.3243, Validation metric: 0.3225\n","Estimator 27/1000, Train metric: 0.3234, Validation metric: 0.3214\n","Estimator 28/1000, Train metric: 0.3220, Validation metric: 0.3202\n","Estimator 29/1000, Train metric: 0.3212, Validation metric: 0.3192\n","Estimator 30/1000, Train metric: 0.3194, Validation metric: 0.3176\n","Estimator 31/1000, Train metric: 0.3180, Validation metric: 0.3167\n","Estimator 32/1000, Train metric: 0.3174, Validation metric: 0.3161\n","Estimator 33/1000, Train metric: 0.3157, Validation metric: 0.3143\n","Estimator 34/1000, Train metric: 0.3143, Validation metric: 0.3129\n","Estimator 35/1000, Train metric: 0.3133, Validation metric: 0.3120\n","Estimator 36/1000, Train metric: 0.3109, Validation metric: 0.3095\n","Estimator 37/1000, Train metric: 0.3105, Validation metric: 0.3090\n","Estimator 38/1000, Train metric: 0.3084, Validation metric: 0.3070\n","Estimator 39/1000, Train metric: 0.3071, Validation metric: 0.3057\n","Estimator 40/1000, Train metric: 0.3060, Validation metric: 0.3053\n","Estimator 41/1000, Train metric: 0.3055, Validation metric: 0.3049\n","Estimator 42/1000, Train metric: 0.3043, Validation metric: 0.3038\n","Estimator 43/1000, Train metric: 0.3026, Validation metric: 0.3022\n","Estimator 44/1000, Train metric: 0.3023, Validation metric: 0.3019\n","Estimator 45/1000, Train metric: 0.3004, Validation metric: 0.3000\n","Estimator 46/1000, Train metric: 0.2992, Validation metric: 0.2990\n","Estimator 47/1000, Train metric: 0.2988, Validation metric: 0.2981\n","Estimator 48/1000, Train metric: 0.2983, Validation metric: 0.2975\n","Estimator 49/1000, Train metric: 0.2976, Validation metric: 0.2968\n","Estimator 50/1000, Train metric: 0.2973, Validation metric: 0.2966\n","Estimator 51/1000, Train metric: 0.2964, Validation metric: 0.2956\n","Estimator 52/1000, Train metric: 0.2950, Validation metric: 0.2945\n","Estimator 53/1000, Train metric: 0.2920, Validation metric: 0.2916\n","Estimator 54/1000, Train metric: 0.2910, Validation metric: 0.2905\n","Estimator 55/1000, Train metric: 0.2900, Validation metric: 0.2896\n","Estimator 56/1000, Train metric: 0.2897, Validation metric: 0.2894\n","Estimator 57/1000, Train metric: 0.2874, Validation metric: 0.2872\n","Estimator 58/1000, Train metric: 0.2868, Validation metric: 0.2867\n","Estimator 59/1000, Train metric: 0.2854, Validation metric: 0.2855\n","Estimator 60/1000, Train metric: 0.2850, Validation metric: 0.2848\n","Estimator 61/1000, Train metric: 0.2839, Validation metric: 0.2837\n","Estimator 62/1000, Train metric: 0.2825, Validation metric: 0.2825\n","Estimator 63/1000, Train metric: 0.2822, Validation metric: 0.2823\n","Estimator 64/1000, Train metric: 0.2818, Validation metric: 0.2819\n","Estimator 65/1000, Train metric: 0.2808, Validation metric: 0.2808\n","Estimator 66/1000, Train metric: 0.2801, Validation metric: 0.2801\n","Estimator 67/1000, Train metric: 0.2795, Validation metric: 0.2796\n","Estimator 68/1000, Train metric: 0.2792, Validation metric: 0.2792\n","Estimator 69/1000, Train metric: 0.2775, Validation metric: 0.2778\n","Estimator 70/1000, Train metric: 0.2772, Validation metric: 0.2774\n","Estimator 71/1000, Train metric: 0.2761, Validation metric: 0.2764\n","Estimator 72/1000, Train metric: 0.2744, Validation metric: 0.2748\n","Estimator 73/1000, Train metric: 0.2742, Validation metric: 0.2751\n","Estimator 74/1000, Train metric: 0.2733, Validation metric: 0.2744\n","Estimator 75/1000, Train metric: 0.2731, Validation metric: 0.2742\n","Estimator 76/1000, Train metric: 0.2725, Validation metric: 0.2737\n","Estimator 77/1000, Train metric: 0.2712, Validation metric: 0.2725\n","Estimator 78/1000, Train metric: 0.2711, Validation metric: 0.2723\n","Estimator 79/1000, Train metric: 0.2708, Validation metric: 0.2720\n","Estimator 80/1000, Train metric: 0.2703, Validation metric: 0.2714\n","Estimator 81/1000, Train metric: 0.2675, Validation metric: 0.2687\n","Estimator 82/1000, Train metric: 0.2663, Validation metric: 0.2675\n","Estimator 83/1000, Train metric: 0.2650, Validation metric: 0.2663\n","Estimator 84/1000, Train metric: 0.2628, Validation metric: 0.2641\n","Estimator 85/1000, Train metric: 0.2624, Validation metric: 0.2637\n","Estimator 86/1000, Train metric: 0.2607, Validation metric: 0.2620\n","Estimator 87/1000, Train metric: 0.2596, Validation metric: 0.2609\n","Estimator 88/1000, Train metric: 0.2593, Validation metric: 0.2604\n","Estimator 89/1000, Train metric: 0.2584, Validation metric: 0.2595\n","Estimator 90/1000, Train metric: 0.2570, Validation metric: 0.2582\n","Estimator 91/1000, Train metric: 0.2567, Validation metric: 0.2574\n","Estimator 92/1000, Train metric: 0.2553, Validation metric: 0.2562\n","Estimator 93/1000, Train metric: 0.2541, Validation metric: 0.2550\n","Estimator 94/1000, Train metric: 0.2539, Validation metric: 0.2548\n","Estimator 95/1000, Train metric: 0.2535, Validation metric: 0.2545\n","Estimator 96/1000, Train metric: 0.2530, Validation metric: 0.2540\n","Estimator 97/1000, Train metric: 0.2526, Validation metric: 0.2536\n","Estimator 98/1000, Train metric: 0.2523, Validation metric: 0.2533\n","Estimator 99/1000, Train metric: 0.2500, Validation metric: 0.2512\n","Estimator 100/1000, Train metric: 0.2484, Validation metric: 0.2496\n","Estimator 101/1000, Train metric: 0.2476, Validation metric: 0.2489\n","Estimator 102/1000, Train metric: 0.2464, Validation metric: 0.2477\n","Estimator 103/1000, Train metric: 0.2455, Validation metric: 0.2469\n","Estimator 104/1000, Train metric: 0.2448, Validation metric: 0.2462\n","Estimator 105/1000, Train metric: 0.2446, Validation metric: 0.2463\n","Estimator 106/1000, Train metric: 0.2444, Validation metric: 0.2461\n","Estimator 107/1000, Train metric: 0.2434, Validation metric: 0.2453\n","Estimator 108/1000, Train metric: 0.2432, Validation metric: 0.2451\n","Estimator 109/1000, Train metric: 0.2428, Validation metric: 0.2446\n","Estimator 110/1000, Train metric: 0.2426, Validation metric: 0.2445\n","Estimator 111/1000, Train metric: 0.2424, Validation metric: 0.2442\n","Estimator 112/1000, Train metric: 0.2419, Validation metric: 0.2438\n","Estimator 113/1000, Train metric: 0.2409, Validation metric: 0.2428\n","Estimator 114/1000, Train metric: 0.2406, Validation metric: 0.2425\n","Estimator 115/1000, Train metric: 0.2403, Validation metric: 0.2422\n","Estimator 116/1000, Train metric: 0.2401, Validation metric: 0.2420\n","Estimator 117/1000, Train metric: 0.2385, Validation metric: 0.2406\n","Estimator 118/1000, Train metric: 0.2380, Validation metric: 0.2400\n","Estimator 119/1000, Train metric: 0.2378, Validation metric: 0.2399\n","Estimator 120/1000, Train metric: 0.2373, Validation metric: 0.2393\n","Estimator 121/1000, Train metric: 0.2360, Validation metric: 0.2381\n","Estimator 122/1000, Train metric: 0.2352, Validation metric: 0.2374\n","Estimator 123/1000, Train metric: 0.2350, Validation metric: 0.2374\n","Estimator 124/1000, Train metric: 0.2346, Validation metric: 0.2370\n","Estimator 125/1000, Train metric: 0.2341, Validation metric: 0.2365\n","Estimator 126/1000, Train metric: 0.2339, Validation metric: 0.2363\n","Estimator 127/1000, Train metric: 0.2334, Validation metric: 0.2358\n","Estimator 128/1000, Train metric: 0.2328, Validation metric: 0.2352\n","Estimator 129/1000, Train metric: 0.2315, Validation metric: 0.2341\n","Estimator 130/1000, Train metric: 0.2306, Validation metric: 0.2333\n","Estimator 131/1000, Train metric: 0.2305, Validation metric: 0.2331\n","Estimator 132/1000, Train metric: 0.2301, Validation metric: 0.2329\n","Estimator 133/1000, Train metric: 0.2299, Validation metric: 0.2329\n","Estimator 134/1000, Train metric: 0.2298, Validation metric: 0.2327\n","Estimator 135/1000, Train metric: 0.2291, Validation metric: 0.2320\n","Estimator 136/1000, Train metric: 0.2289, Validation metric: 0.2319\n","Estimator 137/1000, Train metric: 0.2287, Validation metric: 0.2318\n","Estimator 138/1000, Train metric: 0.2285, Validation metric: 0.2316\n","Estimator 139/1000, Train metric: 0.2281, Validation metric: 0.2312\n","Estimator 140/1000, Train metric: 0.2276, Validation metric: 0.2306\n","Estimator 141/1000, Train metric: 0.2274, Validation metric: 0.2304\n","Estimator 142/1000, Train metric: 0.2272, Validation metric: 0.2303\n","Estimator 143/1000, Train metric: 0.2260, Validation metric: 0.2291\n","Estimator 144/1000, Train metric: 0.2253, Validation metric: 0.2285\n","Estimator 145/1000, Train metric: 0.2243, Validation metric: 0.2275\n","Estimator 146/1000, Train metric: 0.2238, Validation metric: 0.2272\n","Estimator 147/1000, Train metric: 0.2237, Validation metric: 0.2270\n","Estimator 148/1000, Train metric: 0.2235, Validation metric: 0.2267\n","Estimator 149/1000, Train metric: 0.2233, Validation metric: 0.2267\n","Estimator 150/1000, Train metric: 0.2228, Validation metric: 0.2262\n","Estimator 151/1000, Train metric: 0.2213, Validation metric: 0.2249\n","Estimator 152/1000, Train metric: 0.2211, Validation metric: 0.2248\n","Estimator 153/1000, Train metric: 0.2209, Validation metric: 0.2246\n","Estimator 154/1000, Train metric: 0.2200, Validation metric: 0.2237\n","Estimator 155/1000, Train metric: 0.2191, Validation metric: 0.2229\n","Estimator 156/1000, Train metric: 0.2190, Validation metric: 0.2227\n","Estimator 157/1000, Train metric: 0.2186, Validation metric: 0.2224\n","Estimator 158/1000, Train metric: 0.2183, Validation metric: 0.2221\n","Estimator 159/1000, Train metric: 0.2175, Validation metric: 0.2214\n","Estimator 160/1000, Train metric: 0.2169, Validation metric: 0.2209\n","Estimator 161/1000, Train metric: 0.2164, Validation metric: 0.2204\n","Estimator 162/1000, Train metric: 0.2153, Validation metric: 0.2193\n","Estimator 163/1000, Train metric: 0.2148, Validation metric: 0.2188\n","Estimator 164/1000, Train metric: 0.2145, Validation metric: 0.2185\n","Estimator 165/1000, Train metric: 0.2143, Validation metric: 0.2183\n","Estimator 166/1000, Train metric: 0.2142, Validation metric: 0.2183\n","Estimator 167/1000, Train metric: 0.2133, Validation metric: 0.2175\n","Estimator 168/1000, Train metric: 0.2125, Validation metric: 0.2166\n","Estimator 169/1000, Train metric: 0.2120, Validation metric: 0.2161\n","Estimator 170/1000, Train metric: 0.2119, Validation metric: 0.2160\n","Estimator 171/1000, Train metric: 0.2112, Validation metric: 0.2153\n","Estimator 172/1000, Train metric: 0.2107, Validation metric: 0.2147\n","Estimator 173/1000, Train metric: 0.2102, Validation metric: 0.2142\n","Estimator 174/1000, Train metric: 0.2100, Validation metric: 0.2141\n","Estimator 175/1000, Train metric: 0.2096, Validation metric: 0.2137\n","Estimator 176/1000, Train metric: 0.2093, Validation metric: 0.2134\n","Estimator 177/1000, Train metric: 0.2092, Validation metric: 0.2133\n","Estimator 178/1000, Train metric: 0.2085, Validation metric: 0.2127\n","Estimator 179/1000, Train metric: 0.2078, Validation metric: 0.2121\n","Estimator 180/1000, Train metric: 0.2077, Validation metric: 0.2120\n","Estimator 181/1000, Train metric: 0.2075, Validation metric: 0.2119\n","Estimator 182/1000, Train metric: 0.2069, Validation metric: 0.2114\n","Estimator 183/1000, Train metric: 0.2067, Validation metric: 0.2112\n","Estimator 184/1000, Train metric: 0.2062, Validation metric: 0.2107\n","Estimator 185/1000, Train metric: 0.2051, Validation metric: 0.2096\n","Estimator 186/1000, Train metric: 0.2050, Validation metric: 0.2095\n","Estimator 187/1000, Train metric: 0.2049, Validation metric: 0.2094\n","Estimator 188/1000, Train metric: 0.2042, Validation metric: 0.2087\n","Estimator 189/1000, Train metric: 0.2031, Validation metric: 0.2078\n","Estimator 190/1000, Train metric: 0.2030, Validation metric: 0.2077\n","Estimator 191/1000, Train metric: 0.2023, Validation metric: 0.2070\n","Estimator 192/1000, Train metric: 0.2020, Validation metric: 0.2068\n","Estimator 193/1000, Train metric: 0.2014, Validation metric: 0.2061\n","Estimator 194/1000, Train metric: 0.2012, Validation metric: 0.2055\n","Estimator 195/1000, Train metric: 0.2007, Validation metric: 0.2051\n","Estimator 196/1000, Train metric: 0.2002, Validation metric: 0.2046\n","Estimator 197/1000, Train metric: 0.1995, Validation metric: 0.2040\n","Estimator 198/1000, Train metric: 0.1988, Validation metric: 0.2033\n","Estimator 199/1000, Train metric: 0.1988, Validation metric: 0.2033\n","Estimator 200/1000, Train metric: 0.1985, Validation metric: 0.2031\n","Estimator 201/1000, Train metric: 0.1985, Validation metric: 0.2031\n","Estimator 202/1000, Train metric: 0.1980, Validation metric: 0.2026\n","Estimator 203/1000, Train metric: 0.1971, Validation metric: 0.2018\n","Estimator 204/1000, Train metric: 0.1965, Validation metric: 0.2012\n","Estimator 205/1000, Train metric: 0.1961, Validation metric: 0.2009\n","Estimator 206/1000, Train metric: 0.1960, Validation metric: 0.2008\n","Estimator 207/1000, Train metric: 0.1957, Validation metric: 0.2005\n","Estimator 208/1000, Train metric: 0.1956, Validation metric: 0.2005\n","Estimator 209/1000, Train metric: 0.1950, Validation metric: 0.1999\n","Estimator 210/1000, Train metric: 0.1947, Validation metric: 0.1997\n","Estimator 211/1000, Train metric: 0.1939, Validation metric: 0.1988\n","Estimator 212/1000, Train metric: 0.1936, Validation metric: 0.1986\n","Estimator 213/1000, Train metric: 0.1930, Validation metric: 0.1980\n","Estimator 214/1000, Train metric: 0.1927, Validation metric: 0.1978\n","Estimator 215/1000, Train metric: 0.1926, Validation metric: 0.1978\n","Estimator 216/1000, Train metric: 0.1926, Validation metric: 0.1978\n","Estimator 217/1000, Train metric: 0.1924, Validation metric: 0.1976\n","Estimator 218/1000, Train metric: 0.1922, Validation metric: 0.1975\n","Estimator 219/1000, Train metric: 0.1922, Validation metric: 0.1974\n","Estimator 220/1000, Train metric: 0.1918, Validation metric: 0.1971\n","Estimator 221/1000, Train metric: 0.1914, Validation metric: 0.1967\n","Estimator 222/1000, Train metric: 0.1913, Validation metric: 0.1966\n","Estimator 223/1000, Train metric: 0.1903, Validation metric: 0.1957\n","Estimator 224/1000, Train metric: 0.1902, Validation metric: 0.1955\n","Estimator 225/1000, Train metric: 0.1895, Validation metric: 0.1949\n","Estimator 226/1000, Train metric: 0.1890, Validation metric: 0.1945\n","Estimator 227/1000, Train metric: 0.1887, Validation metric: 0.1942\n","Estimator 228/1000, Train metric: 0.1885, Validation metric: 0.1940\n","Estimator 229/1000, Train metric: 0.1884, Validation metric: 0.1939\n","Estimator 230/1000, Train metric: 0.1877, Validation metric: 0.1933\n","Estimator 231/1000, Train metric: 0.1872, Validation metric: 0.1928\n","Estimator 232/1000, Train metric: 0.1870, Validation metric: 0.1926\n","Estimator 233/1000, Train metric: 0.1863, Validation metric: 0.1919\n","Estimator 234/1000, Train metric: 0.1858, Validation metric: 0.1914\n","Estimator 235/1000, Train metric: 0.1852, Validation metric: 0.1909\n","Estimator 236/1000, Train metric: 0.1851, Validation metric: 0.1909\n","Estimator 237/1000, Train metric: 0.1849, Validation metric: 0.1906\n","Estimator 238/1000, Train metric: 0.1844, Validation metric: 0.1902\n","Estimator 239/1000, Train metric: 0.1843, Validation metric: 0.1901\n","Estimator 240/1000, Train metric: 0.1836, Validation metric: 0.1894\n","Estimator 241/1000, Train metric: 0.1835, Validation metric: 0.1895\n","Estimator 242/1000, Train metric: 0.1831, Validation metric: 0.1892\n","Estimator 243/1000, Train metric: 0.1828, Validation metric: 0.1889\n","Estimator 244/1000, Train metric: 0.1827, Validation metric: 0.1888\n","Estimator 245/1000, Train metric: 0.1826, Validation metric: 0.1888\n","Estimator 246/1000, Train metric: 0.1820, Validation metric: 0.1882\n","Estimator 247/1000, Train metric: 0.1819, Validation metric: 0.1882\n","Estimator 248/1000, Train metric: 0.1816, Validation metric: 0.1879\n","Estimator 249/1000, Train metric: 0.1813, Validation metric: 0.1877\n","Estimator 250/1000, Train metric: 0.1808, Validation metric: 0.1872\n","Estimator 251/1000, Train metric: 0.1803, Validation metric: 0.1867\n","Estimator 252/1000, Train metric: 0.1799, Validation metric: 0.1863\n","Estimator 253/1000, Train metric: 0.1794, Validation metric: 0.1859\n","Estimator 254/1000, Train metric: 0.1790, Validation metric: 0.1855\n","Estimator 255/1000, Train metric: 0.1786, Validation metric: 0.1851\n","Estimator 256/1000, Train metric: 0.1783, Validation metric: 0.1849\n","Estimator 257/1000, Train metric: 0.1782, Validation metric: 0.1848\n","Estimator 258/1000, Train metric: 0.1782, Validation metric: 0.1847\n","Estimator 259/1000, Train metric: 0.1778, Validation metric: 0.1844\n","Estimator 260/1000, Train metric: 0.1777, Validation metric: 0.1843\n","Estimator 261/1000, Train metric: 0.1774, Validation metric: 0.1840\n","Estimator 262/1000, Train metric: 0.1774, Validation metric: 0.1840\n","Estimator 263/1000, Train metric: 0.1769, Validation metric: 0.1835\n","Estimator 264/1000, Train metric: 0.1766, Validation metric: 0.1833\n","Estimator 265/1000, Train metric: 0.1762, Validation metric: 0.1829\n","Estimator 266/1000, Train metric: 0.1760, Validation metric: 0.1827\n","Estimator 267/1000, Train metric: 0.1759, Validation metric: 0.1827\n","Estimator 268/1000, Train metric: 0.1758, Validation metric: 0.1827\n","Estimator 269/1000, Train metric: 0.1757, Validation metric: 0.1826\n","Estimator 270/1000, Train metric: 0.1756, Validation metric: 0.1825\n","Estimator 271/1000, Train metric: 0.1752, Validation metric: 0.1822\n","Estimator 272/1000, Train metric: 0.1749, Validation metric: 0.1819\n","Estimator 273/1000, Train metric: 0.1744, Validation metric: 0.1814\n","Estimator 274/1000, Train metric: 0.1742, Validation metric: 0.1812\n","Estimator 275/1000, Train metric: 0.1736, Validation metric: 0.1807\n","Estimator 276/1000, Train metric: 0.1734, Validation metric: 0.1805\n","Estimator 277/1000, Train metric: 0.1732, Validation metric: 0.1803\n","Estimator 278/1000, Train metric: 0.1730, Validation metric: 0.1801\n","Estimator 279/1000, Train metric: 0.1725, Validation metric: 0.1797\n","Estimator 280/1000, Train metric: 0.1724, Validation metric: 0.1797\n","Estimator 281/1000, Train metric: 0.1723, Validation metric: 0.1797\n","Estimator 282/1000, Train metric: 0.1721, Validation metric: 0.1795\n","Estimator 283/1000, Train metric: 0.1717, Validation metric: 0.1791\n","Estimator 284/1000, Train metric: 0.1715, Validation metric: 0.1790\n","Estimator 285/1000, Train metric: 0.1714, Validation metric: 0.1787\n","Estimator 286/1000, Train metric: 0.1710, Validation metric: 0.1783\n","Estimator 287/1000, Train metric: 0.1707, Validation metric: 0.1780\n","Estimator 288/1000, Train metric: 0.1703, Validation metric: 0.1777\n","Estimator 289/1000, Train metric: 0.1701, Validation metric: 0.1775\n","Estimator 290/1000, Train metric: 0.1698, Validation metric: 0.1773\n","Estimator 291/1000, Train metric: 0.1694, Validation metric: 0.1769\n","Estimator 292/1000, Train metric: 0.1691, Validation metric: 0.1767\n","Estimator 293/1000, Train metric: 0.1688, Validation metric: 0.1763\n","Estimator 294/1000, Train metric: 0.1686, Validation metric: 0.1761\n","Estimator 295/1000, Train metric: 0.1685, Validation metric: 0.1761\n","Estimator 296/1000, Train metric: 0.1683, Validation metric: 0.1759\n","Estimator 297/1000, Train metric: 0.1680, Validation metric: 0.1757\n","Estimator 298/1000, Train metric: 0.1679, Validation metric: 0.1757\n","Estimator 299/1000, Train metric: 0.1678, Validation metric: 0.1757\n","Estimator 300/1000, Train metric: 0.1675, Validation metric: 0.1754\n","Estimator 301/1000, Train metric: 0.1673, Validation metric: 0.1753\n","Estimator 302/1000, Train metric: 0.1673, Validation metric: 0.1752\n","Estimator 303/1000, Train metric: 0.1671, Validation metric: 0.1750\n","Estimator 304/1000, Train metric: 0.1668, Validation metric: 0.1747\n","Estimator 305/1000, Train metric: 0.1665, Validation metric: 0.1744\n","Estimator 306/1000, Train metric: 0.1662, Validation metric: 0.1742\n","Estimator 307/1000, Train metric: 0.1656, Validation metric: 0.1737\n","Estimator 308/1000, Train metric: 0.1653, Validation metric: 0.1734\n","Estimator 309/1000, Train metric: 0.1651, Validation metric: 0.1732\n","Estimator 310/1000, Train metric: 0.1646, Validation metric: 0.1728\n","Estimator 311/1000, Train metric: 0.1644, Validation metric: 0.1726\n","Estimator 312/1000, Train metric: 0.1640, Validation metric: 0.1722\n","Estimator 313/1000, Train metric: 0.1640, Validation metric: 0.1722\n","Estimator 314/1000, Train metric: 0.1638, Validation metric: 0.1721\n","Estimator 315/1000, Train metric: 0.1636, Validation metric: 0.1719\n","Estimator 316/1000, Train metric: 0.1634, Validation metric: 0.1717\n","Estimator 317/1000, Train metric: 0.1633, Validation metric: 0.1716\n","Estimator 318/1000, Train metric: 0.1632, Validation metric: 0.1715\n","Estimator 319/1000, Train metric: 0.1628, Validation metric: 0.1712\n","Estimator 320/1000, Train metric: 0.1627, Validation metric: 0.1711\n","Estimator 321/1000, Train metric: 0.1626, Validation metric: 0.1710\n","Estimator 322/1000, Train metric: 0.1622, Validation metric: 0.1707\n","Estimator 323/1000, Train metric: 0.1618, Validation metric: 0.1704\n","Estimator 324/1000, Train metric: 0.1615, Validation metric: 0.1701\n","Estimator 325/1000, Train metric: 0.1614, Validation metric: 0.1701\n","Estimator 326/1000, Train metric: 0.1614, Validation metric: 0.1700\n","Estimator 327/1000, Train metric: 0.1613, Validation metric: 0.1700\n","Estimator 328/1000, Train metric: 0.1610, Validation metric: 0.1697\n","Estimator 329/1000, Train metric: 0.1608, Validation metric: 0.1695\n","Estimator 330/1000, Train metric: 0.1607, Validation metric: 0.1695\n","Estimator 331/1000, Train metric: 0.1606, Validation metric: 0.1695\n","Estimator 332/1000, Train metric: 0.1603, Validation metric: 0.1692\n","Estimator 333/1000, Train metric: 0.1602, Validation metric: 0.1690\n","Estimator 334/1000, Train metric: 0.1600, Validation metric: 0.1687\n","Estimator 335/1000, Train metric: 0.1598, Validation metric: 0.1686\n","Estimator 336/1000, Train metric: 0.1597, Validation metric: 0.1685\n","Estimator 337/1000, Train metric: 0.1596, Validation metric: 0.1684\n","Estimator 338/1000, Train metric: 0.1595, Validation metric: 0.1683\n","Estimator 339/1000, Train metric: 0.1594, Validation metric: 0.1684\n","Estimator 340/1000, Train metric: 0.1593, Validation metric: 0.1683\n","Estimator 341/1000, Train metric: 0.1590, Validation metric: 0.1681\n","Estimator 342/1000, Train metric: 0.1588, Validation metric: 0.1678\n","Estimator 343/1000, Train metric: 0.1586, Validation metric: 0.1676\n","Estimator 344/1000, Train metric: 0.1585, Validation metric: 0.1675\n","Estimator 345/1000, Train metric: 0.1583, Validation metric: 0.1674\n","Estimator 346/1000, Train metric: 0.1580, Validation metric: 0.1672\n","Estimator 347/1000, Train metric: 0.1580, Validation metric: 0.1671\n","Estimator 348/1000, Train metric: 0.1579, Validation metric: 0.1671\n","Estimator 349/1000, Train metric: 0.1579, Validation metric: 0.1671\n","Estimator 350/1000, Train metric: 0.1577, Validation metric: 0.1669\n","Estimator 351/1000, Train metric: 0.1575, Validation metric: 0.1667\n","Estimator 352/1000, Train metric: 0.1572, Validation metric: 0.1665\n","Estimator 353/1000, Train metric: 0.1570, Validation metric: 0.1663\n","Estimator 354/1000, Train metric: 0.1566, Validation metric: 0.1659\n","Estimator 355/1000, Train metric: 0.1563, Validation metric: 0.1656\n","Estimator 356/1000, Train metric: 0.1562, Validation metric: 0.1655\n","Estimator 357/1000, Train metric: 0.1560, Validation metric: 0.1654\n","Estimator 358/1000, Train metric: 0.1559, Validation metric: 0.1652\n","Estimator 359/1000, Train metric: 0.1558, Validation metric: 0.1652\n","Estimator 360/1000, Train metric: 0.1556, Validation metric: 0.1650\n","Estimator 361/1000, Train metric: 0.1554, Validation metric: 0.1651\n","Estimator 362/1000, Train metric: 0.1551, Validation metric: 0.1648\n","Estimator 363/1000, Train metric: 0.1550, Validation metric: 0.1646\n","Estimator 364/1000, Train metric: 0.1549, Validation metric: 0.1647\n","Estimator 365/1000, Train metric: 0.1549, Validation metric: 0.1647\n","Estimator 366/1000, Train metric: 0.1547, Validation metric: 0.1645\n","Estimator 367/1000, Train metric: 0.1546, Validation metric: 0.1644\n","Estimator 368/1000, Train metric: 0.1\n","\n","*** WARNING: max output size exceeded, skipping output. ***\n","\n","21\n","Estimator 478/1000, Train metric: 0.1419\n","Estimator 479/1000, Train metric: 0.1418\n","Estimator 480/1000, Train metric: 0.1417\n","Estimator 481/1000, Train metric: 0.1416\n","Estimator 482/1000, Train metric: 0.1415\n","Estimator 483/1000, Train metric: 0.1413\n","Estimator 484/1000, Train metric: 0.1412\n","Estimator 485/1000, Train metric: 0.1410\n","Estimator 486/1000, Train metric: 0.1409\n","Estimator 487/1000, Train metric: 0.1408\n","Estimator 488/1000, Train metric: 0.1408\n","Estimator 489/1000, Train metric: 0.1407\n","Estimator 490/1000, Train metric: 0.1406\n","Estimator 491/1000, Train metric: 0.1405\n","Estimator 492/1000, Train metric: 0.1404\n","Estimator 493/1000, Train metric: 0.1403\n","Estimator 494/1000, Train metric: 0.1402\n","Estimator 495/1000, Train metric: 0.1402\n","Estimator 496/1000, Train metric: 0.1399\n","Estimator 497/1000, Train metric: 0.1398\n","Estimator 498/1000, Train metric: 0.1398\n","Estimator 499/1000, Train metric: 0.1397\n","Estimator 500/1000, Train metric: 0.1396\n","Estimator 501/1000, Train metric: 0.1395\n","Estimator 502/1000, Train metric: 0.1392\n","Estimator 503/1000, Train metric: 0.1391\n","Estimator 504/1000, Train metric: 0.1391\n","Estimator 505/1000, Train metric: 0.1390\n","Estimator 506/1000, Train metric: 0.1390\n","Estimator 507/1000, Train metric: 0.1389\n","Estimator 508/1000, Train metric: 0.1388\n","Estimator 509/1000, Train metric: 0.1387\n","Estimator 510/1000, Train metric: 0.1386\n","Estimator 511/1000, Train metric: 0.1386\n","Estimator 512/1000, Train metric: 0.1384\n","Estimator 513/1000, Train metric: 0.1383\n","Estimator 514/1000, Train metric: 0.1383\n","Estimator 515/1000, Train metric: 0.1382\n","Estimator 516/1000, Train metric: 0.1381\n","Estimator 517/1000, Train metric: 0.1380\n","Estimator 518/1000, Train metric: 0.1379\n","Estimator 519/1000, Train metric: 0.1378\n","Estimator 520/1000, Train metric: 0.1377\n","Estimator 521/1000, Train metric: 0.1376\n","Estimator 522/1000, Train metric: 0.1375\n","Estimator 523/1000, Train metric: 0.1374\n","Estimator 524/1000, Train metric: 0.1373\n","Estimator 525/1000, Train metric: 0.1372\n","Estimator 526/1000, Train metric: 0.1372\n","Estimator 527/1000, Train metric: 0.1372\n","Estimator 528/1000, Train metric: 0.1371\n","Estimator 529/1000, Train metric: 0.1370\n","Estimator 530/1000, Train metric: 0.1369\n","Estimator 531/1000, Train metric: 0.1368\n","Estimator 532/1000, Train metric: 0.1368\n","Estimator 533/1000, Train metric: 0.1367\n","Estimator 534/1000, Train metric: 0.1367\n","Estimator 535/1000, Train metric: 0.1366\n","Estimator 536/1000, Train metric: 0.1366\n","Estimator 537/1000, Train metric: 0.1365\n","Estimator 538/1000, Train metric: 0.1365\n","Estimator 539/1000, Train metric: 0.1365\n","Estimator 540/1000, Train metric: 0.1363\n","Estimator 541/1000, Train metric: 0.1363\n","Estimator 542/1000, Train metric: 0.1362\n","Estimator 543/1000, Train metric: 0.1361\n","Estimator 544/1000, Train metric: 0.1361\n","Estimator 545/1000, Train metric: 0.1360\n","Estimator 546/1000, Train metric: 0.1359\n","Estimator 547/1000, Train metric: 0.1359\n","Estimator 548/1000, Train metric: 0.1358\n","Estimator 549/1000, Train metric: 0.1357\n","Estimator 550/1000, Train metric: 0.1357\n","Estimator 551/1000, Train metric: 0.1356\n","Estimator 552/1000, Train metric: 0.1356\n","Estimator 553/1000, Train metric: 0.1355\n","Estimator 554/1000, Train metric: 0.1354\n","Estimator 555/1000, Train metric: 0.1353\n","Estimator 556/1000, Train metric: 0.1352\n","Estimator 557/1000, Train metric: 0.1352\n","Estimator 558/1000, Train metric: 0.1351\n","Estimator 559/1000, Train metric: 0.1350\n","Estimator 560/1000, Train metric: 0.1350\n","Estimator 561/1000, Train metric: 0.1349\n","Estimator 562/1000, Train metric: 0.1348\n","Estimator 563/1000, Train metric: 0.1347\n","Estimator 564/1000, Train metric: 0.1346\n","Estimator 565/1000, Train metric: 0.1346\n","Estimator 566/1000, Train metric: 0.1345\n","Estimator 567/1000, Train metric: 0.1345\n","Estimator 568/1000, Train metric: 0.1344\n","Estimator 569/1000, Train metric: 0.1343\n","Estimator 570/1000, Train metric: 0.1342\n","Estimator 571/1000, Train metric: 0.1341\n","Estimator 572/1000, Train metric: 0.1341\n","Estimator 573/1000, Train metric: 0.1340\n","Estimator 574/1000, Train metric: 0.1339\n","Estimator 575/1000, Train metric: 0.1339\n","Estimator 576/1000, Train metric: 0.1338\n","Estimator 577/1000, Train metric: 0.1338\n","Estimator 578/1000, Train metric: 0.1337\n","Estimator 579/1000, Train metric: 0.1337\n","Estimator 580/1000, Train metric: 0.1337\n","Estimator 581/1000, Train metric: 0.1336\n","Estimator 582/1000, Train metric: 0.1336\n","Estimator 583/1000, Train metric: 0.1336\n","Estimator 584/1000, Train metric: 0.1335\n","Estimator 585/1000, Train metric: 0.1335\n","Estimator 586/1000, Train metric: 0.1334\n","Estimator 587/1000, Train metric: 0.1333\n","Estimator 588/1000, Train metric: 0.1333\n","Estimator 589/1000, Train metric: 0.1332\n","Estimator 590/1000, Train metric: 0.1331\n","Estimator 591/1000, Train metric: 0.1330\n","Estimator 592/1000, Train metric: 0.1329\n","Estimator 593/1000, Train metric: 0.1329\n","Estimator 594/1000, Train metric: 0.1329\n","Estimator 595/1000, Train metric: 0.1328\n","Estimator 596/1000, Train metric: 0.1328\n","Estimator 597/1000, Train metric: 0.1327\n","Estimator 598/1000, Train metric: 0.1326\n","Estimator 599/1000, Train metric: 0.1326\n","Estimator 600/1000, Train metric: 0.1325\n","Estimator 601/1000, Train metric: 0.1324\n","Estimator 602/1000, Train metric: 0.1324\n","Estimator 603/1000, Train metric: 0.1324\n","Estimator 604/1000, Train metric: 0.1323\n","Estimator 605/1000, Train metric: 0.1323\n","Estimator 606/1000, Train metric: 0.1323\n","Estimator 607/1000, Train metric: 0.1322\n","Estimator 608/1000, Train metric: 0.1320\n","Estimator 609/1000, Train metric: 0.1320\n","Estimator 610/1000, Train metric: 0.1319\n","Estimator 611/1000, Train metric: 0.1318\n","Estimator 612/1000, Train metric: 0.1318\n","Estimator 613/1000, Train metric: 0.1317\n","Estimator 614/1000, Train metric: 0.1317\n","Estimator 615/1000, Train metric: 0.1316\n","Estimator 616/1000, Train metric: 0.1316\n","Estimator 617/1000, Train metric: 0.1315\n","Estimator 618/1000, Train metric: 0.1315\n","Estimator 619/1000, Train metric: 0.1314\n","Estimator 620/1000, Train metric: 0.1313\n","Estimator 621/1000, Train metric: 0.1312\n","Estimator 622/1000, Train metric: 0.1311\n","Estimator 623/1000, Train metric: 0.1311\n","Estimator 624/1000, Train metric: 0.1310\n","Estimator 625/1000, Train metric: 0.1309\n","Estimator 626/1000, Train metric: 0.1309\n","Estimator 627/1000, Train metric: 0.1308\n","Estimator 628/1000, Train metric: 0.1308\n","Estimator 629/1000, Train metric: 0.1308\n","Estimator 630/1000, Train metric: 0.1307\n","Estimator 631/1000, Train metric: 0.1307\n","Estimator 632/1000, Train metric: 0.1306\n","Estimator 633/1000, Train metric: 0.1305\n","Estimator 634/1000, Train metric: 0.1305\n","Estimator 635/1000, Train metric: 0.1305\n","Estimator 636/1000, Train metric: 0.1304\n","Estimator 637/1000, Train metric: 0.1304\n","Estimator 638/1000, Train metric: 0.1303\n","Estimator 639/1000, Train metric: 0.1303\n","Estimator 640/1000, Train metric: 0.1302\n","Estimator 641/1000, Train metric: 0.1301\n","Estimator 642/1000, Train metric: 0.1300\n","Estimator 643/1000, Train metric: 0.1300\n","Estimator 644/1000, Train metric: 0.1300\n","Estimator 645/1000, Train metric: 0.1299\n","Estimator 646/1000, Train metric: 0.1299\n","Estimator 647/1000, Train metric: 0.1298\n","Estimator 648/1000, Train metric: 0.1298\n","Estimator 649/1000, Train metric: 0.1297\n","Estimator 650/1000, Train metric: 0.1297\n","Estimator 651/1000, Train metric: 0.1296\n","Estimator 652/1000, Train metric: 0.1296\n","Estimator 653/1000, Train metric: 0.1295\n","Estimator 654/1000, Train metric: 0.1294\n","Estimator 655/1000, Train metric: 0.1294\n","Estimator 656/1000, Train metric: 0.1293\n","Estimator 657/1000, Train metric: 0.1293\n","Estimator 658/1000, Train metric: 0.1293\n","Estimator 659/1000, Train metric: 0.1293\n","Estimator 660/1000, Train metric: 0.1292\n","Estimator 661/1000, Train metric: 0.1291\n","Estimator 662/1000, Train metric: 0.1291\n","Estimator 663/1000, Train metric: 0.1290\n","Estimator 664/1000, Train metric: 0.1290\n","Estimator 665/1000, Train metric: 0.1289\n","Estimator 666/1000, Train metric: 0.1288\n","Estimator 667/1000, Train metric: 0.1288\n","Estimator 668/1000, Train metric: 0.1287\n","Estimator 669/1000, Train metric: 0.1286\n","Estimator 670/1000, Train metric: 0.1286\n","Estimator 671/1000, Train metric: 0.1285\n","Estimator 672/1000, Train metric: 0.1284\n","Estimator 673/1000, Train metric: 0.1284\n","Estimator 674/1000, Train metric: 0.1283\n","Estimator 675/1000, Train metric: 0.1282\n","Estimator 676/1000, Train metric: 0.1282\n","Estimator 677/1000, Train metric: 0.1282\n","Estimator 678/1000, Train metric: 0.1281\n","Estimator 679/1000, Train metric: 0.1280\n","Estimator 680/1000, Train metric: 0.1280\n","Estimator 681/1000, Train metric: 0.1280\n","Estimator 682/1000, Train metric: 0.1279\n","Estimator 683/1000, Train metric: 0.1279\n","Estimator 684/1000, Train metric: 0.1279\n","Estimator 685/1000, Train metric: 0.1278\n","Estimator 686/1000, Train metric: 0.1278\n","Estimator 687/1000, Train metric: 0.1278\n","Estimator 688/1000, Train metric: 0.1277\n","Estimator 689/1000, Train metric: 0.1277\n","Estimator 690/1000, Train metric: 0.1276\n","Estimator 691/1000, Train metric: 0.1275\n","Estimator 692/1000, Train metric: 0.1274\n","Estimator 693/1000, Train metric: 0.1274\n","Estimator 694/1000, Train metric: 0.1273\n","Estimator 695/1000, Train metric: 0.1273\n","Estimator 696/1000, Train metric: 0.1272\n","Estimator 697/1000, Train metric: 0.1272\n","Estimator 698/1000, Train metric: 0.1271\n","Estimator 699/1000, Train metric: 0.1271\n","Estimator 700/1000, Train metric: 0.1270\n","Estimator 701/1000, Train metric: 0.1270\n","Estimator 702/1000, Train metric: 0.1269\n","Estimator 703/1000, Train metric: 0.1269\n","Estimator 704/1000, Train metric: 0.1268\n","Estimator 705/1000, Train metric: 0.1268\n","Estimator 706/1000, Train metric: 0.1267\n","Estimator 707/1000, Train metric: 0.1266\n","Estimator 708/1000, Train metric: 0.1266\n","Estimator 709/1000, Train metric: 0.1265\n","Estimator 710/1000, Train metric: 0.1264\n","Estimator 711/1000, Train metric: 0.1264\n","Estimator 712/1000, Train metric: 0.1264\n","Estimator 713/1000, Train metric: 0.1263\n","Estimator 714/1000, Train metric: 0.1263\n","Estimator 715/1000, Train metric: 0.1262\n","Estimator 716/1000, Train metric: 0.1262\n","Estimator 717/1000, Train metric: 0.1261\n","Estimator 718/1000, Train metric: 0.1261\n","Estimator 719/1000, Train metric: 0.1261\n","Estimator 720/1000, Train metric: 0.1260\n","Estimator 721/1000, Train metric: 0.1259\n","Estimator 722/1000, Train metric: 0.1259\n","Estimator 723/1000, Train metric: 0.1258\n","Estimator 724/1000, Train metric: 0.1258\n","Estimator 725/1000, Train metric: 0.1258\n","Estimator 726/1000, Train metric: 0.1258\n","Estimator 727/1000, Train metric: 0.1257\n","Estimator 728/1000, Train metric: 0.1257\n","Estimator 729/1000, Train metric: 0.1256\n","Estimator 730/1000, Train metric: 0.1256\n","Estimator 731/1000, Train metric: 0.1255\n","Estimator 732/1000, Train metric: 0.1255\n","Estimator 733/1000, Train metric: 0.1254\n","Estimator 734/1000, Train metric: 0.1254\n","Estimator 735/1000, Train metric: 0.1254\n","Estimator 736/1000, Train metric: 0.1253\n","Estimator 737/1000, Train metric: 0.1252\n","Estimator 738/1000, Train metric: 0.1252\n","Estimator 739/1000, Train metric: 0.1251\n","Estimator 740/1000, Train metric: 0.1250\n","Estimator 741/1000, Train metric: 0.1248\n","Estimator 742/1000, Train metric: 0.1247\n","Estimator 743/1000, Train metric: 0.1246\n","Estimator 744/1000, Train metric: 0.1246\n","Estimator 745/1000, Train metric: 0.1246\n","Estimator 746/1000, Train metric: 0.1245\n","Estimator 747/1000, Train metric: 0.1245\n","Estimator 748/1000, Train metric: 0.1244\n","Estimator 749/1000, Train metric: 0.1244\n","Estimator 750/1000, Train metric: 0.1244\n","Estimator 751/1000, Train metric: 0.1244\n","Estimator 752/1000, Train metric: 0.1243\n","Estimator 753/1000, Train metric: 0.1243\n","Estimator 754/1000, Train metric: 0.1242\n","Estimator 755/1000, Train metric: 0.1242\n","Estimator 756/1000, Train metric: 0.1242\n","Estimator 757/1000, Train metric: 0.1242\n","Estimator 758/1000, Train metric: 0.1241\n","Estimator 759/1000, Train metric: 0.1241\n","Estimator 760/1000, Train metric: 0.1241\n","Estimator 761/1000, Train metric: 0.1240\n","Estimator 762/1000, Train metric: 0.1240\n","Estimator 763/1000, Train metric: 0.1240\n","Estimator 764/1000, Train metric: 0.1239\n","Estimator 765/1000, Train metric: 0.1239\n","Estimator 766/1000, Train metric: 0.1238\n","Estimator 767/1000, Train metric: 0.1238\n","Estimator 768/1000, Train metric: 0.1238\n","Estimator 769/1000, Train metric: 0.1238\n","Estimator 770/1000, Train metric: 0.1238\n","Estimator 771/1000, Train metric: 0.1237\n","Estimator 772/1000, Train metric: 0.1237\n","Estimator 773/1000, Train metric: 0.1236\n","Estimator 774/1000, Train metric: 0.1236\n","Estimator 775/1000, Train metric: 0.1236\n","Estimator 776/1000, Train metric: 0.1235\n","Estimator 777/1000, Train metric: 0.1235\n","Estimator 778/1000, Train metric: 0.1235\n","Estimator 779/1000, Train metric: 0.1235\n","Estimator 780/1000, Train metric: 0.1234\n","Estimator 781/1000, Train metric: 0.1234\n","Estimator 782/1000, Train metric: 0.1233\n","Estimator 783/1000, Train metric: 0.1233\n","Estimator 784/1000, Train metric: 0.1233\n","Estimator 785/1000, Train metric: 0.1233\n","Estimator 786/1000, Train metric: 0.1232\n","Estimator 787/1000, Train metric: 0.1232\n","Estimator 788/1000, Train metric: 0.1232\n","Estimator 789/1000, Train metric: 0.1231\n","Estimator 790/1000, Train metric: 0.1231\n","Estimator 791/1000, Train metric: 0.1230\n","Estimator 792/1000, Train metric: 0.1230\n","Estimator 793/1000, Train metric: 0.1230\n","Estimator 794/1000, Train metric: 0.1229\n","Estimator 795/1000, Train metric: 0.1229\n","Estimator 796/1000, Train metric: 0.1228\n","Estimator 797/1000, Train metric: 0.1228\n","Estimator 798/1000, Train metric: 0.1228\n","Estimator 799/1000, Train metric: 0.1227\n","Estimator 800/1000, Train metric: 0.1227\n","Estimator 801/1000, Train metric: 0.1226\n","Estimator 802/1000, Train metric: 0.1226\n","Estimator 803/1000, Train metric: 0.1226\n","Estimator 804/1000, Train metric: 0.1225\n","Estimator 805/1000, Train metric: 0.1225\n","Estimator 806/1000, Train metric: 0.1224\n","Estimator 807/1000, Train metric: 0.1224\n","Estimator 808/1000, Train metric: 0.1224\n","Estimator 809/1000, Train metric: 0.1223\n","Estimator 810/1000, Train metric: 0.1223\n","Estimator 811/1000, Train metric: 0.1222\n","Estimator 812/1000, Train metric: 0.1221\n","Estimator 813/1000, Train metric: 0.1221\n","Estimator 814/1000, Train metric: 0.1221\n","Estimator 815/1000, Train metric: 0.1221\n","Estimator 816/1000, Train metric: 0.1220\n","Estimator 817/1000, Train metric: 0.1220\n","Estimator 818/1000, Train metric: 0.1219\n","Estimator 819/1000, Train metric: 0.1219\n","Estimator 820/1000, Train metric: 0.1218\n","Estimator 821/1000, Train metric: 0.1218\n","Estimator 822/1000, Train metric: 0.1218\n","Estimator 823/1000, Train metric: 0.1217\n","Estimator 824/1000, Train metric: 0.1217\n","Estimator 825/1000, Train metric: 0.1217\n","Estimator 826/1000, Train metric: 0.1217\n","Estimator 827/1000, Train metric: 0.1216\n","Estimator 828/1000, Train metric: 0.1216\n","Estimator 829/1000, Train metric: 0.1216\n","Estimator 830/1000, Train metric: 0.1216\n","Estimator 831/1000, Train metric: 0.1216\n","Estimator 832/1000, Train metric: 0.1215\n","Estimator 833/1000, Train metric: 0.1215\n","Estimator 834/1000, Train metric: 0.1214\n","Estimator 835/1000, Train metric: 0.1214\n","Estimator 836/1000, Train metric: 0.1214\n","Estimator 837/1000, Train metric: 0.1213\n","Estimator 838/1000, Train metric: 0.1213\n","Estimator 839/1000, Train metric: 0.1212\n","Estimator 840/1000, Train metric: 0.1212\n","Estimator 841/1000, Train metric: 0.1211\n","Estimator 842/1000, Train metric: 0.1211\n","Estimator 843/1000, Train metric: 0.1211\n","Estimator 844/1000, Train metric: 0.1210\n","Estimator 845/1000, Train metric: 0.1210\n","Estimator 846/1000, Train metric: 0.1209\n","Estimator 847/1000, Train metric: 0.1209\n","Estimator 848/1000, Train metric: 0.1209\n","Estimator 849/1000, Train metric: 0.1208\n","Estimator 850/1000, Train metric: 0.1208\n","Estimator 851/1000, Train metric: 0.1208\n","Estimator 852/1000, Train metric: 0.1207\n","Estimator 853/1000, Train metric: 0.1207\n","Estimator 854/1000, Train metric: 0.1207\n","Estimator 855/1000, Train metric: 0.1206\n","Estimator 856/1000, Train metric: 0.1206\n","Estimator 857/1000, Train metric: 0.1205\n","Estimator 858/1000, Train metric: 0.1205\n","Estimator 859/1000, Train metric: 0.1205\n","Estimator 860/1000, Train metric: 0.1204\n","Estimator 861/1000, Train metric: 0.1204\n","Estimator 862/1000, Train metric: 0.1204\n","Estimator 863/1000, Train metric: 0.1204\n","Estimator 864/1000, Train metric: 0.1203\n","Estimator 865/1000, Train metric: 0.1203\n","Estimator 866/1000, Train metric: 0.1203\n","Estimator 867/1000, Train metric: 0.1203\n","Estimator 868/1000, Train metric: 0.1202\n","Estimator 869/1000, Train metric: 0.1202\n","Estimator 870/1000, Train metric: 0.1202\n","Estimator 871/1000, Train metric: 0.1201\n","Estimator 872/1000, Train metric: 0.1201\n","Estimator 873/1000, Train metric: 0.1201\n","Estimator 874/1000, Train metric: 0.1201\n","Estimator 875/1000, Train metric: 0.1200\n","Estimator 876/1000, Train metric: 0.1200\n","Estimator 877/1000, Train metric: 0.1200\n","Estimator 878/1000, Train metric: 0.1200\n","Estimator 879/1000, Train metric: 0.1199\n","Estimator 880/1000, Train metric: 0.1199\n","Estimator 881/1000, Train metric: 0.1199\n","Estimator 882/1000, Train metric: 0.1198\n","Estimator 883/1000, Train metric: 0.1198\n","Estimator 884/1000, Train metric: 0.1198\n","Estimator 885/1000, Train metric: 0.1198\n","Estimator 886/1000, Train metric: 0.1197\n","Estimator 887/1000, Train metric: 0.1197\n","Estimator 888/1000, Train metric: 0.1197\n","Estimator 889/1000, Train metric: 0.1196\n","Estimator 890/1000, Train metric: 0.1196\n","Estimator 891/1000, Train metric: 0.1196\n","Estimator 892/1000, Train metric: 0.1195\n","Estimator 893/1000, Train metric: 0.1195\n","Estimator 894/1000, Train metric: 0.1194\n","Estimator 895/1000, Train metric: 0.1194\n","Estimator 896/1000, Train metric: 0.1193\n","Estimator 897/1000, Train metric: 0.1193\n","Estimator 898/1000, Train metric: 0.1193\n","Estimator 899/1000, Train metric: 0.1193\n","Estimator 900/1000, Train metric: 0.1192\n","Estimator 901/1000, Train metric: 0.1192\n","Estimator 902/1000, Train metric: 0.1192\n","Estimator 903/1000, Train metric: 0.1191\n","Estimator 904/1000, Train metric: 0.1191\n","Estimator 905/1000, Train metric: 0.1191\n","Estimator 906/1000, Train metric: 0.1191\n","Estimator 907/1000, Train metric: 0.1191\n","Estimator 908/1000, Train metric: 0.1190\n","Estimator 909/1000, Train metric: 0.1190\n","Estimator 910/1000, Train metric: 0.1190\n","Estimator 911/1000, Train metric: 0.1190\n","Estimator 912/1000, Train metric: 0.1190\n","Estimator 913/1000, Train metric: 0.1190\n","Estimator 914/1000, Train metric: 0.1189\n","Estimator 915/1000, Train metric: 0.1189\n","Estimator 916/1000, Train metric: 0.1188\n","Estimator 917/1000, Train metric: 0.1188\n","Estimator 918/1000, Train metric: 0.1188\n","Estimator 919/1000, Train metric: 0.1187\n","Estimator 920/1000, Train metric: 0.1187\n","Estimator 921/1000, Train metric: 0.1187\n","Estimator 922/1000, Train metric: 0.1187\n","Estimator 923/1000, Train metric: 0.1186\n","Estimator 924/1000, Train metric: 0.1186\n","Estimator 925/1000, Train metric: 0.1186\n","Estimator 926/1000, Train metric: 0.1186\n","Estimator 927/1000, Train metric: 0.1185\n","Estimator 928/1000, Train metric: 0.1185\n","Estimator 929/1000, Train metric: 0.1185\n","Estimator 930/1000, Train metric: 0.1184\n","Estimator 931/1000, Train metric: 0.1184\n","Estimator 932/1000, Train metric: 0.1184\n","Estimator 933/1000, Train metric: 0.1183\n","Estimator 934/1000, Train metric: 0.1183\n","Estimator 935/1000, Train metric: 0.1182\n","Estimator 936/1000, Train metric: 0.1182\n","Estimator 937/1000, Train metric: 0.1182\n","Estimator 938/1000, Train metric: 0.1182\n","Estimator 939/1000, Train metric: 0.1182\n","Estimator 940/1000, Train metric: 0.1181\n","Estimator 941/1000, Train metric: 0.1181\n","Estimator 942/1000, Train metric: 0.1181\n","Estimator 943/1000, Train metric: 0.1180\n","Estimator 944/1000, Train metric: 0.1180\n","Estimator 945/1000, Train metric: 0.1180\n","Estimator 946/1000, Train metric: 0.1180\n","Estimator 947/1000, Train metric: 0.1179\n","Estimator 948/1000, Train metric: 0.1179\n","Estimator 949/1000, Train metric: 0.1179\n","Estimator 950/1000, Train metric: 0.1179\n","Estimator 951/1000, Train metric: 0.1179\n","Estimator 952/1000, Train metric: 0.1178\n","Estimator 953/1000, Train metric: 0.1178\n","Estimator 954/1000, Train metric: 0.1178\n","Estimator 955/1000, Train metric: 0.1177\n","Estimator 956/1000, Train metric: 0.1177\n","Estimator 957/1000, Train metric: 0.1177\n","Estimator 958/1000, Train metric: 0.1177\n","Estimator 959/1000, Train metric: 0.1176\n","Estimator 960/1000, Train metric: 0.1175\n","Estimator 961/1000, Train metric: 0.1175\n","Estimator 962/1000, Train metric: 0.1174\n","Estimator 963/1000, Train metric: 0.1173\n","Estimator 964/1000, Train metric: 0.1173\n","Estimator 965/1000, Train metric: 0.1173\n","Estimator 966/1000, Train metric: 0.1172\n","Estimator 967/1000, Train metric: 0.1172\n","Estimator 968/1000, Train metric: 0.1172\n","Estimator 969/1000, Train metric: 0.1172\n","Estimator 970/1000, Train metric: 0.1171\n","Estimator 971/1000, Train metric: 0.1171\n","Estimator 972/1000, Train metric: 0.1171\n","Estimator 973/1000, Train metric: 0.1170\n","Estimator 974/1000, Train metric: 0.1170\n","Estimator 975/1000, Train metric: 0.1170\n","Estimator 976/1000, Train metric: 0.1170\n","Estimator 977/1000, Train metric: 0.1170\n","Estimator 978/1000, Train metric: 0.1169\n","Estimator 979/1000, Train metric: 0.1169\n","Estimator 980/1000, Train metric: 0.1168\n","Estimator 981/1000, Train metric: 0.1168\n","Estimator 982/1000, Train metric: 0.1168\n","Estimator 983/1000, Train metric: 0.1168\n","Estimator 984/1000, Train metric: 0.1168\n","Estimator 985/1000, Train metric: 0.1167\n","Estimator 986/1000, Train metric: 0.1167\n","Estimator 987/1000, Train metric: 0.1166\n","Estimator 988/1000, Train metric: 0.1166\n","Estimator 989/1000, Train metric: 0.1166\n","Estimator 990/1000, Train metric: 0.1166\n","Estimator 991/1000, Train metric: 0.1165\n","Estimator 992/1000, Train metric: 0.1165\n","Estimator 993/1000, Train metric: 0.1165\n","Estimator 994/1000, Train metric: 0.1165\n","Estimator 995/1000, Train metric: 0.1164\n","Estimator 996/1000, Train metric: 0.1164\n","Estimator 997/1000, Train metric: 0.1164\n","Estimator 998/1000, Train metric: 0.1164\n","Estimator 999/1000, Train metric: 0.1164\n","Elapsed time for fitting PGBM model: 99.92 s\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/pgbm/pgbm.py:420: UserWarning: FALLBACK path has been taken inside: compileCudaFusionGroup. This is an indication that codegen Failed for some reason.\n","To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\n","To report the issue, try enable logging via setting the envvariable ` export PYTORCH_JIT_LOG_LEVEL=manager.cpp`\n"," (Triggered internally at ../torch/csrc/jit/codegen/cuda/manager.cpp:239.)\n","  mu, variance = _predict_forest_muvar(X_test_splits, self.nodes_idx,\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/pgbm/pgbm.py:420: UserWarning: FALLBACK path has been taken inside: runCudaFusionGroup. This is an indication that codegen Failed for some reason.\n","To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\n"," (Triggered internally at ../torch/csrc/jit/codegen/cuda/manager.cpp:331.)\n","  mu, variance = _predict_forest_muvar(X_test_splits, self.nodes_idx,\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/lidl_x_tum_uncertainty_estimation/uncertainty_estimation_models.py:552: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  nll_temp = torch.tensor([-dist[i].log_prob(torch.tensor(y_test[i])) for i in range(len(dist))])\n","path: predictions/pointpredictions_pgbm_normal.csv\n","dirname: predictions\n","filename: pointpredictions_pgbm_normal.csv\n","artifact_path: predictions\n","path: /tmp/tmp6qklfqzv/pointpredictions_pgbm_normal.csv\n","tmp_path: /tmp/tmp6qklfqzv/pointpredictions_pgbm_normal.csv\n","path: predictions/samples_pgbm_normal.csv\n","dirname: predictions\n","filename: samples_pgbm_normal.csv\n","artifact_path: predictions\n","path: /tmp/tmpd2g6_75s/samples_pgbm_normal.csv\n","tmp_path: /tmp/tmpd2g6_75s/samples_pgbm_normal.csv\n","path: predictions/distparams_pgbm_normal.csv\n","dirname: predictions\n","filename: distparams_pgbm_normal.csv\n","artifact_path: predictions\n","path: /tmp/tmp2imonpez/distparams_pgbm_normal.csv\n","tmp_path: /tmp/tmp2imonpez/distparams_pgbm_normal.csv\n","path: predictions/quantiles_pgbm_normal0.05.csv\n","dirname: predictions\n","filename: quantiles_pgbm_normal0.05.csv\n","artifact_path: predictions\n","path: /tmp/tmpnrtuwcv9/quantiles_pgbm_normal0.05.csv\n","tmp_path: /tmp/tmpnrtuwcv9/quantiles_pgbm_normal0.05.csv\n","path: predictions/quantiles_pgbm_normal0.1.csv\n","dirname: predictions\n","filename: quantiles_pgbm_normal0.1.csv\n","artifact_path: predictions\n","path: /tmp/tmpikpa88i1/quantiles_pgbm_normal0.1.csv\n","tmp_path: /tmp/tmpikpa88i1/quantiles_pgbm_normal0.1.csv\n","path: predictions/quantiles_pgbm_normal0.5.csv\n","dirname: predictions\n","filename: quantiles_pgbm_normal0.5.csv\n","artifact_path: predictions\n","path: /tmp/tmpaqzq28wj/quantiles_pgbm_normal0.5.csv\n","tmp_path: /tmp/tmpaqzq28wj/quantiles_pgbm_normal0.5.csv\n","path: predictions/quantiles_pgbm_normal0.9.csv\n","dirname: predictions\n","filename: quantiles_pgbm_normal0.9.csv\n","artifact_path: predictions\n","path: /tmp/tmp3v0e6k78/quantiles_pgbm_normal0.9.csv\n","tmp_path: /tmp/tmp3v0e6k78/quantiles_pgbm_normal0.9.csv\n","path: predictions/quantiles_pgbm_normal0.95.csv\n","dirname: predictions\n","filename: quantiles_pgbm_normal0.95.csv\n","artifact_path: predictions\n","path: /tmp/tmpk6aq1ndg/quantiles_pgbm_normal0.95.csv\n","tmp_path: /tmp/tmpk6aq1ndg/quantiles_pgbm_normal0.95.csv\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"6be1ae84f6cf442db89f8b3ab5eb5287\nTraining on GPU\nEstimator 0/1000, Train metric: 0.4159, Validation metric: 0.4036\nEstimator 1/1000, Train metric: 0.4064, Validation metric: 0.3944\nEstimator 2/1000, Train metric: 0.3979, Validation metric: 0.3861\nEstimator 3/1000, Train metric: 0.3906, Validation metric: 0.3795\nEstimator 4/1000, Train metric: 0.3868, Validation metric: 0.3764\nEstimator 5/1000, Train metric: 0.3807, Validation metric: 0.3705\nEstimator 6/1000, Train metric: 0.3765, Validation metric: 0.3670\nEstimator 7/1000, Train metric: 0.3710, Validation metric: 0.3618\nEstimator 8/1000, Train metric: 0.3656, Validation metric: 0.3570\nEstimator 9/1000, Train metric: 0.3606, Validation metric: 0.3524\nEstimator 10/1000, Train metric: 0.3566, Validation metric: 0.3489\nEstimator 11/1000, Train metric: 0.3530, Validation metric: 0.3461\nEstimator 12/1000, Train metric: 0.3502, Validation metric: 0.3444\nEstimator 13/1000, Train metric: 0.3486, Validation metric: 0.3436\nEstimator 14/1000, Train metric: 0.3454, Validation metric: 0.3403\nEstimator 15/1000, Train metric: 0.3428, Validation metric: 0.3380\nEstimator 16/1000, Train metric: 0.3408, Validation metric: 0.3365\nEstimator 17/1000, Train metric: 0.3380, Validation metric: 0.3340\nEstimator 18/1000, Train metric: 0.3367, Validation metric: 0.3330\nEstimator 19/1000, Train metric: 0.3355, Validation metric: 0.3322\nEstimator 20/1000, Train metric: 0.3332, Validation metric: 0.3302\nEstimator 21/1000, Train metric: 0.3314, Validation metric: 0.3285\nEstimator 22/1000, Train metric: 0.3300, Validation metric: 0.3272\nEstimator 23/1000, Train metric: 0.3284, Validation metric: 0.3263\nEstimator 24/1000, Train metric: 0.3267, Validation metric: 0.3251\nEstimator 25/1000, Train metric: 0.3253, Validation metric: 0.3237\nEstimator 26/1000, Train metric: 0.3243, Validation metric: 0.3225\nEstimator 27/1000, Train metric: 0.3234, Validation metric: 0.3214\nEstimator 28/1000, Train metric: 0.3220, Validation metric: 0.3202\nEstimator 29/1000, Train metric: 0.3212, Validation metric: 0.3192\nEstimator 30/1000, Train metric: 0.3194, Validation metric: 0.3176\nEstimator 31/1000, Train metric: 0.3180, Validation metric: 0.3167\nEstimator 32/1000, Train metric: 0.3174, Validation metric: 0.3161\nEstimator 33/1000, Train metric: 0.3157, Validation metric: 0.3143\nEstimator 34/1000, Train metric: 0.3143, Validation metric: 0.3129\nEstimator 35/1000, Train metric: 0.3133, Validation metric: 0.3120\nEstimator 36/1000, Train metric: 0.3109, Validation metric: 0.3095\nEstimator 37/1000, Train metric: 0.3105, Validation metric: 0.3090\nEstimator 38/1000, Train metric: 0.3084, Validation metric: 0.3070\nEstimator 39/1000, Train metric: 0.3071, Validation metric: 0.3057\nEstimator 40/1000, Train metric: 0.3060, Validation metric: 0.3053\nEstimator 41/1000, Train metric: 0.3055, Validation metric: 0.3049\nEstimator 42/1000, Train metric: 0.3043, Validation metric: 0.3038\nEstimator 43/1000, Train metric: 0.3026, Validation metric: 0.3022\nEstimator 44/1000, Train metric: 0.3023, Validation metric: 0.3019\nEstimator 45/1000, Train metric: 0.3004, Validation metric: 0.3000\nEstimator 46/1000, Train metric: 0.2992, Validation metric: 0.2990\nEstimator 47/1000, Train metric: 0.2988, Validation metric: 0.2981\nEstimator 48/1000, Train metric: 0.2983, Validation metric: 0.2975\nEstimator 49/1000, Train metric: 0.2976, Validation metric: 0.2968\nEstimator 50/1000, Train metric: 0.2973, Validation metric: 0.2966\nEstimator 51/1000, Train metric: 0.2964, Validation metric: 0.2956\nEstimator 52/1000, Train metric: 0.2950, Validation metric: 0.2945\nEstimator 53/1000, Train metric: 0.2920, Validation metric: 0.2916\nEstimator 54/1000, Train metric: 0.2910, Validation metric: 0.2905\nEstimator 55/1000, Train metric: 0.2900, Validation metric: 0.2896\nEstimator 56/1000, Train metric: 0.2897, Validation metric: 0.2894\nEstimator 57/1000, Train metric: 0.2874, Validation metric: 0.2872\nEstimator 58/1000, Train metric: 0.2868, Validation metric: 0.2867\nEstimator 59/1000, Train metric: 0.2854, Validation metric: 0.2855\nEstimator 60/1000, Train metric: 0.2850, Validation metric: 0.2848\nEstimator 61/1000, Train metric: 0.2839, Validation metric: 0.2837\nEstimator 62/1000, Train metric: 0.2825, Validation metric: 0.2825\nEstimator 63/1000, Train metric: 0.2822, Validation metric: 0.2823\nEstimator 64/1000, Train metric: 0.2818, Validation metric: 0.2819\nEstimator 65/1000, Train metric: 0.2808, Validation metric: 0.2808\nEstimator 66/1000, Train metric: 0.2801, Validation metric: 0.2801\nEstimator 67/1000, Train metric: 0.2795, Validation metric: 0.2796\nEstimator 68/1000, Train metric: 0.2792, Validation metric: 0.2792\nEstimator 69/1000, Train metric: 0.2775, Validation metric: 0.2778\nEstimator 70/1000, Train metric: 0.2772, Validation metric: 0.2774\nEstimator 71/1000, Train metric: 0.2761, Validation metric: 0.2764\nEstimator 72/1000, Train metric: 0.2744, Validation metric: 0.2748\nEstimator 73/1000, Train metric: 0.2742, Validation metric: 0.2751\nEstimator 74/1000, Train metric: 0.2733, Validation metric: 0.2744\nEstimator 75/1000, Train metric: 0.2731, Validation metric: 0.2742\nEstimator 76/1000, Train metric: 0.2725, Validation metric: 0.2737\nEstimator 77/1000, Train metric: 0.2712, Validation metric: 0.2725\nEstimator 78/1000, Train metric: 0.2711, Validation metric: 0.2723\nEstimator 79/1000, Train metric: 0.2708, Validation metric: 0.2720\nEstimator 80/1000, Train metric: 0.2703, Validation metric: 0.2714\nEstimator 81/1000, Train metric: 0.2675, Validation metric: 0.2687\nEstimator 82/1000, Train metric: 0.2663, Validation metric: 0.2675\nEstimator 83/1000, Train metric: 0.2650, Validation metric: 0.2663\nEstimator 84/1000, Train metric: 0.2628, Validation metric: 0.2641\nEstimator 85/1000, Train metric: 0.2624, Validation metric: 0.2637\nEstimator 86/1000, Train metric: 0.2607, Validation metric: 0.2620\nEstimator 87/1000, Train metric: 0.2596, Validation metric: 0.2609\nEstimator 88/1000, Train metric: 0.2593, Validation metric: 0.2604\nEstimator 89/1000, Train metric: 0.2584, Validation metric: 0.2595\nEstimator 90/1000, Train metric: 0.2570, Validation metric: 0.2582\nEstimator 91/1000, Train metric: 0.2567, Validation metric: 0.2574\nEstimator 92/1000, Train metric: 0.2553, Validation metric: 0.2562\nEstimator 93/1000, Train metric: 0.2541, Validation metric: 0.2550\nEstimator 94/1000, Train metric: 0.2539, Validation metric: 0.2548\nEstimator 95/1000, Train metric: 0.2535, Validation metric: 0.2545\nEstimator 96/1000, Train metric: 0.2530, Validation metric: 0.2540\nEstimator 97/1000, Train metric: 0.2526, Validation metric: 0.2536\nEstimator 98/1000, Train metric: 0.2523, Validation metric: 0.2533\nEstimator 99/1000, Train metric: 0.2500, Validation metric: 0.2512\nEstimator 100/1000, Train metric: 0.2484, Validation metric: 0.2496\nEstimator 101/1000, Train metric: 0.2476, Validation metric: 0.2489\nEstimator 102/1000, Train metric: 0.2464, Validation metric: 0.2477\nEstimator 103/1000, Train metric: 0.2455, Validation metric: 0.2469\nEstimator 104/1000, Train metric: 0.2448, Validation metric: 0.2462\nEstimator 105/1000, Train metric: 0.2446, Validation metric: 0.2463\nEstimator 106/1000, Train metric: 0.2444, Validation metric: 0.2461\nEstimator 107/1000, Train metric: 0.2434, Validation metric: 0.2453\nEstimator 108/1000, Train metric: 0.2432, Validation metric: 0.2451\nEstimator 109/1000, Train metric: 0.2428, Validation metric: 0.2446\nEstimator 110/1000, Train metric: 0.2426, Validation metric: 0.2445\nEstimator 111/1000, Train metric: 0.2424, Validation metric: 0.2442\nEstimator 112/1000, Train metric: 0.2419, Validation metric: 0.2438\nEstimator 113/1000, Train metric: 0.2409, Validation metric: 0.2428\nEstimator 114/1000, Train metric: 0.2406, Validation metric: 0.2425\nEstimator 115/1000, Train metric: 0.2403, Validation metric: 0.2422\nEstimator 116/1000, Train metric: 0.2401, Validation metric: 0.2420\nEstimator 117/1000, Train metric: 0.2385, Validation metric: 0.2406\nEstimator 118/1000, Train metric: 0.2380, Validation metric: 0.2400\nEstimator 119/1000, Train metric: 0.2378, Validation metric: 0.2399\nEstimator 120/1000, Train metric: 0.2373, Validation metric: 0.2393\nEstimator 121/1000, Train metric: 0.2360, Validation metric: 0.2381\nEstimator 122/1000, Train metric: 0.2352, Validation metric: 0.2374\nEstimator 123/1000, Train metric: 0.2350, Validation metric: 0.2374\nEstimator 124/1000, Train metric: 0.2346, Validation metric: 0.2370\nEstimator 125/1000, Train metric: 0.2341, Validation metric: 0.2365\nEstimator 126/1000, Train metric: 0.2339, Validation metric: 0.2363\nEstimator 127/1000, Train metric: 0.2334, Validation metric: 0.2358\nEstimator 128/1000, Train metric: 0.2328, Validation metric: 0.2352\nEstimator 129/1000, Train metric: 0.2315, Validation metric: 0.2341\nEstimator 130/1000, Train metric: 0.2306, Validation metric: 0.2333\nEstimator 131/1000, Train metric: 0.2305, Validation metric: 0.2331\nEstimator 132/1000, Train metric: 0.2301, Validation metric: 0.2329\nEstimator 133/1000, Train metric: 0.2299, Validation metric: 0.2329\nEstimator 134/1000, Train metric: 0.2298, Validation metric: 0.2327\nEstimator 135/1000, Train metric: 0.2291, Validation metric: 0.2320\nEstimator 136/1000, Train metric: 0.2289, Validation metric: 0.2319\nEstimator 137/1000, Train metric: 0.2287, Validation metric: 0.2318\nEstimator 138/1000, Train metric: 0.2285, Validation metric: 0.2316\nEstimator 139/1000, Train metric: 0.2281, Validation metric: 0.2312\nEstimator 140/1000, Train metric: 0.2276, Validation metric: 0.2306\nEstimator 141/1000, Train metric: 0.2274, Validation metric: 0.2304\nEstimator 142/1000, Train metric: 0.2272, Validation metric: 0.2303\nEstimator 143/1000, Train metric: 0.2260, Validation metric: 0.2291\nEstimator 144/1000, Train metric: 0.2253, Validation metric: 0.2285\nEstimator 145/1000, Train metric: 0.2243, Validation metric: 0.2275\nEstimator 146/1000, Train metric: 0.2238, Validation metric: 0.2272\nEstimator 147/1000, Train metric: 0.2237, Validation metric: 0.2270\nEstimator 148/1000, Train metric: 0.2235, Validation metric: 0.2267\nEstimator 149/1000, Train metric: 0.2233, Validation metric: 0.2267\nEstimator 150/1000, Train metric: 0.2228, Validation metric: 0.2262\nEstimator 151/1000, Train metric: 0.2213, Validation metric: 0.2249\nEstimator 152/1000, Train metric: 0.2211, Validation metric: 0.2248\nEstimator 153/1000, Train metric: 0.2209, Validation metric: 0.2246\nEstimator 154/1000, Train metric: 0.2200, Validation metric: 0.2237\nEstimator 155/1000, Train metric: 0.2191, Validation metric: 0.2229\nEstimator 156/1000, Train metric: 0.2190, Validation metric: 0.2227\nEstimator 157/1000, Train metric: 0.2186, Validation metric: 0.2224\nEstimator 158/1000, Train metric: 0.2183, Validation metric: 0.2221\nEstimator 159/1000, Train metric: 0.2175, Validation metric: 0.2214\nEstimator 160/1000, Train metric: 0.2169, Validation metric: 0.2209\nEstimator 161/1000, Train metric: 0.2164, Validation metric: 0.2204\nEstimator 162/1000, Train metric: 0.2153, Validation metric: 0.2193\nEstimator 163/1000, Train metric: 0.2148, Validation metric: 0.2188\nEstimator 164/1000, Train metric: 0.2145, Validation metric: 0.2185\nEstimator 165/1000, Train metric: 0.2143, Validation metric: 0.2183\nEstimator 166/1000, Train metric: 0.2142, Validation metric: 0.2183\nEstimator 167/1000, Train metric: 0.2133, Validation metric: 0.2175\nEstimator 168/1000, Train metric: 0.2125, Validation metric: 0.2166\nEstimator 169/1000, Train metric: 0.2120, Validation metric: 0.2161\nEstimator 170/1000, Train metric: 0.2119, Validation metric: 0.2160\nEstimator 171/1000, Train metric: 0.2112, Validation metric: 0.2153\nEstimator 172/1000, Train metric: 0.2107, Validation metric: 0.2147\nEstimator 173/1000, Train metric: 0.2102, Validation metric: 0.2142\nEstimator 174/1000, Train metric: 0.2100, Validation metric: 0.2141\nEstimator 175/1000, Train metric: 0.2096, Validation metric: 0.2137\nEstimator 176/1000, Train metric: 0.2093, Validation metric: 0.2134\nEstimator 177/1000, Train metric: 0.2092, Validation metric: 0.2133\nEstimator 178/1000, Train metric: 0.2085, Validation metric: 0.2127\nEstimator 179/1000, Train metric: 0.2078, Validation metric: 0.2121\nEstimator 180/1000, Train metric: 0.2077, Validation metric: 0.2120\nEstimator 181/1000, Train metric: 0.2075, Validation metric: 0.2119\nEstimator 182/1000, Train metric: 0.2069, Validation metric: 0.2114\nEstimator 183/1000, Train metric: 0.2067, Validation metric: 0.2112\nEstimator 184/1000, Train metric: 0.2062, Validation metric: 0.2107\nEstimator 185/1000, Train metric: 0.2051, Validation metric: 0.2096\nEstimator 186/1000, Train metric: 0.2050, Validation metric: 0.2095\nEstimator 187/1000, Train metric: 0.2049, Validation metric: 0.2094\nEstimator 188/1000, Train metric: 0.2042, Validation metric: 0.2087\nEstimator 189/1000, Train metric: 0.2031, Validation metric: 0.2078\nEstimator 190/1000, Train metric: 0.2030, Validation metric: 0.2077\nEstimator 191/1000, Train metric: 0.2023, Validation metric: 0.2070\nEstimator 192/1000, Train metric: 0.2020, Validation metric: 0.2068\nEstimator 193/1000, Train metric: 0.2014, Validation metric: 0.2061\nEstimator 194/1000, Train metric: 0.2012, Validation metric: 0.2055\nEstimator 195/1000, Train metric: 0.2007, Validation metric: 0.2051\nEstimator 196/1000, Train metric: 0.2002, Validation metric: 0.2046\nEstimator 197/1000, Train metric: 0.1995, Validation metric: 0.2040\nEstimator 198/1000, Train metric: 0.1988, Validation metric: 0.2033\nEstimator 199/1000, Train metric: 0.1988, Validation metric: 0.2033\nEstimator 200/1000, Train metric: 0.1985, Validation metric: 0.2031\nEstimator 201/1000, Train metric: 0.1985, Validation metric: 0.2031\nEstimator 202/1000, Train metric: 0.1980, Validation metric: 0.2026\nEstimator 203/1000, Train metric: 0.1971, Validation metric: 0.2018\nEstimator 204/1000, Train metric: 0.1965, Validation metric: 0.2012\nEstimator 205/1000, Train metric: 0.1961, Validation metric: 0.2009\nEstimator 206/1000, Train metric: 0.1960, Validation metric: 0.2008\nEstimator 207/1000, Train metric: 0.1957, Validation metric: 0.2005\nEstimator 208/1000, Train metric: 0.1956, Validation metric: 0.2005\nEstimator 209/1000, Train metric: 0.1950, Validation metric: 0.1999\nEstimator 210/1000, Train metric: 0.1947, Validation metric: 0.1997\nEstimator 211/1000, Train metric: 0.1939, Validation metric: 0.1988\nEstimator 212/1000, Train metric: 0.1936, Validation metric: 0.1986\nEstimator 213/1000, Train metric: 0.1930, Validation metric: 0.1980\nEstimator 214/1000, Train metric: 0.1927, Validation metric: 0.1978\nEstimator 215/1000, Train metric: 0.1926, Validation metric: 0.1978\nEstimator 216/1000, Train metric: 0.1926, Validation metric: 0.1978\nEstimator 217/1000, Train metric: 0.1924, Validation metric: 0.1976\nEstimator 218/1000, Train metric: 0.1922, Validation metric: 0.1975\nEstimator 219/1000, Train metric: 0.1922, Validation metric: 0.1974\nEstimator 220/1000, Train metric: 0.1918, Validation metric: 0.1971\nEstimator 221/1000, Train metric: 0.1914, Validation metric: 0.1967\nEstimator 222/1000, Train metric: 0.1913, Validation metric: 0.1966\nEstimator 223/1000, Train metric: 0.1903, Validation metric: 0.1957\nEstimator 224/1000, Train metric: 0.1902, Validation metric: 0.1955\nEstimator 225/1000, Train metric: 0.1895, Validation metric: 0.1949\nEstimator 226/1000, Train metric: 0.1890, Validation metric: 0.1945\nEstimator 227/1000, Train metric: 0.1887, Validation metric: 0.1942\nEstimator 228/1000, Train metric: 0.1885, Validation metric: 0.1940\nEstimator 229/1000, Train metric: 0.1884, Validation metric: 0.1939\nEstimator 230/1000, Train metric: 0.1877, Validation metric: 0.1933\nEstimator 231/1000, Train metric: 0.1872, Validation metric: 0.1928\nEstimator 232/1000, Train metric: 0.1870, Validation metric: 0.1926\nEstimator 233/1000, Train metric: 0.1863, Validation metric: 0.1919\nEstimator 234/1000, Train metric: 0.1858, Validation metric: 0.1914\nEstimator 235/1000, Train metric: 0.1852, Validation metric: 0.1909\nEstimator 236/1000, Train metric: 0.1851, Validation metric: 0.1909\nEstimator 237/1000, Train metric: 0.1849, Validation metric: 0.1906\nEstimator 238/1000, Train metric: 0.1844, Validation metric: 0.1902\nEstimator 239/1000, Train metric: 0.1843, Validation metric: 0.1901\nEstimator 240/1000, Train metric: 0.1836, Validation metric: 0.1894\nEstimator 241/1000, Train metric: 0.1835, Validation metric: 0.1895\nEstimator 242/1000, Train metric: 0.1831, Validation metric: 0.1892\nEstimator 243/1000, Train metric: 0.1828, Validation metric: 0.1889\nEstimator 244/1000, Train metric: 0.1827, Validation metric: 0.1888\nEstimator 245/1000, Train metric: 0.1826, Validation metric: 0.1888\nEstimator 246/1000, Train metric: 0.1820, Validation metric: 0.1882\nEstimator 247/1000, Train metric: 0.1819, Validation metric: 0.1882\nEstimator 248/1000, Train metric: 0.1816, Validation metric: 0.1879\nEstimator 249/1000, Train metric: 0.1813, Validation metric: 0.1877\nEstimator 250/1000, Train metric: 0.1808, Validation metric: 0.1872\nEstimator 251/1000, Train metric: 0.1803, Validation metric: 0.1867\nEstimator 252/1000, Train metric: 0.1799, Validation metric: 0.1863\nEstimator 253/1000, Train metric: 0.1794, Validation metric: 0.1859\nEstimator 254/1000, Train metric: 0.1790, Validation metric: 0.1855\nEstimator 255/1000, Train metric: 0.1786, Validation metric: 0.1851\nEstimator 256/1000, Train metric: 0.1783, Validation metric: 0.1849\nEstimator 257/1000, Train metric: 0.1782, Validation metric: 0.1848\nEstimator 258/1000, Train metric: 0.1782, Validation metric: 0.1847\nEstimator 259/1000, Train metric: 0.1778, Validation metric: 0.1844\nEstimator 260/1000, Train metric: 0.1777, Validation metric: 0.1843\nEstimator 261/1000, Train metric: 0.1774, Validation metric: 0.1840\nEstimator 262/1000, Train metric: 0.1774, Validation metric: 0.1840\nEstimator 263/1000, Train metric: 0.1769, Validation metric: 0.1835\nEstimator 264/1000, Train metric: 0.1766, Validation metric: 0.1833\nEstimator 265/1000, Train metric: 0.1762, Validation metric: 0.1829\nEstimator 266/1000, Train metric: 0.1760, Validation metric: 0.1827\nEstimator 267/1000, Train metric: 0.1759, Validation metric: 0.1827\nEstimator 268/1000, Train metric: 0.1758, Validation metric: 0.1827\nEstimator 269/1000, Train metric: 0.1757, Validation metric: 0.1826\nEstimator 270/1000, Train metric: 0.1756, Validation metric: 0.1825\nEstimator 271/1000, Train metric: 0.1752, Validation metric: 0.1822\nEstimator 272/1000, Train metric: 0.1749, Validation metric: 0.1819\nEstimator 273/1000, Train metric: 0.1744, Validation metric: 0.1814\nEstimator 274/1000, Train metric: 0.1742, Validation metric: 0.1812\nEstimator 275/1000, Train metric: 0.1736, Validation metric: 0.1807\nEstimator 276/1000, Train metric: 0.1734, Validation metric: 0.1805\nEstimator 277/1000, Train metric: 0.1732, Validation metric: 0.1803\nEstimator 278/1000, Train metric: 0.1730, Validation metric: 0.1801\nEstimator 279/1000, Train metric: 0.1725, Validation metric: 0.1797\nEstimator 280/1000, Train metric: 0.1724, Validation metric: 0.1797\nEstimator 281/1000, Train metric: 0.1723, Validation metric: 0.1797\nEstimator 282/1000, Train metric: 0.1721, Validation metric: 0.1795\nEstimator 283/1000, Train metric: 0.1717, Validation metric: 0.1791\nEstimator 284/1000, Train metric: 0.1715, Validation metric: 0.1790\nEstimator 285/1000, Train metric: 0.1714, Validation metric: 0.1787\nEstimator 286/1000, Train metric: 0.1710, Validation metric: 0.1783\nEstimator 287/1000, Train metric: 0.1707, Validation metric: 0.1780\nEstimator 288/1000, Train metric: 0.1703, Validation metric: 0.1777\nEstimator 289/1000, Train metric: 0.1701, Validation metric: 0.1775\nEstimator 290/1000, Train metric: 0.1698, Validation metric: 0.1773\nEstimator 291/1000, Train metric: 0.1694, Validation metric: 0.1769\nEstimator 292/1000, Train metric: 0.1691, Validation metric: 0.1767\nEstimator 293/1000, Train metric: 0.1688, Validation metric: 0.1763\nEstimator 294/1000, Train metric: 0.1686, Validation metric: 0.1761\nEstimator 295/1000, Train metric: 0.1685, Validation metric: 0.1761\nEstimator 296/1000, Train metric: 0.1683, Validation metric: 0.1759\nEstimator 297/1000, Train metric: 0.1680, Validation metric: 0.1757\nEstimator 298/1000, Train metric: 0.1679, Validation metric: 0.1757\nEstimator 299/1000, Train metric: 0.1678, Validation metric: 0.1757\nEstimator 300/1000, Train metric: 0.1675, Validation metric: 0.1754\nEstimator 301/1000, Train metric: 0.1673, Validation metric: 0.1753\nEstimator 302/1000, Train metric: 0.1673, Validation metric: 0.1752\nEstimator 303/1000, Train metric: 0.1671, Validation metric: 0.1750\nEstimator 304/1000, Train metric: 0.1668, Validation metric: 0.1747\nEstimator 305/1000, Train metric: 0.1665, Validation metric: 0.1744\nEstimator 306/1000, Train metric: 0.1662, Validation metric: 0.1742\nEstimator 307/1000, Train metric: 0.1656, Validation metric: 0.1737\nEstimator 308/1000, Train metric: 0.1653, Validation metric: 0.1734\nEstimator 309/1000, Train metric: 0.1651, Validation metric: 0.1732\nEstimator 310/1000, Train metric: 0.1646, Validation metric: 0.1728\nEstimator 311/1000, Train metric: 0.1644, Validation metric: 0.1726\nEstimator 312/1000, Train metric: 0.1640, Validation metric: 0.1722\nEstimator 313/1000, Train metric: 0.1640, Validation metric: 0.1722\nEstimator 314/1000, Train metric: 0.1638, Validation metric: 0.1721\nEstimator 315/1000, Train metric: 0.1636, Validation metric: 0.1719\nEstimator 316/1000, Train metric: 0.1634, Validation metric: 0.1717\nEstimator 317/1000, Train metric: 0.1633, Validation metric: 0.1716\nEstimator 318/1000, Train metric: 0.1632, Validation metric: 0.1715\nEstimator 319/1000, Train metric: 0.1628, Validation metric: 0.1712\nEstimator 320/1000, Train metric: 0.1627, Validation metric: 0.1711\nEstimator 321/1000, Train metric: 0.1626, Validation metric: 0.1710\nEstimator 322/1000, Train metric: 0.1622, Validation metric: 0.1707\nEstimator 323/1000, Train metric: 0.1618, Validation metric: 0.1704\nEstimator 324/1000, Train metric: 0.1615, Validation metric: 0.1701\nEstimator 325/1000, Train metric: 0.1614, Validation metric: 0.1701\nEstimator 326/1000, Train metric: 0.1614, Validation metric: 0.1700\nEstimator 327/1000, Train metric: 0.1613, Validation metric: 0.1700\nEstimator 328/1000, Train metric: 0.1610, Validation metric: 0.1697\nEstimator 329/1000, Train metric: 0.1608, Validation metric: 0.1695\nEstimator 330/1000, Train metric: 0.1607, Validation metric: 0.1695\nEstimator 331/1000, Train metric: 0.1606, Validation metric: 0.1695\nEstimator 332/1000, Train metric: 0.1603, Validation metric: 0.1692\nEstimator 333/1000, Train metric: 0.1602, Validation metric: 0.1690\nEstimator 334/1000, Train metric: 0.1600, Validation metric: 0.1687\nEstimator 335/1000, Train metric: 0.1598, Validation metric: 0.1686\nEstimator 336/1000, Train metric: 0.1597, Validation metric: 0.1685\nEstimator 337/1000, Train metric: 0.1596, Validation metric: 0.1684\nEstimator 338/1000, Train metric: 0.1595, Validation metric: 0.1683\nEstimator 339/1000, Train metric: 0.1594, Validation metric: 0.1684\nEstimator 340/1000, Train metric: 0.1593, Validation metric: 0.1683\nEstimator 341/1000, Train metric: 0.1590, Validation metric: 0.1681\nEstimator 342/1000, Train metric: 0.1588, Validation metric: 0.1678\nEstimator 343/1000, Train metric: 0.1586, Validation metric: 0.1676\nEstimator 344/1000, Train metric: 0.1585, Validation metric: 0.1675\nEstimator 345/1000, Train metric: 0.1583, Validation metric: 0.1674\nEstimator 346/1000, Train metric: 0.1580, Validation metric: 0.1672\nEstimator 347/1000, Train metric: 0.1580, Validation metric: 0.1671\nEstimator 348/1000, Train metric: 0.1579, Validation metric: 0.1671\nEstimator 349/1000, Train metric: 0.1579, Validation metric: 0.1671\nEstimator 350/1000, Train metric: 0.1577, Validation metric: 0.1669\nEstimator 351/1000, Train metric: 0.1575, Validation metric: 0.1667\nEstimator 352/1000, Train metric: 0.1572, Validation metric: 0.1665\nEstimator 353/1000, Train metric: 0.1570, Validation metric: 0.1663\nEstimator 354/1000, Train metric: 0.1566, Validation metric: 0.1659\nEstimator 355/1000, Train metric: 0.1563, Validation metric: 0.1656\nEstimator 356/1000, Train metric: 0.1562, Validation metric: 0.1655\nEstimator 357/1000, Train metric: 0.1560, Validation metric: 0.1654\nEstimator 358/1000, Train metric: 0.1559, Validation metric: 0.1652\nEstimator 359/1000, Train metric: 0.1558, Validation metric: 0.1652\nEstimator 360/1000, Train metric: 0.1556, Validation metric: 0.1650\nEstimator 361/1000, Train metric: 0.1554, Validation metric: 0.1651\nEstimator 362/1000, Train metric: 0.1551, Validation metric: 0.1648\nEstimator 363/1000, Train metric: 0.1550, Validation metric: 0.1646\nEstimator 364/1000, Train metric: 0.1549, Validation metric: 0.1647\nEstimator 365/1000, Train metric: 0.1549, Validation metric: 0.1647\nEstimator 366/1000, Train metric: 0.1547, Validation metric: 0.1645\nEstimator 367/1000, Train metric: 0.1546, Validation metric: 0.1644\nEstimator 368/1000, Train metric: 0.1\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n21\nEstimator 478/1000, Train metric: 0.1419\nEstimator 479/1000, Train metric: 0.1418\nEstimator 480/1000, Train metric: 0.1417\nEstimator 481/1000, Train metric: 0.1416\nEstimator 482/1000, Train metric: 0.1415\nEstimator 483/1000, Train metric: 0.1413\nEstimator 484/1000, Train metric: 0.1412\nEstimator 485/1000, Train metric: 0.1410\nEstimator 486/1000, Train metric: 0.1409\nEstimator 487/1000, Train metric: 0.1408\nEstimator 488/1000, Train metric: 0.1408\nEstimator 489/1000, Train metric: 0.1407\nEstimator 490/1000, Train metric: 0.1406\nEstimator 491/1000, Train metric: 0.1405\nEstimator 492/1000, Train metric: 0.1404\nEstimator 493/1000, Train metric: 0.1403\nEstimator 494/1000, Train metric: 0.1402\nEstimator 495/1000, Train metric: 0.1402\nEstimator 496/1000, Train metric: 0.1399\nEstimator 497/1000, Train metric: 0.1398\nEstimator 498/1000, Train metric: 0.1398\nEstimator 499/1000, Train metric: 0.1397\nEstimator 500/1000, Train metric: 0.1396\nEstimator 501/1000, Train metric: 0.1395\nEstimator 502/1000, Train metric: 0.1392\nEstimator 503/1000, Train metric: 0.1391\nEstimator 504/1000, Train metric: 0.1391\nEstimator 505/1000, Train metric: 0.1390\nEstimator 506/1000, Train metric: 0.1390\nEstimator 507/1000, Train metric: 0.1389\nEstimator 508/1000, Train metric: 0.1388\nEstimator 509/1000, Train metric: 0.1387\nEstimator 510/1000, Train metric: 0.1386\nEstimator 511/1000, Train metric: 0.1386\nEstimator 512/1000, Train metric: 0.1384\nEstimator 513/1000, Train metric: 0.1383\nEstimator 514/1000, Train metric: 0.1383\nEstimator 515/1000, Train metric: 0.1382\nEstimator 516/1000, Train metric: 0.1381\nEstimator 517/1000, Train metric: 0.1380\nEstimator 518/1000, Train metric: 0.1379\nEstimator 519/1000, Train metric: 0.1378\nEstimator 520/1000, Train metric: 0.1377\nEstimator 521/1000, Train metric: 0.1376\nEstimator 522/1000, Train metric: 0.1375\nEstimator 523/1000, Train metric: 0.1374\nEstimator 524/1000, Train metric: 0.1373\nEstimator 525/1000, Train metric: 0.1372\nEstimator 526/1000, Train metric: 0.1372\nEstimator 527/1000, Train metric: 0.1372\nEstimator 528/1000, Train metric: 0.1371\nEstimator 529/1000, Train metric: 0.1370\nEstimator 530/1000, Train metric: 0.1369\nEstimator 531/1000, Train metric: 0.1368\nEstimator 532/1000, Train metric: 0.1368\nEstimator 533/1000, Train metric: 0.1367\nEstimator 534/1000, Train metric: 0.1367\nEstimator 535/1000, Train metric: 0.1366\nEstimator 536/1000, Train metric: 0.1366\nEstimator 537/1000, Train metric: 0.1365\nEstimator 538/1000, Train metric: 0.1365\nEstimator 539/1000, Train metric: 0.1365\nEstimator 540/1000, Train metric: 0.1363\nEstimator 541/1000, Train metric: 0.1363\nEstimator 542/1000, Train metric: 0.1362\nEstimator 543/1000, Train metric: 0.1361\nEstimator 544/1000, Train metric: 0.1361\nEstimator 545/1000, Train metric: 0.1360\nEstimator 546/1000, Train metric: 0.1359\nEstimator 547/1000, Train metric: 0.1359\nEstimator 548/1000, Train metric: 0.1358\nEstimator 549/1000, Train metric: 0.1357\nEstimator 550/1000, Train metric: 0.1357\nEstimator 551/1000, Train metric: 0.1356\nEstimator 552/1000, Train metric: 0.1356\nEstimator 553/1000, Train metric: 0.1355\nEstimator 554/1000, Train metric: 0.1354\nEstimator 555/1000, Train metric: 0.1353\nEstimator 556/1000, Train metric: 0.1352\nEstimator 557/1000, Train metric: 0.1352\nEstimator 558/1000, Train metric: 0.1351\nEstimator 559/1000, Train metric: 0.1350\nEstimator 560/1000, Train metric: 0.1350\nEstimator 561/1000, Train metric: 0.1349\nEstimator 562/1000, Train metric: 0.1348\nEstimator 563/1000, Train metric: 0.1347\nEstimator 564/1000, Train metric: 0.1346\nEstimator 565/1000, Train metric: 0.1346\nEstimator 566/1000, Train metric: 0.1345\nEstimator 567/1000, Train metric: 0.1345\nEstimator 568/1000, Train metric: 0.1344\nEstimator 569/1000, Train metric: 0.1343\nEstimator 570/1000, Train metric: 0.1342\nEstimator 571/1000, Train metric: 0.1341\nEstimator 572/1000, Train metric: 0.1341\nEstimator 573/1000, Train metric: 0.1340\nEstimator 574/1000, Train metric: 0.1339\nEstimator 575/1000, Train metric: 0.1339\nEstimator 576/1000, Train metric: 0.1338\nEstimator 577/1000, Train metric: 0.1338\nEstimator 578/1000, Train metric: 0.1337\nEstimator 579/1000, Train metric: 0.1337\nEstimator 580/1000, Train metric: 0.1337\nEstimator 581/1000, Train metric: 0.1336\nEstimator 582/1000, Train metric: 0.1336\nEstimator 583/1000, Train metric: 0.1336\nEstimator 584/1000, Train metric: 0.1335\nEstimator 585/1000, Train metric: 0.1335\nEstimator 586/1000, Train metric: 0.1334\nEstimator 587/1000, Train metric: 0.1333\nEstimator 588/1000, Train metric: 0.1333\nEstimator 589/1000, Train metric: 0.1332\nEstimator 590/1000, Train metric: 0.1331\nEstimator 591/1000, Train metric: 0.1330\nEstimator 592/1000, Train metric: 0.1329\nEstimator 593/1000, Train metric: 0.1329\nEstimator 594/1000, Train metric: 0.1329\nEstimator 595/1000, Train metric: 0.1328\nEstimator 596/1000, Train metric: 0.1328\nEstimator 597/1000, Train metric: 0.1327\nEstimator 598/1000, Train metric: 0.1326\nEstimator 599/1000, Train metric: 0.1326\nEstimator 600/1000, Train metric: 0.1325\nEstimator 601/1000, Train metric: 0.1324\nEstimator 602/1000, Train metric: 0.1324\nEstimator 603/1000, Train metric: 0.1324\nEstimator 604/1000, Train metric: 0.1323\nEstimator 605/1000, Train metric: 0.1323\nEstimator 606/1000, Train metric: 0.1323\nEstimator 607/1000, Train metric: 0.1322\nEstimator 608/1000, Train metric: 0.1320\nEstimator 609/1000, Train metric: 0.1320\nEstimator 610/1000, Train metric: 0.1319\nEstimator 611/1000, Train metric: 0.1318\nEstimator 612/1000, Train metric: 0.1318\nEstimator 613/1000, Train metric: 0.1317\nEstimator 614/1000, Train metric: 0.1317\nEstimator 615/1000, Train metric: 0.1316\nEstimator 616/1000, Train metric: 0.1316\nEstimator 617/1000, Train metric: 0.1315\nEstimator 618/1000, Train metric: 0.1315\nEstimator 619/1000, Train metric: 0.1314\nEstimator 620/1000, Train metric: 0.1313\nEstimator 621/1000, Train metric: 0.1312\nEstimator 622/1000, Train metric: 0.1311\nEstimator 623/1000, Train metric: 0.1311\nEstimator 624/1000, Train metric: 0.1310\nEstimator 625/1000, Train metric: 0.1309\nEstimator 626/1000, Train metric: 0.1309\nEstimator 627/1000, Train metric: 0.1308\nEstimator 628/1000, Train metric: 0.1308\nEstimator 629/1000, Train metric: 0.1308\nEstimator 630/1000, Train metric: 0.1307\nEstimator 631/1000, Train metric: 0.1307\nEstimator 632/1000, Train metric: 0.1306\nEstimator 633/1000, Train metric: 0.1305\nEstimator 634/1000, Train metric: 0.1305\nEstimator 635/1000, Train metric: 0.1305\nEstimator 636/1000, Train metric: 0.1304\nEstimator 637/1000, Train metric: 0.1304\nEstimator 638/1000, Train metric: 0.1303\nEstimator 639/1000, Train metric: 0.1303\nEstimator 640/1000, Train metric: 0.1302\nEstimator 641/1000, Train metric: 0.1301\nEstimator 642/1000, Train metric: 0.1300\nEstimator 643/1000, Train metric: 0.1300\nEstimator 644/1000, Train metric: 0.1300\nEstimator 645/1000, Train metric: 0.1299\nEstimator 646/1000, Train metric: 0.1299\nEstimator 647/1000, Train metric: 0.1298\nEstimator 648/1000, Train metric: 0.1298\nEstimator 649/1000, Train metric: 0.1297\nEstimator 650/1000, Train metric: 0.1297\nEstimator 651/1000, Train metric: 0.1296\nEstimator 652/1000, Train metric: 0.1296\nEstimator 653/1000, Train metric: 0.1295\nEstimator 654/1000, Train metric: 0.1294\nEstimator 655/1000, Train metric: 0.1294\nEstimator 656/1000, Train metric: 0.1293\nEstimator 657/1000, Train metric: 0.1293\nEstimator 658/1000, Train metric: 0.1293\nEstimator 659/1000, Train metric: 0.1293\nEstimator 660/1000, Train metric: 0.1292\nEstimator 661/1000, Train metric: 0.1291\nEstimator 662/1000, Train metric: 0.1291\nEstimator 663/1000, Train metric: 0.1290\nEstimator 664/1000, Train metric: 0.1290\nEstimator 665/1000, Train metric: 0.1289\nEstimator 666/1000, Train metric: 0.1288\nEstimator 667/1000, Train metric: 0.1288\nEstimator 668/1000, Train metric: 0.1287\nEstimator 669/1000, Train metric: 0.1286\nEstimator 670/1000, Train metric: 0.1286\nEstimator 671/1000, Train metric: 0.1285\nEstimator 672/1000, Train metric: 0.1284\nEstimator 673/1000, Train metric: 0.1284\nEstimator 674/1000, Train metric: 0.1283\nEstimator 675/1000, Train metric: 0.1282\nEstimator 676/1000, Train metric: 0.1282\nEstimator 677/1000, Train metric: 0.1282\nEstimator 678/1000, Train metric: 0.1281\nEstimator 679/1000, Train metric: 0.1280\nEstimator 680/1000, Train metric: 0.1280\nEstimator 681/1000, Train metric: 0.1280\nEstimator 682/1000, Train metric: 0.1279\nEstimator 683/1000, Train metric: 0.1279\nEstimator 684/1000, Train metric: 0.1279\nEstimator 685/1000, Train metric: 0.1278\nEstimator 686/1000, Train metric: 0.1278\nEstimator 687/1000, Train metric: 0.1278\nEstimator 688/1000, Train metric: 0.1277\nEstimator 689/1000, Train metric: 0.1277\nEstimator 690/1000, Train metric: 0.1276\nEstimator 691/1000, Train metric: 0.1275\nEstimator 692/1000, Train metric: 0.1274\nEstimator 693/1000, Train metric: 0.1274\nEstimator 694/1000, Train metric: 0.1273\nEstimator 695/1000, Train metric: 0.1273\nEstimator 696/1000, Train metric: 0.1272\nEstimator 697/1000, Train metric: 0.1272\nEstimator 698/1000, Train metric: 0.1271\nEstimator 699/1000, Train metric: 0.1271\nEstimator 700/1000, Train metric: 0.1270\nEstimator 701/1000, Train metric: 0.1270\nEstimator 702/1000, Train metric: 0.1269\nEstimator 703/1000, Train metric: 0.1269\nEstimator 704/1000, Train metric: 0.1268\nEstimator 705/1000, Train metric: 0.1268\nEstimator 706/1000, Train metric: 0.1267\nEstimator 707/1000, Train metric: 0.1266\nEstimator 708/1000, Train metric: 0.1266\nEstimator 709/1000, Train metric: 0.1265\nEstimator 710/1000, Train metric: 0.1264\nEstimator 711/1000, Train metric: 0.1264\nEstimator 712/1000, Train metric: 0.1264\nEstimator 713/1000, Train metric: 0.1263\nEstimator 714/1000, Train metric: 0.1263\nEstimator 715/1000, Train metric: 0.1262\nEstimator 716/1000, Train metric: 0.1262\nEstimator 717/1000, Train metric: 0.1261\nEstimator 718/1000, Train metric: 0.1261\nEstimator 719/1000, Train metric: 0.1261\nEstimator 720/1000, Train metric: 0.1260\nEstimator 721/1000, Train metric: 0.1259\nEstimator 722/1000, Train metric: 0.1259\nEstimator 723/1000, Train metric: 0.1258\nEstimator 724/1000, Train metric: 0.1258\nEstimator 725/1000, Train metric: 0.1258\nEstimator 726/1000, Train metric: 0.1258\nEstimator 727/1000, Train metric: 0.1257\nEstimator 728/1000, Train metric: 0.1257\nEstimator 729/1000, Train metric: 0.1256\nEstimator 730/1000, Train metric: 0.1256\nEstimator 731/1000, Train metric: 0.1255\nEstimator 732/1000, Train metric: 0.1255\nEstimator 733/1000, Train metric: 0.1254\nEstimator 734/1000, Train metric: 0.1254\nEstimator 735/1000, Train metric: 0.1254\nEstimator 736/1000, Train metric: 0.1253\nEstimator 737/1000, Train metric: 0.1252\nEstimator 738/1000, Train metric: 0.1252\nEstimator 739/1000, Train metric: 0.1251\nEstimator 740/1000, Train metric: 0.1250\nEstimator 741/1000, Train metric: 0.1248\nEstimator 742/1000, Train metric: 0.1247\nEstimator 743/1000, Train metric: 0.1246\nEstimator 744/1000, Train metric: 0.1246\nEstimator 745/1000, Train metric: 0.1246\nEstimator 746/1000, Train metric: 0.1245\nEstimator 747/1000, Train metric: 0.1245\nEstimator 748/1000, Train metric: 0.1244\nEstimator 749/1000, Train metric: 0.1244\nEstimator 750/1000, Train metric: 0.1244\nEstimator 751/1000, Train metric: 0.1244\nEstimator 752/1000, Train metric: 0.1243\nEstimator 753/1000, Train metric: 0.1243\nEstimator 754/1000, Train metric: 0.1242\nEstimator 755/1000, Train metric: 0.1242\nEstimator 756/1000, Train metric: 0.1242\nEstimator 757/1000, Train metric: 0.1242\nEstimator 758/1000, Train metric: 0.1241\nEstimator 759/1000, Train metric: 0.1241\nEstimator 760/1000, Train metric: 0.1241\nEstimator 761/1000, Train metric: 0.1240\nEstimator 762/1000, Train metric: 0.1240\nEstimator 763/1000, Train metric: 0.1240\nEstimator 764/1000, Train metric: 0.1239\nEstimator 765/1000, Train metric: 0.1239\nEstimator 766/1000, Train metric: 0.1238\nEstimator 767/1000, Train metric: 0.1238\nEstimator 768/1000, Train metric: 0.1238\nEstimator 769/1000, Train metric: 0.1238\nEstimator 770/1000, Train metric: 0.1238\nEstimator 771/1000, Train metric: 0.1237\nEstimator 772/1000, Train metric: 0.1237\nEstimator 773/1000, Train metric: 0.1236\nEstimator 774/1000, Train metric: 0.1236\nEstimator 775/1000, Train metric: 0.1236\nEstimator 776/1000, Train metric: 0.1235\nEstimator 777/1000, Train metric: 0.1235\nEstimator 778/1000, Train metric: 0.1235\nEstimator 779/1000, Train metric: 0.1235\nEstimator 780/1000, Train metric: 0.1234\nEstimator 781/1000, Train metric: 0.1234\nEstimator 782/1000, Train metric: 0.1233\nEstimator 783/1000, Train metric: 0.1233\nEstimator 784/1000, Train metric: 0.1233\nEstimator 785/1000, Train metric: 0.1233\nEstimator 786/1000, Train metric: 0.1232\nEstimator 787/1000, Train metric: 0.1232\nEstimator 788/1000, Train metric: 0.1232\nEstimator 789/1000, Train metric: 0.1231\nEstimator 790/1000, Train metric: 0.1231\nEstimator 791/1000, Train metric: 0.1230\nEstimator 792/1000, Train metric: 0.1230\nEstimator 793/1000, Train metric: 0.1230\nEstimator 794/1000, Train metric: 0.1229\nEstimator 795/1000, Train metric: 0.1229\nEstimator 796/1000, Train metric: 0.1228\nEstimator 797/1000, Train metric: 0.1228\nEstimator 798/1000, Train metric: 0.1228\nEstimator 799/1000, Train metric: 0.1227\nEstimator 800/1000, Train metric: 0.1227\nEstimator 801/1000, Train metric: 0.1226\nEstimator 802/1000, Train metric: 0.1226\nEstimator 803/1000, Train metric: 0.1226\nEstimator 804/1000, Train metric: 0.1225\nEstimator 805/1000, Train metric: 0.1225\nEstimator 806/1000, Train metric: 0.1224\nEstimator 807/1000, Train metric: 0.1224\nEstimator 808/1000, Train metric: 0.1224\nEstimator 809/1000, Train metric: 0.1223\nEstimator 810/1000, Train metric: 0.1223\nEstimator 811/1000, Train metric: 0.1222\nEstimator 812/1000, Train metric: 0.1221\nEstimator 813/1000, Train metric: 0.1221\nEstimator 814/1000, Train metric: 0.1221\nEstimator 815/1000, Train metric: 0.1221\nEstimator 816/1000, Train metric: 0.1220\nEstimator 817/1000, Train metric: 0.1220\nEstimator 818/1000, Train metric: 0.1219\nEstimator 819/1000, Train metric: 0.1219\nEstimator 820/1000, Train metric: 0.1218\nEstimator 821/1000, Train metric: 0.1218\nEstimator 822/1000, Train metric: 0.1218\nEstimator 823/1000, Train metric: 0.1217\nEstimator 824/1000, Train metric: 0.1217\nEstimator 825/1000, Train metric: 0.1217\nEstimator 826/1000, Train metric: 0.1217\nEstimator 827/1000, Train metric: 0.1216\nEstimator 828/1000, Train metric: 0.1216\nEstimator 829/1000, Train metric: 0.1216\nEstimator 830/1000, Train metric: 0.1216\nEstimator 831/1000, Train metric: 0.1216\nEstimator 832/1000, Train metric: 0.1215\nEstimator 833/1000, Train metric: 0.1215\nEstimator 834/1000, Train metric: 0.1214\nEstimator 835/1000, Train metric: 0.1214\nEstimator 836/1000, Train metric: 0.1214\nEstimator 837/1000, Train metric: 0.1213\nEstimator 838/1000, Train metric: 0.1213\nEstimator 839/1000, Train metric: 0.1212\nEstimator 840/1000, Train metric: 0.1212\nEstimator 841/1000, Train metric: 0.1211\nEstimator 842/1000, Train metric: 0.1211\nEstimator 843/1000, Train metric: 0.1211\nEstimator 844/1000, Train metric: 0.1210\nEstimator 845/1000, Train metric: 0.1210\nEstimator 846/1000, Train metric: 0.1209\nEstimator 847/1000, Train metric: 0.1209\nEstimator 848/1000, Train metric: 0.1209\nEstimator 849/1000, Train metric: 0.1208\nEstimator 850/1000, Train metric: 0.1208\nEstimator 851/1000, Train metric: 0.1208\nEstimator 852/1000, Train metric: 0.1207\nEstimator 853/1000, Train metric: 0.1207\nEstimator 854/1000, Train metric: 0.1207\nEstimator 855/1000, Train metric: 0.1206\nEstimator 856/1000, Train metric: 0.1206\nEstimator 857/1000, Train metric: 0.1205\nEstimator 858/1000, Train metric: 0.1205\nEstimator 859/1000, Train metric: 0.1205\nEstimator 860/1000, Train metric: 0.1204\nEstimator 861/1000, Train metric: 0.1204\nEstimator 862/1000, Train metric: 0.1204\nEstimator 863/1000, Train metric: 0.1204\nEstimator 864/1000, Train metric: 0.1203\nEstimator 865/1000, Train metric: 0.1203\nEstimator 866/1000, Train metric: 0.1203\nEstimator 867/1000, Train metric: 0.1203\nEstimator 868/1000, Train metric: 0.1202\nEstimator 869/1000, Train metric: 0.1202\nEstimator 870/1000, Train metric: 0.1202\nEstimator 871/1000, Train metric: 0.1201\nEstimator 872/1000, Train metric: 0.1201\nEstimator 873/1000, Train metric: 0.1201\nEstimator 874/1000, Train metric: 0.1201\nEstimator 875/1000, Train metric: 0.1200\nEstimator 876/1000, Train metric: 0.1200\nEstimator 877/1000, Train metric: 0.1200\nEstimator 878/1000, Train metric: 0.1200\nEstimator 879/1000, Train metric: 0.1199\nEstimator 880/1000, Train metric: 0.1199\nEstimator 881/1000, Train metric: 0.1199\nEstimator 882/1000, Train metric: 0.1198\nEstimator 883/1000, Train metric: 0.1198\nEstimator 884/1000, Train metric: 0.1198\nEstimator 885/1000, Train metric: 0.1198\nEstimator 886/1000, Train metric: 0.1197\nEstimator 887/1000, Train metric: 0.1197\nEstimator 888/1000, Train metric: 0.1197\nEstimator 889/1000, Train metric: 0.1196\nEstimator 890/1000, Train metric: 0.1196\nEstimator 891/1000, Train metric: 0.1196\nEstimator 892/1000, Train metric: 0.1195\nEstimator 893/1000, Train metric: 0.1195\nEstimator 894/1000, Train metric: 0.1194\nEstimator 895/1000, Train metric: 0.1194\nEstimator 896/1000, Train metric: 0.1193\nEstimator 897/1000, Train metric: 0.1193\nEstimator 898/1000, Train metric: 0.1193\nEstimator 899/1000, Train metric: 0.1193\nEstimator 900/1000, Train metric: 0.1192\nEstimator 901/1000, Train metric: 0.1192\nEstimator 902/1000, Train metric: 0.1192\nEstimator 903/1000, Train metric: 0.1191\nEstimator 904/1000, Train metric: 0.1191\nEstimator 905/1000, Train metric: 0.1191\nEstimator 906/1000, Train metric: 0.1191\nEstimator 907/1000, Train metric: 0.1191\nEstimator 908/1000, Train metric: 0.1190\nEstimator 909/1000, Train metric: 0.1190\nEstimator 910/1000, Train metric: 0.1190\nEstimator 911/1000, Train metric: 0.1190\nEstimator 912/1000, Train metric: 0.1190\nEstimator 913/1000, Train metric: 0.1190\nEstimator 914/1000, Train metric: 0.1189\nEstimator 915/1000, Train metric: 0.1189\nEstimator 916/1000, Train metric: 0.1188\nEstimator 917/1000, Train metric: 0.1188\nEstimator 918/1000, Train metric: 0.1188\nEstimator 919/1000, Train metric: 0.1187\nEstimator 920/1000, Train metric: 0.1187\nEstimator 921/1000, Train metric: 0.1187\nEstimator 922/1000, Train metric: 0.1187\nEstimator 923/1000, Train metric: 0.1186\nEstimator 924/1000, Train metric: 0.1186\nEstimator 925/1000, Train metric: 0.1186\nEstimator 926/1000, Train metric: 0.1186\nEstimator 927/1000, Train metric: 0.1185\nEstimator 928/1000, Train metric: 0.1185\nEstimator 929/1000, Train metric: 0.1185\nEstimator 930/1000, Train metric: 0.1184\nEstimator 931/1000, Train metric: 0.1184\nEstimator 932/1000, Train metric: 0.1184\nEstimator 933/1000, Train metric: 0.1183\nEstimator 934/1000, Train metric: 0.1183\nEstimator 935/1000, Train metric: 0.1182\nEstimator 936/1000, Train metric: 0.1182\nEstimator 937/1000, Train metric: 0.1182\nEstimator 938/1000, Train metric: 0.1182\nEstimator 939/1000, Train metric: 0.1182\nEstimator 940/1000, Train metric: 0.1181\nEstimator 941/1000, Train metric: 0.1181\nEstimator 942/1000, Train metric: 0.1181\nEstimator 943/1000, Train metric: 0.1180\nEstimator 944/1000, Train metric: 0.1180\nEstimator 945/1000, Train metric: 0.1180\nEstimator 946/1000, Train metric: 0.1180\nEstimator 947/1000, Train metric: 0.1179\nEstimator 948/1000, Train metric: 0.1179\nEstimator 949/1000, Train metric: 0.1179\nEstimator 950/1000, Train metric: 0.1179\nEstimator 951/1000, Train metric: 0.1179\nEstimator 952/1000, Train metric: 0.1178\nEstimator 953/1000, Train metric: 0.1178\nEstimator 954/1000, Train metric: 0.1178\nEstimator 955/1000, Train metric: 0.1177\nEstimator 956/1000, Train metric: 0.1177\nEstimator 957/1000, Train metric: 0.1177\nEstimator 958/1000, Train metric: 0.1177\nEstimator 959/1000, Train metric: 0.1176\nEstimator 960/1000, Train metric: 0.1175\nEstimator 961/1000, Train metric: 0.1175\nEstimator 962/1000, Train metric: 0.1174\nEstimator 963/1000, Train metric: 0.1173\nEstimator 964/1000, Train metric: 0.1173\nEstimator 965/1000, Train metric: 0.1173\nEstimator 966/1000, Train metric: 0.1172\nEstimator 967/1000, Train metric: 0.1172\nEstimator 968/1000, Train metric: 0.1172\nEstimator 969/1000, Train metric: 0.1172\nEstimator 970/1000, Train metric: 0.1171\nEstimator 971/1000, Train metric: 0.1171\nEstimator 972/1000, Train metric: 0.1171\nEstimator 973/1000, Train metric: 0.1170\nEstimator 974/1000, Train metric: 0.1170\nEstimator 975/1000, Train metric: 0.1170\nEstimator 976/1000, Train metric: 0.1170\nEstimator 977/1000, Train metric: 0.1170\nEstimator 978/1000, Train metric: 0.1169\nEstimator 979/1000, Train metric: 0.1169\nEstimator 980/1000, Train metric: 0.1168\nEstimator 981/1000, Train metric: 0.1168\nEstimator 982/1000, Train metric: 0.1168\nEstimator 983/1000, Train metric: 0.1168\nEstimator 984/1000, Train metric: 0.1168\nEstimator 985/1000, Train metric: 0.1167\nEstimator 986/1000, Train metric: 0.1167\nEstimator 987/1000, Train metric: 0.1166\nEstimator 988/1000, Train metric: 0.1166\nEstimator 989/1000, Train metric: 0.1166\nEstimator 990/1000, Train metric: 0.1166\nEstimator 991/1000, Train metric: 0.1165\nEstimator 992/1000, Train metric: 0.1165\nEstimator 993/1000, Train metric: 0.1165\nEstimator 994/1000, Train metric: 0.1165\nEstimator 995/1000, Train metric: 0.1164\nEstimator 996/1000, Train metric: 0.1164\nEstimator 997/1000, Train metric: 0.1164\nEstimator 998/1000, Train metric: 0.1164\nEstimator 999/1000, Train metric: 0.1164\nElapsed time for fitting PGBM model: 99.92 s\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/pgbm/pgbm.py:420: UserWarning: FALLBACK path has been taken inside: compileCudaFusionGroup. This is an indication that codegen Failed for some reason.\nTo debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\nTo report the issue, try enable logging via setting the envvariable ` export PYTORCH_JIT_LOG_LEVEL=manager.cpp`\n (Triggered internally at ../torch/csrc/jit/codegen/cuda/manager.cpp:239.)\n  mu, variance = _predict_forest_muvar(X_test_splits, self.nodes_idx,\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/pgbm/pgbm.py:420: UserWarning: FALLBACK path has been taken inside: runCudaFusionGroup. This is an indication that codegen Failed for some reason.\nTo debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\n (Triggered internally at ../torch/csrc/jit/codegen/cuda/manager.cpp:331.)\n  mu, variance = _predict_forest_muvar(X_test_splits, self.nodes_idx,\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/lidl_x_tum_uncertainty_estimation/uncertainty_estimation_models.py:552: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  nll_temp = torch.tensor([-dist[i].log_prob(torch.tensor(y_test[i])) for i in range(len(dist))])\npath: predictions/pointpredictions_pgbm_normal.csv\ndirname: predictions\nfilename: pointpredictions_pgbm_normal.csv\nartifact_path: predictions\npath: /tmp/tmp6qklfqzv/pointpredictions_pgbm_normal.csv\ntmp_path: /tmp/tmp6qklfqzv/pointpredictions_pgbm_normal.csv\npath: predictions/samples_pgbm_normal.csv\ndirname: predictions\nfilename: samples_pgbm_normal.csv\nartifact_path: predictions\npath: /tmp/tmpd2g6_75s/samples_pgbm_normal.csv\ntmp_path: /tmp/tmpd2g6_75s/samples_pgbm_normal.csv\npath: predictions/distparams_pgbm_normal.csv\ndirname: predictions\nfilename: distparams_pgbm_normal.csv\nartifact_path: predictions\npath: /tmp/tmp2imonpez/distparams_pgbm_normal.csv\ntmp_path: /tmp/tmp2imonpez/distparams_pgbm_normal.csv\npath: predictions/quantiles_pgbm_normal0.05.csv\ndirname: predictions\nfilename: quantiles_pgbm_normal0.05.csv\nartifact_path: predictions\npath: /tmp/tmpnrtuwcv9/quantiles_pgbm_normal0.05.csv\ntmp_path: /tmp/tmpnrtuwcv9/quantiles_pgbm_normal0.05.csv\npath: predictions/quantiles_pgbm_normal0.1.csv\ndirname: predictions\nfilename: quantiles_pgbm_normal0.1.csv\nartifact_path: predictions\npath: /tmp/tmpikpa88i1/quantiles_pgbm_normal0.1.csv\ntmp_path: /tmp/tmpikpa88i1/quantiles_pgbm_normal0.1.csv\npath: predictions/quantiles_pgbm_normal0.5.csv\ndirname: predictions\nfilename: quantiles_pgbm_normal0.5.csv\nartifact_path: predictions\npath: /tmp/tmpaqzq28wj/quantiles_pgbm_normal0.5.csv\ntmp_path: /tmp/tmpaqzq28wj/quantiles_pgbm_normal0.5.csv\npath: predictions/quantiles_pgbm_normal0.9.csv\ndirname: predictions\nfilename: quantiles_pgbm_normal0.9.csv\nartifact_path: predictions\npath: /tmp/tmp3v0e6k78/quantiles_pgbm_normal0.9.csv\ntmp_path: /tmp/tmp3v0e6k78/quantiles_pgbm_normal0.9.csv\npath: predictions/quantiles_pgbm_normal0.95.csv\ndirname: predictions\nfilename: quantiles_pgbm_normal0.95.csv\nartifact_path: predictions\npath: /tmp/tmpk6aq1ndg/quantiles_pgbm_normal0.95.csv\ntmp_path: /tmp/tmpk6aq1ndg/quantiles_pgbm_normal0.95.csv\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["pgbm_normal_params = {\n","    'derivatives': 'exact',\n","    'distribution': 'normal',\n","    'device': 'gpu',\n","    'gpu_device_id': 0,\n","    \"n_jobs\": -1,\n","    \"min_split_gain\": 0.0,\n","    \"min_data_in_leaf\": 1,\n","    \"max_bin\": 1024,\n","    \"max_leaves\": 64,\n","    \"max_depth\": -1,\n","    \"learning_rate\": 0.1,\n","    \"n_estimators\": 1000,\n","    \"feature_fraction\": 0.7,\n","    \"bagging_fraction\": 0.7,\n","    \"seed\": 1,\n","    \"lambda\": 1,\n","}\n","\n","early_stopping_round = 20\n","quantiles = [0.05, 0.1, 0.5, 0.9, 0.95]\n","\n","start_time = time.perf_counter()\n","    \n","# fitting model on train set with early stopping on valid set\n","pgbm_normal_fit_params = {**pgbm_normal_params, \"early_stopping_round\": early_stopping_round}\n","pgbm_normal_reg = PGBM(vectorizer_without_nan, target_transformer=target_transformer)\n","pgbm_normal_reg.fit(train_val_df, TARGET, X_val=valid_df, y_val=np.array(valid_df[TARGET]), params=pgbm_normal_fit_params, apply_optimize_distribution=False, verbose=True)\n","pgbm_normal_best_iteration = pgbm_normal_reg.best_iteration\n","print(\"Early stopping performed. Best iteration:\", pgbm_normal_best_iteration)\n","\n","# fitting model on train+val set with best_iteration\n","pgbm_normal_full_fit_params = {**pgbm_normal_params, \"n_estimators\": pgbm_normal_best_iteration}\n","pgbm_normal_full_train_reg = PGBM(vectorizer_without_nan, target_transformer=target_transformer)\n","pgbm_normal_full_train_reg.fit(train_df, TARGET, params=pgbm_normal_full_fit_params, apply_optimize_distribution=False, verbose=True)\n","\n","# predicting on test set with our fully trained model\n","pgbm_normal_pred = pgbm_normal_full_train_reg.predict(test_df, quantiles=quantiles, prediction_types=[PredEnum.POINT_ESTIMATES, PredEnum.QUANTILES, PredEnum.SAMPLES, PredEnum.DISTRIBUTION_PARAMS], sample_size=300)\n","\n","pgbm_normal_metrics = pgbm_normal_full_train_reg.metrics(np.array(test_df[TARGET]), pgbm_normal_pred, confidence_interval_quantiles=[0.1,0.9])\n","\n","end_time = time.perf_counter()\n","full_time = np.round(end_time - start_time, 2)\n","pgbm_normal_metrics['time'] = full_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pgbm_normal_metrics"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.5.2 PGBM best dist"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c8473d89-daff-4904-841a-a6a53ae0c5d3","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["f49c673699f74b959224c30627c22a7e\n","Training on GPU\n","Estimator 0/1000, Train metric: 0.4159, Validation metric: 0.4036\n","Estimator 1/1000, Train metric: 0.4064, Validation metric: 0.3944\n","Estimator 2/1000, Train metric: 0.3979, Validation metric: 0.3861\n","Estimator 3/1000, Train metric: 0.3906, Validation metric: 0.3795\n","Estimator 4/1000, Train metric: 0.3868, Validation metric: 0.3764\n","Estimator 5/1000, Train metric: 0.3807, Validation metric: 0.3705\n","Estimator 6/1000, Train metric: 0.3765, Validation metric: 0.3670\n","Estimator 7/1000, Train metric: 0.3710, Validation metric: 0.3618\n","Estimator 8/1000, Train metric: 0.3656, Validation metric: 0.3570\n","Estimator 9/1000, Train metric: 0.3606, Validation metric: 0.3524\n","Estimator 10/1000, Train metric: 0.3566, Validation metric: 0.3489\n","Estimator 11/1000, Train metric: 0.3530, Validation metric: 0.3461\n","Estimator 12/1000, Train metric: 0.3502, Validation metric: 0.3444\n","Estimator 13/1000, Train metric: 0.3486, Validation metric: 0.3436\n","Estimator 14/1000, Train metric: 0.3454, Validation metric: 0.3403\n","Estimator 15/1000, Train metric: 0.3428, Validation metric: 0.3380\n","Estimator 16/1000, Train metric: 0.3408, Validation metric: 0.3365\n","Estimator 17/1000, Train metric: 0.3380, Validation metric: 0.3340\n","Estimator 18/1000, Train metric: 0.3367, Validation metric: 0.3330\n","Estimator 19/1000, Train metric: 0.3355, Validation metric: 0.3323\n","Estimator 20/1000, Train metric: 0.3332, Validation metric: 0.3303\n","Estimator 21/1000, Train metric: 0.3314, Validation metric: 0.3286\n","Estimator 22/1000, Train metric: 0.3300, Validation metric: 0.3272\n","Estimator 23/1000, Train metric: 0.3284, Validation metric: 0.3264\n","Estimator 24/1000, Train metric: 0.3267, Validation metric: 0.3251\n","Estimator 25/1000, Train metric: 0.3253, Validation metric: 0.3238\n","Estimator 26/1000, Train metric: 0.3243, Validation metric: 0.3225\n","Estimator 27/1000, Train metric: 0.3234, Validation metric: 0.3214\n","Estimator 28/1000, Train metric: 0.3220, Validation metric: 0.3202\n","Estimator 29/1000, Train metric: 0.3212, Validation metric: 0.3192\n","Estimator 30/1000, Train metric: 0.3194, Validation metric: 0.3176\n","Estimator 31/1000, Train metric: 0.3180, Validation metric: 0.3167\n","Estimator 32/1000, Train metric: 0.3174, Validation metric: 0.3161\n","Estimator 33/1000, Train metric: 0.3157, Validation metric: 0.3143\n","Estimator 34/1000, Train metric: 0.3143, Validation metric: 0.3129\n","Estimator 35/1000, Train metric: 0.3133, Validation metric: 0.3119\n","Estimator 36/1000, Train metric: 0.3109, Validation metric: 0.3095\n","Estimator 37/1000, Train metric: 0.3105, Validation metric: 0.3089\n","Estimator 38/1000, Train metric: 0.3084, Validation metric: 0.3070\n","Estimator 39/1000, Train metric: 0.3071, Validation metric: 0.3056\n","Estimator 40/1000, Train metric: 0.3060, Validation metric: 0.3053\n","Estimator 41/1000, Train metric: 0.3055, Validation metric: 0.3049\n","Estimator 42/1000, Train metric: 0.3043, Validation metric: 0.3038\n","Estimator 43/1000, Train metric: 0.3026, Validation metric: 0.3022\n","Estimator 44/1000, Train metric: 0.3023, Validation metric: 0.3019\n","Estimator 45/1000, Train metric: 0.3004, Validation metric: 0.3000\n","Estimator 46/1000, Train metric: 0.2992, Validation metric: 0.2990\n","Estimator 47/1000, Train metric: 0.2988, Validation metric: 0.2980\n","Estimator 48/1000, Train metric: 0.2983, Validation metric: 0.2975\n","Estimator 49/1000, Train metric: 0.2976, Validation metric: 0.2968\n","Estimator 50/1000, Train metric: 0.2973, Validation metric: 0.2966\n","Estimator 51/1000, Train metric: 0.2964, Validation metric: 0.2956\n","Estimator 52/1000, Train metric: 0.2950, Validation metric: 0.2945\n","Estimator 53/1000, Train metric: 0.2920, Validation metric: 0.2915\n","Estimator 54/1000, Train metric: 0.2910, Validation metric: 0.2905\n","Estimator 55/1000, Train metric: 0.2900, Validation metric: 0.2896\n","Estimator 56/1000, Train metric: 0.2897, Validation metric: 0.2894\n","Estimator 57/1000, Train metric: 0.2874, Validation metric: 0.2871\n","Estimator 58/1000, Train metric: 0.2868, Validation metric: 0.2867\n","Estimator 59/1000, Train metric: 0.2854, Validation metric: 0.2855\n","Estimator 60/1000, Train metric: 0.2850, Validation metric: 0.2848\n","Estimator 61/1000, Train metric: 0.2839, Validation metric: 0.2837\n","Estimator 62/1000, Train metric: 0.2825, Validation metric: 0.2824\n","Estimator 63/1000, Train metric: 0.2822, Validation metric: 0.2822\n","Estimator 64/1000, Train metric: 0.2818, Validation metric: 0.2819\n","Estimator 65/1000, Train metric: 0.2808, Validation metric: 0.2808\n","Estimator 66/1000, Train metric: 0.2801, Validation metric: 0.2801\n","Estimator 67/1000, Train metric: 0.2795, Validation metric: 0.2796\n","Estimator 68/1000, Train metric: 0.2792, Validation metric: 0.2792\n","Estimator 69/1000, Train metric: 0.2775, Validation metric: 0.2778\n","Estimator 70/1000, Train metric: 0.2772, Validation metric: 0.2774\n","Estimator 71/1000, Train metric: 0.2761, Validation metric: 0.2763\n","Estimator 72/1000, Train metric: 0.2744, Validation metric: 0.2747\n","Estimator 73/1000, Train metric: 0.2742, Validation metric: 0.2751\n","Estimator 74/1000, Train metric: 0.2733, Validation metric: 0.2744\n","Estimator 75/1000, Train metric: 0.2731, Validation metric: 0.2743\n","Estimator 76/1000, Train metric: 0.2725, Validation metric: 0.2737\n","Estimator 77/1000, Train metric: 0.2712, Validation metric: 0.2725\n","Estimator 78/1000, Train metric: 0.2711, Validation metric: 0.2723\n","Estimator 79/1000, Train metric: 0.2708, Validation metric: 0.2720\n","Estimator 80/1000, Train metric: 0.2703, Validation metric: 0.2714\n","Estimator 81/1000, Train metric: 0.2675, Validation metric: 0.2687\n","Estimator 82/1000, Train metric: 0.2663, Validation metric: 0.2675\n","Estimator 83/1000, Train metric: 0.2650, Validation metric: 0.2663\n","Estimator 84/1000, Train metric: 0.2628, Validation metric: 0.2641\n","Estimator 85/1000, Train metric: 0.2624, Validation metric: 0.2637\n","Estimator 86/1000, Train metric: 0.2607, Validation metric: 0.2620\n","Estimator 87/1000, Train metric: 0.2596, Validation metric: 0.2609\n","Estimator 88/1000, Train metric: 0.2593, Validation metric: 0.2604\n","Estimator 89/1000, Train metric: 0.2584, Validation metric: 0.2595\n","Estimator 90/1000, Train metric: 0.2570, Validation metric: 0.2582\n","Estimator 91/1000, Train metric: 0.2567, Validation metric: 0.2573\n","Estimator 92/1000, Train metric: 0.2553, Validation metric: 0.2561\n","Estimator 93/1000, Train metric: 0.2541, Validation metric: 0.2549\n","Estimator 94/1000, Train metric: 0.2539, Validation metric: 0.2547\n","Estimator 95/1000, Train metric: 0.2535, Validation metric: 0.2544\n","Estimator 96/1000, Train metric: 0.2530, Validation metric: 0.2539\n","Estimator 97/1000, Train metric: 0.2526, Validation metric: 0.2534\n","Estimator 98/1000, Train metric: 0.2523, Validation metric: 0.2532\n","Estimator 99/1000, Train metric: 0.2500, Validation metric: 0.2511\n","Estimator 100/1000, Train metric: 0.2484, Validation metric: 0.2495\n","Estimator 101/1000, Train metric: 0.2476, Validation metric: 0.2488\n","Estimator 102/1000, Train metric: 0.2464, Validation metric: 0.2475\n","Estimator 103/1000, Train metric: 0.2455, Validation metric: 0.2467\n","Estimator 104/1000, Train metric: 0.2448, Validation metric: 0.2460\n","Estimator 105/1000, Train metric: 0.2446, Validation metric: 0.2462\n","Estimator 106/1000, Train metric: 0.2444, Validation metric: 0.2460\n","Estimator 107/1000, Train metric: 0.2434, Validation metric: 0.2452\n","Estimator 108/1000, Train metric: 0.2432, Validation metric: 0.2450\n","Estimator 109/1000, Train metric: 0.2428, Validation metric: 0.2445\n","Estimator 110/1000, Train metric: 0.2426, Validation metric: 0.2444\n","Estimator 111/1000, Train metric: 0.2424, Validation metric: 0.2442\n","Estimator 112/1000, Train metric: 0.2419, Validation metric: 0.2438\n","Estimator 113/1000, Train metric: 0.2409, Validation metric: 0.2428\n","Estimator 114/1000, Train metric: 0.2406, Validation metric: 0.2425\n","Estimator 115/1000, Train metric: 0.2403, Validation metric: 0.2422\n","Estimator 116/1000, Train metric: 0.2401, Validation metric: 0.2420\n","Estimator 117/1000, Train metric: 0.2385, Validation metric: 0.2406\n","Estimator 118/1000, Train metric: 0.2380, Validation metric: 0.2400\n","Estimator 119/1000, Train metric: 0.2378, Validation metric: 0.2399\n","Estimator 120/1000, Train metric: 0.2373, Validation metric: 0.2393\n","Estimator 121/1000, Train metric: 0.2360, Validation metric: 0.2381\n","Estimator 122/1000, Train metric: 0.2352, Validation metric: 0.2374\n","Estimator 123/1000, Train metric: 0.2350, Validation metric: 0.2371\n","Estimator 124/1000, Train metric: 0.2346, Validation metric: 0.2368\n","Estimator 125/1000, Train metric: 0.2341, Validation metric: 0.2362\n","Estimator 126/1000, Train metric: 0.2339, Validation metric: 0.2360\n","Estimator 127/1000, Train metric: 0.2334, Validation metric: 0.2355\n","Estimator 128/1000, Train metric: 0.2328, Validation metric: 0.2350\n","Estimator 129/1000, Train metric: 0.2315, Validation metric: 0.2338\n","Estimator 130/1000, Train metric: 0.2306, Validation metric: 0.2330\n","Estimator 131/1000, Train metric: 0.2305, Validation metric: 0.2329\n","Estimator 132/1000, Train metric: 0.2301, Validation metric: 0.2326\n","Estimator 133/1000, Train metric: 0.2299, Validation metric: 0.2326\n","Estimator 134/1000, Train metric: 0.2298, Validation metric: 0.2324\n","Estimator 135/1000, Train metric: 0.2291, Validation metric: 0.2317\n","Estimator 136/1000, Train metric: 0.2289, Validation metric: 0.2316\n","Estimator 137/1000, Train metric: 0.2287, Validation metric: 0.2315\n","Estimator 138/1000, Train metric: 0.2285, Validation metric: 0.2313\n","Estimator 139/1000, Train metric: 0.2281, Validation metric: 0.2309\n","Estimator 140/1000, Train metric: 0.2276, Validation metric: 0.2303\n","Estimator 141/1000, Train metric: 0.2274, Validation metric: 0.2301\n","Estimator 142/1000, Train metric: 0.2272, Validation metric: 0.2300\n","Estimator 143/1000, Train metric: 0.2260, Validation metric: 0.2288\n","Estimator 144/1000, Train metric: 0.2253, Validation metric: 0.2281\n","Estimator 145/1000, Train metric: 0.2243, Validation metric: 0.2272\n","Estimator 146/1000, Train metric: 0.2238, Validation metric: 0.2268\n","Estimator 147/1000, Train metric: 0.2237, Validation metric: 0.2266\n","Estimator 148/1000, Train metric: 0.2235, Validation metric: 0.2264\n","Estimator 149/1000, Train metric: 0.2233, Validation metric: 0.2264\n","Estimator 150/1000, Train metric: 0.2228, Validation metric: 0.2258\n","Estimator 151/1000, Train metric: 0.2213, Validation metric: 0.2245\n","Estimator 152/1000, Train metric: 0.2211, Validation metric: 0.2244\n","Estimator 153/1000, Train metric: 0.2209, Validation metric: 0.2242\n","Estimator 154/1000, Train metric: 0.2200, Validation metric: 0.2233\n","Estimator 155/1000, Train metric: 0.2191, Validation metric: 0.2225\n","Estimator 156/1000, Train metric: 0.2190, Validation metric: 0.2224\n","Estimator 157/1000, Train metric: 0.2186, Validation metric: 0.2221\n","Estimator 158/1000, Train metric: 0.2183, Validation metric: 0.2217\n","Estimator 159/1000, Train metric: 0.2175, Validation metric: 0.2211\n","Estimator 160/1000, Train metric: 0.2169, Validation metric: 0.2205\n","Estimator 161/1000, Train metric: 0.2164, Validation metric: 0.2200\n","Estimator 162/1000, Train metric: 0.2153, Validation metric: 0.2189\n","Estimator 163/1000, Train metric: 0.2148, Validation metric: 0.2184\n","Estimator 164/1000, Train metric: 0.2145, Validation metric: 0.2181\n","Estimator 165/1000, Train metric: 0.2143, Validation metric: 0.2180\n","Estimator 166/1000, Train metric: 0.2142, Validation metric: 0.2179\n","Estimator 167/1000, Train metric: 0.2133, Validation metric: 0.2171\n","Estimator 168/1000, Train metric: 0.2125, Validation metric: 0.2163\n","Estimator 169/1000, Train metric: 0.2120, Validation metric: 0.2157\n","Estimator 170/1000, Train metric: 0.2119, Validation metric: 0.2156\n","Estimator 171/1000, Train metric: 0.2112, Validation metric: 0.2150\n","Estimator 172/1000, Train metric: 0.2107, Validation metric: 0.2143\n","Estimator 173/1000, Train metric: 0.2102, Validation metric: 0.2138\n","Estimator 174/1000, Train metric: 0.2100, Validation metric: 0.2137\n","Estimator 175/1000, Train metric: 0.2096, Validation metric: 0.2133\n","Estimator 176/1000, Train metric: 0.2093, Validation metric: 0.2130\n","Estimator 177/1000, Train metric: 0.2092, Validation metric: 0.2128\n","Estimator 178/1000, Train metric: 0.2085, Validation metric: 0.2122\n","Estimator 179/1000, Train metric: 0.2078, Validation metric: 0.2116\n","Estimator 180/1000, Train metric: 0.2077, Validation metric: 0.2115\n","Estimator 181/1000, Train metric: 0.2075, Validation metric: 0.2114\n","Estimator 182/1000, Train metric: 0.2069, Validation metric: 0.2108\n","Estimator 183/1000, Train metric: 0.2067, Validation metric: 0.2106\n","Estimator 184/1000, Train metric: 0.2062, Validation metric: 0.2101\n","Estimator 185/1000, Train metric: 0.2051, Validation metric: 0.2090\n","Estimator 186/1000, Train metric: 0.2050, Validation metric: 0.2090\n","Estimator 187/1000, Train metric: 0.2049, Validation metric: 0.2089\n","Estimator 188/1000, Train metric: 0.2042, Validation metric: 0.2082\n","Estimator 189/1000, Train metric: 0.2031, Validation metric: 0.2072\n","Estimator 190/1000, Train metric: 0.2030, Validation metric: 0.2071\n","Estimator 191/1000, Train metric: 0.2023, Validation metric: 0.2065\n","Estimator 192/1000, Train metric: 0.2020, Validation metric: 0.2062\n","Estimator 193/1000, Train metric: 0.2014, Validation metric: 0.2055\n","Estimator 194/1000, Train metric: 0.2012, Validation metric: 0.2049\n","Estimator 195/1000, Train metric: 0.2007, Validation metric: 0.2045\n","Estimator 196/1000, Train metric: 0.2002, Validation metric: 0.2040\n","Estimator 197/1000, Train metric: 0.1995, Validation metric: 0.2033\n","Estimator 198/1000, Train metric: 0.1988, Validation metric: 0.2027\n","Estimator 199/1000, Train metric: 0.1988, Validation metric: 0.2026\n","Estimator 200/1000, Train metric: 0.1985, Validation metric: 0.2025\n","Estimator 201/1000, Train metric: 0.1985, Validation metric: 0.2024\n","Estimator 202/1000, Train metric: 0.1980, Validation metric: 0.2020\n","Estimator 203/1000, Train metric: 0.1971, Validation metric: 0.2011\n","Estimator 204/1000, Train metric: 0.1965, Validation metric: 0.2006\n","Estimator 205/1000, Train metric: 0.1961, Validation metric: 0.2002\n","Estimator 206/1000, Train metric: 0.1960, Validation metric: 0.2001\n","Estimator 207/1000, Train metric: 0.1957, Validation metric: 0.1998\n","Estimator 208/1000, Train metric: 0.1956, Validation metric: 0.1998\n","Estimator 209/1000, Train metric: 0.1950, Validation metric: 0.1993\n","Estimator 210/1000, Train metric: 0.1947, Validation metric: 0.1990\n","Estimator 211/1000, Train metric: 0.1939, Validation metric: 0.1982\n","Estimator 212/1000, Train metric: 0.1936, Validation metric: 0.1979\n","Estimator 213/1000, Train metric: 0.1930, Validation metric: 0.1974\n","Estimator 214/1000, Train metric: 0.1927, Validation metric: 0.1971\n","Estimator 215/1000, Train metric: 0.1926, Validation metric: 0.1971\n","Estimator 216/1000, Train metric: 0.1926, Validation metric: 0.1970\n","Estimator 217/1000, Train metric: 0.1924, Validation metric: 0.1968\n","Estimator 218/1000, Train metric: 0.1922, Validation metric: 0.1967\n","Estimator 219/1000, Train metric: 0.1922, Validation metric: 0.1967\n","Estimator 220/1000, Train metric: 0.1918, Validation metric: 0.1964\n","Estimator 221/1000, Train metric: 0.1914, Validation metric: 0.1960\n","Estimator 222/1000, Train metric: 0.1913, Validation metric: 0.1958\n","Estimator 223/1000, Train metric: 0.1903, Validation metric: 0.1949\n","Estimator 224/1000, Train metric: 0.1902, Validation metric: 0.1947\n","Estimator 225/1000, Train metric: 0.1895, Validation metric: 0.1941\n","Estimator 226/1000, Train metric: 0.1890, Validation metric: 0.1937\n","Estimator 227/1000, Train metric: 0.1887, Validation metric: 0.1934\n","Estimator 228/1000, Train metric: 0.1885, Validation metric: 0.1933\n","Estimator 229/1000, Train metric: 0.1884, Validation metric: 0.1931\n","Estimator 230/1000, Train metric: 0.1877, Validation metric: 0.1925\n","Estimator 231/1000, Train metric: 0.1872, Validation metric: 0.1920\n","Estimator 232/1000, Train metric: 0.1870, Validation metric: 0.1918\n","Estimator 233/1000, Train metric: 0.1863, Validation metric: 0.1911\n","Estimator 234/1000, Train metric: 0.1858, Validation metric: 0.1906\n","Estimator 235/1000, Train metric: 0.1852, Validation metric: 0.1901\n","Estimator 236/1000, Train metric: 0.1851, Validation metric: 0.1901\n","Estimator 237/1000, Train metric: 0.1849, Validation metric: 0.1899\n","Estimator 238/1000, Train metric: 0.1844, Validation metric: 0.1894\n","Estimator 239/1000, Train metric: 0.1843, Validation metric: 0.1893\n","Estimator 240/1000, Train metric: 0.1836, Validation metric: 0.1886\n","Estimator 241/1000, Train metric: 0.1835, Validation metric: 0.1887\n","Estimator 242/1000, Train metric: 0.1831, Validation metric: 0.1883\n","Estimator 243/1000, Train metric: 0.1828, Validation metric: 0.1881\n","Estimator 244/1000, Train metric: 0.1827, Validation metric: 0.1879\n","Estimator 245/1000, Train metric: 0.1826, Validation metric: 0.1879\n","Estimator 246/1000, Train metric: 0.1820, Validation metric: 0.1874\n","Estimator 247/1000, Train metric: 0.1819, Validation metric: 0.1874\n","Estimator 248/1000, Train metric: 0.1816, Validation metric: 0.1870\n","Estimator 249/1000, Train metric: 0.1813, Validation metric: 0.1868\n","Estimator 250/1000, Train metric: 0.1808, Validation metric: 0.1863\n","Estimator 251/1000, Train metric: 0.1803, Validation metric: 0.1858\n","Estimator 252/1000, Train metric: 0.1799, Validation metric: 0.1855\n","Estimator 253/1000, Train metric: 0.1794, Validation metric: 0.1851\n","Estimator 254/1000, Train metric: 0.1790, Validation metric: 0.1846\n","Estimator 255/1000, Train metric: 0.1786, Validation metric: 0.1843\n","Estimator 256/1000, Train metric: 0.1783, Validation metric: 0.1840\n","Estimator 257/1000, Train metric: 0.1782, Validation metric: 0.1839\n","Estimator 258/1000, Train metric: 0.1782, Validation metric: 0.1839\n","Estimator 259/1000, Train metric: 0.1778, Validation metric: 0.1835\n","Estimator 260/1000, Train metric: 0.1777, Validation metric: 0.1834\n","Estimator 261/1000, Train metric: 0.1774, Validation metric: 0.1832\n","Estimator 262/1000, Train metric: 0.1774, Validation metric: 0.1831\n","Estimator 263/1000, Train metric: 0.1769, Validation metric: 0.1827\n","Estimator 264/1000, Train metric: 0.1766, Validation metric: 0.1824\n","Estimator 265/1000, Train metric: 0.1762, Validation metric: 0.1821\n","Estimator 266/1000, Train metric: 0.1760, Validation metric: 0.1819\n","Estimator 267/1000, Train metric: 0.1759, Validation metric: 0.1818\n","Estimator 268/1000, Train metric: 0.1758, Validation metric: 0.1818\n","Estimator 269/1000, Train metric: 0.1757, Validation metric: 0.1817\n","Estimator 270/1000, Train metric: 0.1756, Validation metric: 0.1816\n","Estimator 271/1000, Train metric: 0.1752, Validation metric: 0.1813\n","Estimator 272/1000, Train metric: 0.1749, Validation metric: 0.1810\n","Estimator 273/1000, Train metric: 0.1744, Validation metric: 0.1805\n","Estimator 274/1000, Train metric: 0.1742, Validation metric: 0.1803\n","Estimator 275/1000, Train metric: 0.1736, Validation metric: 0.1797\n","Estimator 276/1000, Train metric: 0.1734, Validation metric: 0.1796\n","Estimator 277/1000, Train metric: 0.1732, Validation metric: 0.1793\n","Estimator 278/1000, Train metric: 0.1730, Validation metric: 0.1792\n","Estimator 279/1000, Train metric: 0.1725, Validation metric: 0.1787\n","Estimator 280/1000, Train metric: 0.1724, Validation metric: 0.1787\n","Estimator 281/1000, Train metric: 0.1723, Validation metric: 0.1787\n","Estimator 282/1000, Train metric: 0.1721, Validation metric: 0.1785\n","Estimator 283/1000, Train metric: 0.1717, Validation metric: 0.1782\n","Estimator 284/1000, Train metric: 0.1715, Validation metric: 0.1780\n","Estimator 285/1000, Train metric: 0.1714, Validation metric: 0.1778\n","Estimator 286/1000, Train metric: 0.1710, Validation metric: 0.1774\n","Estimator 287/1000, Train metric: 0.1707, Validation metric: 0.1771\n","Estimator 288/1000, Train metric: 0.1703, Validation metric: 0.1768\n","Estimator 289/1000, Train metric: 0.1701, Validation metric: 0.1766\n","Estimator 290/1000, Train metric: 0.1698, Validation metric: 0.1763\n","Estimator 291/1000, Train metric: 0.1694, Validation metric: 0.1760\n","Estimator 292/1000, Train metric: 0.1691, Validation metric: 0.1757\n","Estimator 293/1000, Train metric: 0.1688, Validation metric: 0.1754\n","Estimator 294/1000, Train metric: 0.1686, Validation metric: 0.1752\n","Estimator 295/1000, Train metric: 0.1685, Validation metric: 0.1751\n","Estimator 296/1000, Train metric: 0.1683, Validation metric: 0.1749\n","Estimator 297/1000, Train metric: 0.1680, Validation metric: 0.1747\n","Estimator 298/1000, Train metric: 0.1679, Validation metric: 0.1749\n","Estimator 299/1000, Train metric: 0.1678, Validation metric: 0.1750\n","Estimator 300/1000, Train metric: 0.1675, Validation metric: 0.1746\n","Estimator 301/1000, Train metric: 0.1673, Validation metric: 0.1745\n","Estimator 302/1000, Train metric: 0.1673, Validation metric: 0.1745\n","Estimator 303/1000, Train metric: 0.1671, Validation metric: 0.1742\n","Estimator 304/1000, Train metric: 0.1668, Validation metric: 0.1740\n","Estimator 305/1000, Train metric: 0.1665, Validation metric: 0.1737\n","Estimator 306/1000, Train metric: 0.1662, Validation metric: 0.1734\n","Estimator 307/1000, Train metric: 0.1656, Validation metric: 0.1730\n","Estimator 308/1000, Train metric: 0.1653, Validation metric: 0.1726\n","Estimator 309/1000, Train metric: 0.1651, Validation metric: 0.1725\n","Estimator 310/1000, Train metric: 0.1646, Validation metric: 0.1720\n","Estimator 311/1000, Train metric: 0.1644, Validation metric: 0.1718\n","Estimator 312/1000, Train metric: 0.1640, Validation metric: 0.1715\n","Estimator 313/1000, Train metric: 0.1640, Validation metric: 0.1714\n","Estimator 314/1000, Train metric: 0.1638, Validation metric: 0.1713\n","Estimator 315/1000, Train metric: 0.1636, Validation metric: 0.1711\n","Estimator 316/1000, Train metric: 0.1634, Validation metric: 0.1709\n","Estimator 317/1000, Train metric: 0.1633, Validation metric: 0.1708\n","Estimator 318/1000, Train metric: 0.1632, Validation metric: 0.1707\n","Estimator 319/1000, Train metric: 0.1628, Validation metric: 0.1704\n","Estimator 320/1000, Train metric: 0.1627, Validation metric: 0.1703\n","Estimator 321/1000, Train metric: 0.1626, Validation metric: 0.1702\n","Estimator 322/1000, Train metric: 0.1622, Validation metric: 0.1699\n","Estimator 323/1000, Train metric: 0.1618, Validation metric: 0.1696\n","Estimator 324/1000, Train metric: 0.1615, Validation metric: 0.1693\n","Estimator 325/1000, Train metric: 0.1614, Validation metric: 0.1693\n","Estimator 326/1000, Train metric: 0.1614, Validation metric: 0.1692\n","Estimator 327/1000, Train metric: 0.1613, Validation metric: 0.1692\n","Estimator 328/1000, Train metric: 0.1610, Validation metric: 0.1689\n","Estimator 329/1000, Train metric: 0.1608, Validation metric: 0.1687\n","Estimator 330/1000, Train metric: 0.1607, Validation metric: 0.1687\n","Estimator 331/1000, Train metric: 0.1606, Validation metric: 0.1687\n","Estimator 332/1000, Train metric: 0.1603, Validation metric: 0.1684\n","Estimator 333/1000, Train metric: 0.1602, Validation metric: 0.1682\n","Estimator 334/1000, Train metric: 0.1600, Validation metric: 0.1679\n","Estimator 335/1000, Train metric: 0.1598, Validation metric: 0.1677\n","Estimator 336/1000, Train metric: 0.1597, Validation metric: 0.1676\n","Estimator 337/1000, Train metric: 0.1596, Validation metric: 0.1675\n","Estimator 338/1000, Train metric: 0.1595, Validation metric: 0.1675\n","Estimator 339/1000, Train metric: 0.1594, Validation metric: 0.1675\n","Estimator 340/1000, Train metric: 0.1593, Validation metric: 0.1674\n","Estimator 341/1000, Train metric: 0.1590, Validation metric: 0.1672\n","Estimator 342/1000, Train metric: 0.1588, Validation metric: 0.1669\n","Estimator 343/1000, Train metric: 0.1586, Validation metric: 0.1668\n","Estimator 344/1000, Train metric: 0.1585, Validation metric: 0.1666\n","Estimator 345/1000, Train metric: 0.1583, Validation metric: 0.1666\n","Estimator 346/1000, Train metric: 0.1580, Validation metric: 0.1664\n","Estimator 347/1000, Train metric: 0.1580, Validation metric: 0.1663\n","Estimator 348/1000, Train metric: 0.1579, Validation metric: 0.1663\n","Estimator 349/1000, Train metric: 0.1579, Validation metric: 0.1663\n","Estimator 350/1000, Train metric: 0.1577, Validation metric: 0.1661\n","Estimator 351/1000, Train metric: 0.1575, Validation metric: 0.1659\n","Estimator 352/1000, Train metric: 0.1572, Validation metric: 0.1658\n","Estimator 353/1000, Train metric: 0.1570, Validation metric: 0.1655\n","Estimator 354/1000, Train metric: 0.1566, Validation metric: 0.1651\n","Estimator 355/1000, Train metric: 0.1563, Validation metric: 0.1648\n","Estimator 356/1000, Train metric: 0.1562, Validation metric: 0.1648\n","Estimator 357/1000, Train metric: 0.1560, Validation metric: 0.1646\n","Estimator 358/1000, Train metric: 0.1559, Validation metric: 0.1645\n","Estimator 359/1000, Train metric: 0.1558, Validation metric: 0.1644\n","Estimator 360/1000, Train metric: 0.1556, Validation metric: 0.1642\n","Estimator 361/1000, Train metric: 0.1554, Validation metric: 0.1642\n","Estimator 362/1000, Train metric: 0.1551, Validation metric: 0.1639\n","Estimator 363/1000, Train metric: 0.1550, Validation metric: 0.1638\n","Estimator 364/1000, Train metric: 0.1549, Validation metric: 0.1639\n","Estimator 365/1000, Train metric: 0.1549, Validation metric: 0.1639\n","Estimator 366/1000, Train metric: 0.1547, Validation metric: 0.1637\n","Estimator 367/1000, Train metric: 0.1546, Validation metric: 0.1636\n","Estimator 368/1000, Train metric: 0.1\n","\n","*** WARNING: max output size exceeded, skipping output. ***\n","\n","ain metric: 0.1413\n","Estimator 484/1000, Train metric: 0.1412\n","Estimator 485/1000, Train metric: 0.1412\n","Estimator 486/1000, Train metric: 0.1411\n","Estimator 487/1000, Train metric: 0.1411\n","Estimator 488/1000, Train metric: 0.1411\n","Estimator 489/1000, Train metric: 0.1410\n","Estimator 490/1000, Train metric: 0.1409\n","Estimator 491/1000, Train metric: 0.1408\n","Estimator 492/1000, Train metric: 0.1407\n","Estimator 493/1000, Train metric: 0.1406\n","Estimator 494/1000, Train metric: 0.1406\n","Estimator 495/1000, Train metric: 0.1405\n","Estimator 496/1000, Train metric: 0.1404\n","Estimator 497/1000, Train metric: 0.1404\n","Estimator 498/1000, Train metric: 0.1403\n","Estimator 499/1000, Train metric: 0.1403\n","Estimator 500/1000, Train metric: 0.1402\n","Estimator 501/1000, Train metric: 0.1401\n","Estimator 502/1000, Train metric: 0.1400\n","Estimator 503/1000, Train metric: 0.1399\n","Estimator 504/1000, Train metric: 0.1399\n","Estimator 505/1000, Train metric: 0.1397\n","Estimator 506/1000, Train metric: 0.1396\n","Estimator 507/1000, Train metric: 0.1394\n","Estimator 508/1000, Train metric: 0.1394\n","Estimator 509/1000, Train metric: 0.1393\n","Estimator 510/1000, Train metric: 0.1393\n","Estimator 511/1000, Train metric: 0.1392\n","Estimator 512/1000, Train metric: 0.1391\n","Estimator 513/1000, Train metric: 0.1390\n","Estimator 514/1000, Train metric: 0.1389\n","Estimator 515/1000, Train metric: 0.1388\n","Estimator 516/1000, Train metric: 0.1387\n","Estimator 517/1000, Train metric: 0.1386\n","Estimator 518/1000, Train metric: 0.1385\n","Estimator 519/1000, Train metric: 0.1385\n","Estimator 520/1000, Train metric: 0.1384\n","Estimator 521/1000, Train metric: 0.1383\n","Estimator 522/1000, Train metric: 0.1382\n","Estimator 523/1000, Train metric: 0.1382\n","Estimator 524/1000, Train metric: 0.1381\n","Estimator 525/1000, Train metric: 0.1381\n","Estimator 526/1000, Train metric: 0.1379\n","Estimator 527/1000, Train metric: 0.1378\n","Estimator 528/1000, Train metric: 0.1377\n","Estimator 529/1000, Train metric: 0.1376\n","Estimator 530/1000, Train metric: 0.1375\n","Estimator 531/1000, Train metric: 0.1374\n","Estimator 532/1000, Train metric: 0.1374\n","Estimator 533/1000, Train metric: 0.1373\n","Estimator 534/1000, Train metric: 0.1372\n","Estimator 535/1000, Train metric: 0.1371\n","Estimator 536/1000, Train metric: 0.1370\n","Estimator 537/1000, Train metric: 0.1370\n","Estimator 538/1000, Train metric: 0.1369\n","Estimator 539/1000, Train metric: 0.1369\n","Estimator 540/1000, Train metric: 0.1367\n","Estimator 541/1000, Train metric: 0.1367\n","Estimator 542/1000, Train metric: 0.1366\n","Estimator 543/1000, Train metric: 0.1366\n","Estimator 544/1000, Train metric: 0.1364\n","Estimator 545/1000, Train metric: 0.1364\n","Estimator 546/1000, Train metric: 0.1363\n","Estimator 547/1000, Train metric: 0.1363\n","Estimator 548/1000, Train metric: 0.1362\n","Estimator 549/1000, Train metric: 0.1362\n","Estimator 550/1000, Train metric: 0.1361\n","Estimator 551/1000, Train metric: 0.1361\n","Estimator 552/1000, Train metric: 0.1360\n","Estimator 553/1000, Train metric: 0.1359\n","Estimator 554/1000, Train metric: 0.1358\n","Estimator 555/1000, Train metric: 0.1357\n","Estimator 556/1000, Train metric: 0.1356\n","Estimator 557/1000, Train metric: 0.1356\n","Estimator 558/1000, Train metric: 0.1355\n","Estimator 559/1000, Train metric: 0.1354\n","Estimator 560/1000, Train metric: 0.1353\n","Estimator 561/1000, Train metric: 0.1352\n","Estimator 562/1000, Train metric: 0.1351\n","Estimator 563/1000, Train metric: 0.1350\n","Estimator 564/1000, Train metric: 0.1350\n","Estimator 565/1000, Train metric: 0.1349\n","Estimator 566/1000, Train metric: 0.1348\n","Estimator 567/1000, Train metric: 0.1348\n","Estimator 568/1000, Train metric: 0.1348\n","Estimator 569/1000, Train metric: 0.1346\n","Estimator 570/1000, Train metric: 0.1346\n","Estimator 571/1000, Train metric: 0.1345\n","Estimator 572/1000, Train metric: 0.1344\n","Estimator 573/1000, Train metric: 0.1343\n","Estimator 574/1000, Train metric: 0.1343\n","Estimator 575/1000, Train metric: 0.1342\n","Estimator 576/1000, Train metric: 0.1341\n","Estimator 577/1000, Train metric: 0.1340\n","Estimator 578/1000, Train metric: 0.1340\n","Estimator 579/1000, Train metric: 0.1339\n","Estimator 580/1000, Train metric: 0.1338\n","Estimator 581/1000, Train metric: 0.1338\n","Estimator 582/1000, Train metric: 0.1338\n","Estimator 583/1000, Train metric: 0.1338\n","Estimator 584/1000, Train metric: 0.1337\n","Estimator 585/1000, Train metric: 0.1335\n","Estimator 586/1000, Train metric: 0.1335\n","Estimator 587/1000, Train metric: 0.1334\n","Estimator 588/1000, Train metric: 0.1334\n","Estimator 589/1000, Train metric: 0.1333\n","Estimator 590/1000, Train metric: 0.1333\n","Estimator 591/1000, Train metric: 0.1331\n","Estimator 592/1000, Train metric: 0.1331\n","Estimator 593/1000, Train metric: 0.1330\n","Estimator 594/1000, Train metric: 0.1330\n","Estimator 595/1000, Train metric: 0.1329\n","Estimator 596/1000, Train metric: 0.1329\n","Estimator 597/1000, Train metric: 0.1329\n","Estimator 598/1000, Train metric: 0.1328\n","Estimator 599/1000, Train metric: 0.1327\n","Estimator 600/1000, Train metric: 0.1327\n","Estimator 601/1000, Train metric: 0.1327\n","Estimator 602/1000, Train metric: 0.1326\n","Estimator 603/1000, Train metric: 0.1326\n","Estimator 604/1000, Train metric: 0.1326\n","Estimator 605/1000, Train metric: 0.1325\n","Estimator 606/1000, Train metric: 0.1325\n","Estimator 607/1000, Train metric: 0.1324\n","Estimator 608/1000, Train metric: 0.1323\n","Estimator 609/1000, Train metric: 0.1323\n","Estimator 610/1000, Train metric: 0.1322\n","Estimator 611/1000, Train metric: 0.1321\n","Estimator 612/1000, Train metric: 0.1320\n","Estimator 613/1000, Train metric: 0.1319\n","Estimator 614/1000, Train metric: 0.1319\n","Estimator 615/1000, Train metric: 0.1318\n","Estimator 616/1000, Train metric: 0.1318\n","Estimator 617/1000, Train metric: 0.1318\n","Estimator 618/1000, Train metric: 0.1317\n","Estimator 619/1000, Train metric: 0.1317\n","Estimator 620/1000, Train metric: 0.1316\n","Estimator 621/1000, Train metric: 0.1315\n","Estimator 622/1000, Train metric: 0.1315\n","Estimator 623/1000, Train metric: 0.1314\n","Estimator 624/1000, Train metric: 0.1314\n","Estimator 625/1000, Train metric: 0.1313\n","Estimator 626/1000, Train metric: 0.1312\n","Estimator 627/1000, Train metric: 0.1311\n","Estimator 628/1000, Train metric: 0.1311\n","Estimator 629/1000, Train metric: 0.1311\n","Estimator 630/1000, Train metric: 0.1310\n","Estimator 631/1000, Train metric: 0.1310\n","Estimator 632/1000, Train metric: 0.1310\n","Estimator 633/1000, Train metric: 0.1309\n","Estimator 634/1000, Train metric: 0.1309\n","Estimator 635/1000, Train metric: 0.1308\n","Estimator 636/1000, Train metric: 0.1308\n","Estimator 637/1000, Train metric: 0.1307\n","Estimator 638/1000, Train metric: 0.1307\n","Estimator 639/1000, Train metric: 0.1306\n","Estimator 640/1000, Train metric: 0.1306\n","Estimator 641/1000, Train metric: 0.1305\n","Estimator 642/1000, Train metric: 0.1304\n","Estimator 643/1000, Train metric: 0.1303\n","Estimator 644/1000, Train metric: 0.1303\n","Estimator 645/1000, Train metric: 0.1302\n","Estimator 646/1000, Train metric: 0.1302\n","Estimator 647/1000, Train metric: 0.1301\n","Estimator 648/1000, Train metric: 0.1301\n","Estimator 649/1000, Train metric: 0.1300\n","Estimator 650/1000, Train metric: 0.1299\n","Estimator 651/1000, Train metric: 0.1299\n","Estimator 652/1000, Train metric: 0.1299\n","Estimator 653/1000, Train metric: 0.1299\n","Estimator 654/1000, Train metric: 0.1298\n","Estimator 655/1000, Train metric: 0.1297\n","Estimator 656/1000, Train metric: 0.1297\n","Estimator 657/1000, Train metric: 0.1296\n","Estimator 658/1000, Train metric: 0.1296\n","Estimator 659/1000, Train metric: 0.1295\n","Estimator 660/1000, Train metric: 0.1295\n","Estimator 661/1000, Train metric: 0.1294\n","Estimator 662/1000, Train metric: 0.1293\n","Estimator 663/1000, Train metric: 0.1292\n","Estimator 664/1000, Train metric: 0.1290\n","Estimator 665/1000, Train metric: 0.1290\n","Estimator 666/1000, Train metric: 0.1289\n","Estimator 667/1000, Train metric: 0.1289\n","Estimator 668/1000, Train metric: 0.1288\n","Estimator 669/1000, Train metric: 0.1288\n","Estimator 670/1000, Train metric: 0.1287\n","Estimator 671/1000, Train metric: 0.1287\n","Estimator 672/1000, Train metric: 0.1287\n","Estimator 673/1000, Train metric: 0.1286\n","Estimator 674/1000, Train metric: 0.1286\n","Estimator 675/1000, Train metric: 0.1286\n","Estimator 676/1000, Train metric: 0.1285\n","Estimator 677/1000, Train metric: 0.1285\n","Estimator 678/1000, Train metric: 0.1284\n","Estimator 679/1000, Train metric: 0.1283\n","Estimator 680/1000, Train metric: 0.1283\n","Estimator 681/1000, Train metric: 0.1283\n","Estimator 682/1000, Train metric: 0.1282\n","Estimator 683/1000, Train metric: 0.1281\n","Estimator 684/1000, Train metric: 0.1281\n","Estimator 685/1000, Train metric: 0.1281\n","Estimator 686/1000, Train metric: 0.1280\n","Estimator 687/1000, Train metric: 0.1279\n","Estimator 688/1000, Train metric: 0.1279\n","Estimator 689/1000, Train metric: 0.1278\n","Estimator 690/1000, Train metric: 0.1278\n","Estimator 691/1000, Train metric: 0.1277\n","Estimator 692/1000, Train metric: 0.1277\n","Estimator 693/1000, Train metric: 0.1277\n","Estimator 694/1000, Train metric: 0.1276\n","Estimator 695/1000, Train metric: 0.1276\n","Estimator 696/1000, Train metric: 0.1276\n","Estimator 697/1000, Train metric: 0.1275\n","Estimator 698/1000, Train metric: 0.1275\n","Estimator 699/1000, Train metric: 0.1275\n","Estimator 700/1000, Train metric: 0.1274\n","Estimator 701/1000, Train metric: 0.1273\n","Estimator 702/1000, Train metric: 0.1273\n","Estimator 703/1000, Train metric: 0.1273\n","Estimator 704/1000, Train metric: 0.1272\n","Estimator 705/1000, Train metric: 0.1272\n","Estimator 706/1000, Train metric: 0.1271\n","Estimator 707/1000, Train metric: 0.1270\n","Estimator 708/1000, Train metric: 0.1269\n","Estimator 709/1000, Train metric: 0.1269\n","Estimator 710/1000, Train metric: 0.1269\n","Estimator 711/1000, Train metric: 0.1268\n","Estimator 712/1000, Train metric: 0.1268\n","Estimator 713/1000, Train metric: 0.1268\n","Estimator 714/1000, Train metric: 0.1267\n","Estimator 715/1000, Train metric: 0.1267\n","Estimator 716/1000, Train metric: 0.1266\n","Estimator 717/1000, Train metric: 0.1265\n","Estimator 718/1000, Train metric: 0.1265\n","Estimator 719/1000, Train metric: 0.1265\n","Estimator 720/1000, Train metric: 0.1264\n","Estimator 721/1000, Train metric: 0.1264\n","Estimator 722/1000, Train metric: 0.1263\n","Estimator 723/1000, Train metric: 0.1263\n","Estimator 724/1000, Train metric: 0.1263\n","Estimator 725/1000, Train metric: 0.1262\n","Estimator 726/1000, Train metric: 0.1262\n","Estimator 727/1000, Train metric: 0.1262\n","Estimator 728/1000, Train metric: 0.1261\n","Estimator 729/1000, Train metric: 0.1261\n","Estimator 730/1000, Train metric: 0.1261\n","Estimator 731/1000, Train metric: 0.1260\n","Estimator 732/1000, Train metric: 0.1259\n","Estimator 733/1000, Train metric: 0.1259\n","Estimator 734/1000, Train metric: 0.1258\n","Estimator 735/1000, Train metric: 0.1258\n","Estimator 736/1000, Train metric: 0.1258\n","Estimator 737/1000, Train metric: 0.1257\n","Estimator 738/1000, Train metric: 0.1257\n","Estimator 739/1000, Train metric: 0.1256\n","Estimator 740/1000, Train metric: 0.1256\n","Estimator 741/1000, Train metric: 0.1255\n","Estimator 742/1000, Train metric: 0.1255\n","Estimator 743/1000, Train metric: 0.1254\n","Estimator 744/1000, Train metric: 0.1254\n","Estimator 745/1000, Train metric: 0.1254\n","Estimator 746/1000, Train metric: 0.1253\n","Estimator 747/1000, Train metric: 0.1253\n","Estimator 748/1000, Train metric: 0.1252\n","Estimator 749/1000, Train metric: 0.1252\n","Estimator 750/1000, Train metric: 0.1252\n","Estimator 751/1000, Train metric: 0.1252\n","Estimator 752/1000, Train metric: 0.1252\n","Estimator 753/1000, Train metric: 0.1251\n","Estimator 754/1000, Train metric: 0.1251\n","Estimator 755/1000, Train metric: 0.1250\n","Estimator 756/1000, Train metric: 0.1250\n","Estimator 757/1000, Train metric: 0.1249\n","Estimator 758/1000, Train metric: 0.1249\n","Estimator 759/1000, Train metric: 0.1248\n","Estimator 760/1000, Train metric: 0.1248\n","Estimator 761/1000, Train metric: 0.1248\n","Estimator 762/1000, Train metric: 0.1248\n","Estimator 763/1000, Train metric: 0.1247\n","Estimator 764/1000, Train metric: 0.1247\n","Estimator 765/1000, Train metric: 0.1247\n","Estimator 766/1000, Train metric: 0.1246\n","Estimator 767/1000, Train metric: 0.1246\n","Estimator 768/1000, Train metric: 0.1246\n","Estimator 769/1000, Train metric: 0.1245\n","Estimator 770/1000, Train metric: 0.1245\n","Estimator 771/1000, Train metric: 0.1245\n","Estimator 772/1000, Train metric: 0.1245\n","Estimator 773/1000, Train metric: 0.1244\n","Estimator 774/1000, Train metric: 0.1244\n","Estimator 775/1000, Train metric: 0.1244\n","Estimator 776/1000, Train metric: 0.1243\n","Estimator 777/1000, Train metric: 0.1242\n","Estimator 778/1000, Train metric: 0.1242\n","Estimator 779/1000, Train metric: 0.1242\n","Estimator 780/1000, Train metric: 0.1241\n","Estimator 781/1000, Train metric: 0.1241\n","Estimator 782/1000, Train metric: 0.1241\n","Estimator 783/1000, Train metric: 0.1241\n","Estimator 784/1000, Train metric: 0.1240\n","Estimator 785/1000, Train metric: 0.1240\n","Estimator 786/1000, Train metric: 0.1240\n","Estimator 787/1000, Train metric: 0.1239\n","Estimator 788/1000, Train metric: 0.1239\n","Estimator 789/1000, Train metric: 0.1238\n","Estimator 790/1000, Train metric: 0.1238\n","Estimator 791/1000, Train metric: 0.1237\n","Estimator 792/1000, Train metric: 0.1237\n","Estimator 793/1000, Train metric: 0.1237\n","Estimator 794/1000, Train metric: 0.1236\n","Estimator 795/1000, Train metric: 0.1236\n","Estimator 796/1000, Train metric: 0.1235\n","Estimator 797/1000, Train metric: 0.1235\n","Estimator 798/1000, Train metric: 0.1234\n","Estimator 799/1000, Train metric: 0.1234\n","Estimator 800/1000, Train metric: 0.1233\n","Estimator 801/1000, Train metric: 0.1233\n","Estimator 802/1000, Train metric: 0.1232\n","Estimator 803/1000, Train metric: 0.1232\n","Estimator 804/1000, Train metric: 0.1232\n","Estimator 805/1000, Train metric: 0.1231\n","Estimator 806/1000, Train metric: 0.1231\n","Estimator 807/1000, Train metric: 0.1230\n","Estimator 808/1000, Train metric: 0.1230\n","Estimator 809/1000, Train metric: 0.1230\n","Estimator 810/1000, Train metric: 0.1230\n","Estimator 811/1000, Train metric: 0.1229\n","Estimator 812/1000, Train metric: 0.1229\n","Estimator 813/1000, Train metric: 0.1229\n","Estimator 814/1000, Train metric: 0.1228\n","Estimator 815/1000, Train metric: 0.1228\n","Estimator 816/1000, Train metric: 0.1227\n","Estimator 817/1000, Train metric: 0.1227\n","Estimator 818/1000, Train metric: 0.1226\n","Estimator 819/1000, Train metric: 0.1226\n","Estimator 820/1000, Train metric: 0.1226\n","Estimator 821/1000, Train metric: 0.1225\n","Estimator 822/1000, Train metric: 0.1224\n","Estimator 823/1000, Train metric: 0.1224\n","Estimator 824/1000, Train metric: 0.1224\n","Estimator 825/1000, Train metric: 0.1223\n","Estimator 826/1000, Train metric: 0.1223\n","Estimator 827/1000, Train metric: 0.1223\n","Estimator 828/1000, Train metric: 0.1222\n","Estimator 829/1000, Train metric: 0.1222\n","Estimator 830/1000, Train metric: 0.1222\n","Estimator 831/1000, Train metric: 0.1222\n","Estimator 832/1000, Train metric: 0.1221\n","Estimator 833/1000, Train metric: 0.1221\n","Estimator 834/1000, Train metric: 0.1220\n","Estimator 835/1000, Train metric: 0.1220\n","Estimator 836/1000, Train metric: 0.1220\n","Estimator 837/1000, Train metric: 0.1220\n","Estimator 838/1000, Train metric: 0.1219\n","Estimator 839/1000, Train metric: 0.1218\n","Estimator 840/1000, Train metric: 0.1218\n","Estimator 841/1000, Train metric: 0.1218\n","Estimator 842/1000, Train metric: 0.1218\n","Estimator 843/1000, Train metric: 0.1218\n","Estimator 844/1000, Train metric: 0.1217\n","Estimator 845/1000, Train metric: 0.1217\n","Estimator 846/1000, Train metric: 0.1216\n","Estimator 847/1000, Train metric: 0.1216\n","Estimator 848/1000, Train metric: 0.1216\n","Estimator 849/1000, Train metric: 0.1216\n","Estimator 850/1000, Train metric: 0.1215\n","Estimator 851/1000, Train metric: 0.1215\n","Estimator 852/1000, Train metric: 0.1214\n","Estimator 853/1000, Train metric: 0.1214\n","Estimator 854/1000, Train metric: 0.1214\n","Estimator 855/1000, Train metric: 0.1213\n","Estimator 856/1000, Train metric: 0.1213\n","Estimator 857/1000, Train metric: 0.1212\n","Estimator 858/1000, Train metric: 0.1211\n","Estimator 859/1000, Train metric: 0.1211\n","Estimator 860/1000, Train metric: 0.1210\n","Estimator 861/1000, Train metric: 0.1210\n","Estimator 862/1000, Train metric: 0.1210\n","Estimator 863/1000, Train metric: 0.1209\n","Estimator 864/1000, Train metric: 0.1209\n","Estimator 865/1000, Train metric: 0.1208\n","Estimator 866/1000, Train metric: 0.1208\n","Estimator 867/1000, Train metric: 0.1208\n","Estimator 868/1000, Train metric: 0.1208\n","Estimator 869/1000, Train metric: 0.1207\n","Estimator 870/1000, Train metric: 0.1207\n","Estimator 871/1000, Train metric: 0.1207\n","Estimator 872/1000, Train metric: 0.1206\n","Estimator 873/1000, Train metric: 0.1206\n","Estimator 874/1000, Train metric: 0.1205\n","Estimator 875/1000, Train metric: 0.1205\n","Estimator 876/1000, Train metric: 0.1205\n","Estimator 877/1000, Train metric: 0.1205\n","Estimator 878/1000, Train metric: 0.1204\n","Estimator 879/1000, Train metric: 0.1204\n","Estimator 880/1000, Train metric: 0.1204\n","Estimator 881/1000, Train metric: 0.1203\n","Estimator 882/1000, Train metric: 0.1202\n","Estimator 883/1000, Train metric: 0.1202\n","Estimator 884/1000, Train metric: 0.1202\n","Estimator 885/1000, Train metric: 0.1202\n","Estimator 886/1000, Train metric: 0.1201\n","Estimator 887/1000, Train metric: 0.1201\n","Estimator 888/1000, Train metric: 0.1201\n","Estimator 889/1000, Train metric: 0.1200\n","Estimator 890/1000, Train metric: 0.1200\n","Estimator 891/1000, Train metric: 0.1200\n","Estimator 892/1000, Train metric: 0.1199\n","Estimator 893/1000, Train metric: 0.1199\n","Estimator 894/1000, Train metric: 0.1199\n","Estimator 895/1000, Train metric: 0.1198\n","Estimator 896/1000, Train metric: 0.1198\n","Estimator 897/1000, Train metric: 0.1198\n","Estimator 898/1000, Train metric: 0.1197\n","Estimator 899/1000, Train metric: 0.1197\n","Estimator 900/1000, Train metric: 0.1197\n","Estimator 901/1000, Train metric: 0.1196\n","Estimator 902/1000, Train metric: 0.1196\n","Estimator 903/1000, Train metric: 0.1195\n","Estimator 904/1000, Train metric: 0.1195\n","Estimator 905/1000, Train metric: 0.1195\n","Estimator 906/1000, Train metric: 0.1195\n","Estimator 907/1000, Train metric: 0.1194\n","Estimator 908/1000, Train metric: 0.1194\n","Estimator 909/1000, Train metric: 0.1194\n","Estimator 910/1000, Train metric: 0.1194\n","Estimator 911/1000, Train metric: 0.1194\n","Estimator 912/1000, Train metric: 0.1193\n","Estimator 913/1000, Train metric: 0.1193\n","Estimator 914/1000, Train metric: 0.1192\n","Estimator 915/1000, Train metric: 0.1192\n","Estimator 916/1000, Train metric: 0.1192\n","Estimator 917/1000, Train metric: 0.1192\n","Estimator 918/1000, Train metric: 0.1191\n","Estimator 919/1000, Train metric: 0.1191\n","Estimator 920/1000, Train metric: 0.1191\n","Estimator 921/1000, Train metric: 0.1190\n","Estimator 922/1000, Train metric: 0.1190\n","Estimator 923/1000, Train metric: 0.1190\n","Estimator 924/1000, Train metric: 0.1190\n","Estimator 925/1000, Train metric: 0.1190\n","Estimator 926/1000, Train metric: 0.1189\n","Estimator 927/1000, Train metric: 0.1189\n","Estimator 928/1000, Train metric: 0.1189\n","Estimator 929/1000, Train metric: 0.1189\n","Estimator 930/1000, Train metric: 0.1188\n","Estimator 931/1000, Train metric: 0.1188\n","Estimator 932/1000, Train metric: 0.1187\n","Estimator 933/1000, Train metric: 0.1187\n","Estimator 934/1000, Train metric: 0.1186\n","Estimator 935/1000, Train metric: 0.1186\n","Estimator 936/1000, Train metric: 0.1186\n","Estimator 937/1000, Train metric: 0.1186\n","Estimator 938/1000, Train metric: 0.1186\n","Estimator 939/1000, Train metric: 0.1185\n","Estimator 940/1000, Train metric: 0.1185\n","Estimator 941/1000, Train metric: 0.1185\n","Estimator 942/1000, Train metric: 0.1185\n","Estimator 943/1000, Train metric: 0.1184\n","Estimator 944/1000, Train metric: 0.1184\n","Estimator 945/1000, Train metric: 0.1184\n","Estimator 946/1000, Train metric: 0.1183\n","Estimator 947/1000, Train metric: 0.1183\n","Estimator 948/1000, Train metric: 0.1183\n","Estimator 949/1000, Train metric: 0.1183\n","Estimator 950/1000, Train metric: 0.1183\n","Estimator 951/1000, Train metric: 0.1182\n","Estimator 952/1000, Train metric: 0.1182\n","Estimator 953/1000, Train metric: 0.1182\n","Estimator 954/1000, Train metric: 0.1181\n","Estimator 955/1000, Train metric: 0.1181\n","Estimator 956/1000, Train metric: 0.1181\n","Estimator 957/1000, Train metric: 0.1181\n","Estimator 958/1000, Train metric: 0.1180\n","Estimator 959/1000, Train metric: 0.1180\n","Estimator 960/1000, Train metric: 0.1179\n","Estimator 961/1000, Train metric: 0.1179\n","Estimator 962/1000, Train metric: 0.1179\n","Estimator 963/1000, Train metric: 0.1178\n","Estimator 964/1000, Train metric: 0.1178\n","Estimator 965/1000, Train metric: 0.1178\n","Estimator 966/1000, Train metric: 0.1178\n","Estimator 967/1000, Train metric: 0.1178\n","Estimator 968/1000, Train metric: 0.1177\n","Estimator 969/1000, Train metric: 0.1177\n","Estimator 970/1000, Train metric: 0.1177\n","Estimator 971/1000, Train metric: 0.1177\n","Estimator 972/1000, Train metric: 0.1176\n","Estimator 973/1000, Train metric: 0.1176\n","Estimator 974/1000, Train metric: 0.1175\n","Estimator 975/1000, Train metric: 0.1175\n","Estimator 976/1000, Train metric: 0.1175\n","Estimator 977/1000, Train metric: 0.1174\n","Estimator 978/1000, Train metric: 0.1174\n","Estimator 979/1000, Train metric: 0.1174\n","Estimator 980/1000, Train metric: 0.1173\n","Estimator 981/1000, Train metric: 0.1173\n","Estimator 982/1000, Train metric: 0.1172\n","Estimator 983/1000, Train metric: 0.1172\n","Estimator 984/1000, Train metric: 0.1172\n","Estimator 985/1000, Train metric: 0.1172\n","Estimator 986/1000, Train metric: 0.1172\n","Estimator 987/1000, Train metric: 0.1171\n","Estimator 988/1000, Train metric: 0.1171\n","Estimator 989/1000, Train metric: 0.1171\n","Estimator 990/1000, Train metric: 0.1170\n","Estimator 991/1000, Train metric: 0.1170\n","Estimator 992/1000, Train metric: 0.1170\n","Estimator 993/1000, Train metric: 0.1169\n","Estimator 994/1000, Train metric: 0.1169\n","Estimator 995/1000, Train metric: 0.1169\n","Estimator 996/1000, Train metric: 0.1169\n","Estimator 997/1000, Train metric: 0.1168\n","Estimator 998/1000, Train metric: 0.1168\n","Estimator 999/1000, Train metric: 0.1168\n","Elapsed time for fitting PGBM model: 99.31 s\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/pgbm/pgbm.py:378: UserWarning: FALLBACK path has been taken inside: compileCudaFusionGroup. This is an indication that codegen Failed for some reason.\n","To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\n","To report the issue, try enable logging via setting the envvariable ` export PYTORCH_JIT_LOG_LEVEL=manager.cpp`\n"," (Triggered internally at ../torch/csrc/jit/codegen/cuda/manager.cpp:239.)\n","  mu = _predict_forest_mu(X_test_splits, self.nodes_idx,\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/pgbm/pgbm.py:378: UserWarning: FALLBACK path has been taken inside: runCudaFusionGroup. This is an indication that codegen Failed for some reason.\n","To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\n"," (Triggered internally at ../torch/csrc/jit/codegen/cuda/manager.cpp:331.)\n","  mu = _predict_forest_mu(X_test_splits, self.nodes_idx,\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/lidl_x_tum_uncertainty_estimation/uncertainty_estimation_models.py:552: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  nll_temp = torch.tensor([-dist[i].log_prob(torch.tensor(y_test[i])) for i in range(len(dist))])\n","path: predictions/pointpredictions_pgbm_with_best_dist.csv\n","dirname: predictions\n","filename: pointpredictions_pgbm_with_best_dist.csv\n","artifact_path: predictions\n","path: /tmp/tmpsq_sdqos/pointpredictions_pgbm_with_best_dist.csv\n","tmp_path: /tmp/tmpsq_sdqos/pointpredictions_pgbm_with_best_dist.csv\n","path: predictions/samples_pgbm_with_best_dist.csv\n","dirname: predictions\n","filename: samples_pgbm_with_best_dist.csv\n","artifact_path: predictions\n","path: /tmp/tmpdbjjqim1/samples_pgbm_with_best_dist.csv\n","tmp_path: /tmp/tmpdbjjqim1/samples_pgbm_with_best_dist.csv\n","path: predictions/distparams_pgbm_with_best_dist.csv\n","dirname: predictions\n","filename: distparams_pgbm_with_best_dist.csv\n","artifact_path: predictions\n","path: /tmp/tmpt6cm6hvr/distparams_pgbm_with_best_dist.csv\n","tmp_path: /tmp/tmpt6cm6hvr/distparams_pgbm_with_best_dist.csv\n","path: predictions/quantiles_pgbm_with_best_dist0.05.csv\n","dirname: predictions\n","filename: quantiles_pgbm_with_best_dist0.05.csv\n","artifact_path: predictions\n","path: /tmp/tmpsrx4fm99/quantiles_pgbm_with_best_dist0.05.csv\n","tmp_path: /tmp/tmpsrx4fm99/quantiles_pgbm_with_best_dist0.05.csv\n","path: predictions/quantiles_pgbm_with_best_dist0.1.csv\n","dirname: predictions\n","filename: quantiles_pgbm_with_best_dist0.1.csv\n","artifact_path: predictions\n","path: /tmp/tmpkos8ucjb/quantiles_pgbm_with_best_dist0.1.csv\n","tmp_path: /tmp/tmpkos8ucjb/quantiles_pgbm_with_best_dist0.1.csv\n","path: predictions/quantiles_pgbm_with_best_dist0.5.csv\n","dirname: predictions\n","filename: quantiles_pgbm_with_best_dist0.5.csv\n","artifact_path: predictions\n","path: /tmp/tmpg_cjj_pt/quantiles_pgbm_with_best_dist0.5.csv\n","tmp_path: /tmp/tmpg_cjj_pt/quantiles_pgbm_with_best_dist0.5.csv\n","path: predictions/quantiles_pgbm_with_best_dist0.9.csv\n","dirname: predictions\n","filename: quantiles_pgbm_with_best_dist0.9.csv\n","artifact_path: predictions\n","path: /tmp/tmpa7ma9dq6/quantiles_pgbm_with_best_dist0.9.csv\n","tmp_path: /tmp/tmpa7ma9dq6/quantiles_pgbm_with_best_dist0.9.csv\n","path: predictions/quantiles_pgbm_with_best_dist0.95.csv\n","dirname: predictions\n","filename: quantiles_pgbm_with_best_dist0.95.csv\n","artifact_path: predictions\n","path: /tmp/tmpr0ly2c4x/quantiles_pgbm_with_best_dist0.95.csv\n","tmp_path: /tmp/tmpr0ly2c4x/quantiles_pgbm_with_best_dist0.95.csv\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"f49c673699f74b959224c30627c22a7e\nTraining on GPU\nEstimator 0/1000, Train metric: 0.4159, Validation metric: 0.4036\nEstimator 1/1000, Train metric: 0.4064, Validation metric: 0.3944\nEstimator 2/1000, Train metric: 0.3979, Validation metric: 0.3861\nEstimator 3/1000, Train metric: 0.3906, Validation metric: 0.3795\nEstimator 4/1000, Train metric: 0.3868, Validation metric: 0.3764\nEstimator 5/1000, Train metric: 0.3807, Validation metric: 0.3705\nEstimator 6/1000, Train metric: 0.3765, Validation metric: 0.3670\nEstimator 7/1000, Train metric: 0.3710, Validation metric: 0.3618\nEstimator 8/1000, Train metric: 0.3656, Validation metric: 0.3570\nEstimator 9/1000, Train metric: 0.3606, Validation metric: 0.3524\nEstimator 10/1000, Train metric: 0.3566, Validation metric: 0.3489\nEstimator 11/1000, Train metric: 0.3530, Validation metric: 0.3461\nEstimator 12/1000, Train metric: 0.3502, Validation metric: 0.3444\nEstimator 13/1000, Train metric: 0.3486, Validation metric: 0.3436\nEstimator 14/1000, Train metric: 0.3454, Validation metric: 0.3403\nEstimator 15/1000, Train metric: 0.3428, Validation metric: 0.3380\nEstimator 16/1000, Train metric: 0.3408, Validation metric: 0.3365\nEstimator 17/1000, Train metric: 0.3380, Validation metric: 0.3340\nEstimator 18/1000, Train metric: 0.3367, Validation metric: 0.3330\nEstimator 19/1000, Train metric: 0.3355, Validation metric: 0.3323\nEstimator 20/1000, Train metric: 0.3332, Validation metric: 0.3303\nEstimator 21/1000, Train metric: 0.3314, Validation metric: 0.3286\nEstimator 22/1000, Train metric: 0.3300, Validation metric: 0.3272\nEstimator 23/1000, Train metric: 0.3284, Validation metric: 0.3264\nEstimator 24/1000, Train metric: 0.3267, Validation metric: 0.3251\nEstimator 25/1000, Train metric: 0.3253, Validation metric: 0.3238\nEstimator 26/1000, Train metric: 0.3243, Validation metric: 0.3225\nEstimator 27/1000, Train metric: 0.3234, Validation metric: 0.3214\nEstimator 28/1000, Train metric: 0.3220, Validation metric: 0.3202\nEstimator 29/1000, Train metric: 0.3212, Validation metric: 0.3192\nEstimator 30/1000, Train metric: 0.3194, Validation metric: 0.3176\nEstimator 31/1000, Train metric: 0.3180, Validation metric: 0.3167\nEstimator 32/1000, Train metric: 0.3174, Validation metric: 0.3161\nEstimator 33/1000, Train metric: 0.3157, Validation metric: 0.3143\nEstimator 34/1000, Train metric: 0.3143, Validation metric: 0.3129\nEstimator 35/1000, Train metric: 0.3133, Validation metric: 0.3119\nEstimator 36/1000, Train metric: 0.3109, Validation metric: 0.3095\nEstimator 37/1000, Train metric: 0.3105, Validation metric: 0.3089\nEstimator 38/1000, Train metric: 0.3084, Validation metric: 0.3070\nEstimator 39/1000, Train metric: 0.3071, Validation metric: 0.3056\nEstimator 40/1000, Train metric: 0.3060, Validation metric: 0.3053\nEstimator 41/1000, Train metric: 0.3055, Validation metric: 0.3049\nEstimator 42/1000, Train metric: 0.3043, Validation metric: 0.3038\nEstimator 43/1000, Train metric: 0.3026, Validation metric: 0.3022\nEstimator 44/1000, Train metric: 0.3023, Validation metric: 0.3019\nEstimator 45/1000, Train metric: 0.3004, Validation metric: 0.3000\nEstimator 46/1000, Train metric: 0.2992, Validation metric: 0.2990\nEstimator 47/1000, Train metric: 0.2988, Validation metric: 0.2980\nEstimator 48/1000, Train metric: 0.2983, Validation metric: 0.2975\nEstimator 49/1000, Train metric: 0.2976, Validation metric: 0.2968\nEstimator 50/1000, Train metric: 0.2973, Validation metric: 0.2966\nEstimator 51/1000, Train metric: 0.2964, Validation metric: 0.2956\nEstimator 52/1000, Train metric: 0.2950, Validation metric: 0.2945\nEstimator 53/1000, Train metric: 0.2920, Validation metric: 0.2915\nEstimator 54/1000, Train metric: 0.2910, Validation metric: 0.2905\nEstimator 55/1000, Train metric: 0.2900, Validation metric: 0.2896\nEstimator 56/1000, Train metric: 0.2897, Validation metric: 0.2894\nEstimator 57/1000, Train metric: 0.2874, Validation metric: 0.2871\nEstimator 58/1000, Train metric: 0.2868, Validation metric: 0.2867\nEstimator 59/1000, Train metric: 0.2854, Validation metric: 0.2855\nEstimator 60/1000, Train metric: 0.2850, Validation metric: 0.2848\nEstimator 61/1000, Train metric: 0.2839, Validation metric: 0.2837\nEstimator 62/1000, Train metric: 0.2825, Validation metric: 0.2824\nEstimator 63/1000, Train metric: 0.2822, Validation metric: 0.2822\nEstimator 64/1000, Train metric: 0.2818, Validation metric: 0.2819\nEstimator 65/1000, Train metric: 0.2808, Validation metric: 0.2808\nEstimator 66/1000, Train metric: 0.2801, Validation metric: 0.2801\nEstimator 67/1000, Train metric: 0.2795, Validation metric: 0.2796\nEstimator 68/1000, Train metric: 0.2792, Validation metric: 0.2792\nEstimator 69/1000, Train metric: 0.2775, Validation metric: 0.2778\nEstimator 70/1000, Train metric: 0.2772, Validation metric: 0.2774\nEstimator 71/1000, Train metric: 0.2761, Validation metric: 0.2763\nEstimator 72/1000, Train metric: 0.2744, Validation metric: 0.2747\nEstimator 73/1000, Train metric: 0.2742, Validation metric: 0.2751\nEstimator 74/1000, Train metric: 0.2733, Validation metric: 0.2744\nEstimator 75/1000, Train metric: 0.2731, Validation metric: 0.2743\nEstimator 76/1000, Train metric: 0.2725, Validation metric: 0.2737\nEstimator 77/1000, Train metric: 0.2712, Validation metric: 0.2725\nEstimator 78/1000, Train metric: 0.2711, Validation metric: 0.2723\nEstimator 79/1000, Train metric: 0.2708, Validation metric: 0.2720\nEstimator 80/1000, Train metric: 0.2703, Validation metric: 0.2714\nEstimator 81/1000, Train metric: 0.2675, Validation metric: 0.2687\nEstimator 82/1000, Train metric: 0.2663, Validation metric: 0.2675\nEstimator 83/1000, Train metric: 0.2650, Validation metric: 0.2663\nEstimator 84/1000, Train metric: 0.2628, Validation metric: 0.2641\nEstimator 85/1000, Train metric: 0.2624, Validation metric: 0.2637\nEstimator 86/1000, Train metric: 0.2607, Validation metric: 0.2620\nEstimator 87/1000, Train metric: 0.2596, Validation metric: 0.2609\nEstimator 88/1000, Train metric: 0.2593, Validation metric: 0.2604\nEstimator 89/1000, Train metric: 0.2584, Validation metric: 0.2595\nEstimator 90/1000, Train metric: 0.2570, Validation metric: 0.2582\nEstimator 91/1000, Train metric: 0.2567, Validation metric: 0.2573\nEstimator 92/1000, Train metric: 0.2553, Validation metric: 0.2561\nEstimator 93/1000, Train metric: 0.2541, Validation metric: 0.2549\nEstimator 94/1000, Train metric: 0.2539, Validation metric: 0.2547\nEstimator 95/1000, Train metric: 0.2535, Validation metric: 0.2544\nEstimator 96/1000, Train metric: 0.2530, Validation metric: 0.2539\nEstimator 97/1000, Train metric: 0.2526, Validation metric: 0.2534\nEstimator 98/1000, Train metric: 0.2523, Validation metric: 0.2532\nEstimator 99/1000, Train metric: 0.2500, Validation metric: 0.2511\nEstimator 100/1000, Train metric: 0.2484, Validation metric: 0.2495\nEstimator 101/1000, Train metric: 0.2476, Validation metric: 0.2488\nEstimator 102/1000, Train metric: 0.2464, Validation metric: 0.2475\nEstimator 103/1000, Train metric: 0.2455, Validation metric: 0.2467\nEstimator 104/1000, Train metric: 0.2448, Validation metric: 0.2460\nEstimator 105/1000, Train metric: 0.2446, Validation metric: 0.2462\nEstimator 106/1000, Train metric: 0.2444, Validation metric: 0.2460\nEstimator 107/1000, Train metric: 0.2434, Validation metric: 0.2452\nEstimator 108/1000, Train metric: 0.2432, Validation metric: 0.2450\nEstimator 109/1000, Train metric: 0.2428, Validation metric: 0.2445\nEstimator 110/1000, Train metric: 0.2426, Validation metric: 0.2444\nEstimator 111/1000, Train metric: 0.2424, Validation metric: 0.2442\nEstimator 112/1000, Train metric: 0.2419, Validation metric: 0.2438\nEstimator 113/1000, Train metric: 0.2409, Validation metric: 0.2428\nEstimator 114/1000, Train metric: 0.2406, Validation metric: 0.2425\nEstimator 115/1000, Train metric: 0.2403, Validation metric: 0.2422\nEstimator 116/1000, Train metric: 0.2401, Validation metric: 0.2420\nEstimator 117/1000, Train metric: 0.2385, Validation metric: 0.2406\nEstimator 118/1000, Train metric: 0.2380, Validation metric: 0.2400\nEstimator 119/1000, Train metric: 0.2378, Validation metric: 0.2399\nEstimator 120/1000, Train metric: 0.2373, Validation metric: 0.2393\nEstimator 121/1000, Train metric: 0.2360, Validation metric: 0.2381\nEstimator 122/1000, Train metric: 0.2352, Validation metric: 0.2374\nEstimator 123/1000, Train metric: 0.2350, Validation metric: 0.2371\nEstimator 124/1000, Train metric: 0.2346, Validation metric: 0.2368\nEstimator 125/1000, Train metric: 0.2341, Validation metric: 0.2362\nEstimator 126/1000, Train metric: 0.2339, Validation metric: 0.2360\nEstimator 127/1000, Train metric: 0.2334, Validation metric: 0.2355\nEstimator 128/1000, Train metric: 0.2328, Validation metric: 0.2350\nEstimator 129/1000, Train metric: 0.2315, Validation metric: 0.2338\nEstimator 130/1000, Train metric: 0.2306, Validation metric: 0.2330\nEstimator 131/1000, Train metric: 0.2305, Validation metric: 0.2329\nEstimator 132/1000, Train metric: 0.2301, Validation metric: 0.2326\nEstimator 133/1000, Train metric: 0.2299, Validation metric: 0.2326\nEstimator 134/1000, Train metric: 0.2298, Validation metric: 0.2324\nEstimator 135/1000, Train metric: 0.2291, Validation metric: 0.2317\nEstimator 136/1000, Train metric: 0.2289, Validation metric: 0.2316\nEstimator 137/1000, Train metric: 0.2287, Validation metric: 0.2315\nEstimator 138/1000, Train metric: 0.2285, Validation metric: 0.2313\nEstimator 139/1000, Train metric: 0.2281, Validation metric: 0.2309\nEstimator 140/1000, Train metric: 0.2276, Validation metric: 0.2303\nEstimator 141/1000, Train metric: 0.2274, Validation metric: 0.2301\nEstimator 142/1000, Train metric: 0.2272, Validation metric: 0.2300\nEstimator 143/1000, Train metric: 0.2260, Validation metric: 0.2288\nEstimator 144/1000, Train metric: 0.2253, Validation metric: 0.2281\nEstimator 145/1000, Train metric: 0.2243, Validation metric: 0.2272\nEstimator 146/1000, Train metric: 0.2238, Validation metric: 0.2268\nEstimator 147/1000, Train metric: 0.2237, Validation metric: 0.2266\nEstimator 148/1000, Train metric: 0.2235, Validation metric: 0.2264\nEstimator 149/1000, Train metric: 0.2233, Validation metric: 0.2264\nEstimator 150/1000, Train metric: 0.2228, Validation metric: 0.2258\nEstimator 151/1000, Train metric: 0.2213, Validation metric: 0.2245\nEstimator 152/1000, Train metric: 0.2211, Validation metric: 0.2244\nEstimator 153/1000, Train metric: 0.2209, Validation metric: 0.2242\nEstimator 154/1000, Train metric: 0.2200, Validation metric: 0.2233\nEstimator 155/1000, Train metric: 0.2191, Validation metric: 0.2225\nEstimator 156/1000, Train metric: 0.2190, Validation metric: 0.2224\nEstimator 157/1000, Train metric: 0.2186, Validation metric: 0.2221\nEstimator 158/1000, Train metric: 0.2183, Validation metric: 0.2217\nEstimator 159/1000, Train metric: 0.2175, Validation metric: 0.2211\nEstimator 160/1000, Train metric: 0.2169, Validation metric: 0.2205\nEstimator 161/1000, Train metric: 0.2164, Validation metric: 0.2200\nEstimator 162/1000, Train metric: 0.2153, Validation metric: 0.2189\nEstimator 163/1000, Train metric: 0.2148, Validation metric: 0.2184\nEstimator 164/1000, Train metric: 0.2145, Validation metric: 0.2181\nEstimator 165/1000, Train metric: 0.2143, Validation metric: 0.2180\nEstimator 166/1000, Train metric: 0.2142, Validation metric: 0.2179\nEstimator 167/1000, Train metric: 0.2133, Validation metric: 0.2171\nEstimator 168/1000, Train metric: 0.2125, Validation metric: 0.2163\nEstimator 169/1000, Train metric: 0.2120, Validation metric: 0.2157\nEstimator 170/1000, Train metric: 0.2119, Validation metric: 0.2156\nEstimator 171/1000, Train metric: 0.2112, Validation metric: 0.2150\nEstimator 172/1000, Train metric: 0.2107, Validation metric: 0.2143\nEstimator 173/1000, Train metric: 0.2102, Validation metric: 0.2138\nEstimator 174/1000, Train metric: 0.2100, Validation metric: 0.2137\nEstimator 175/1000, Train metric: 0.2096, Validation metric: 0.2133\nEstimator 176/1000, Train metric: 0.2093, Validation metric: 0.2130\nEstimator 177/1000, Train metric: 0.2092, Validation metric: 0.2128\nEstimator 178/1000, Train metric: 0.2085, Validation metric: 0.2122\nEstimator 179/1000, Train metric: 0.2078, Validation metric: 0.2116\nEstimator 180/1000, Train metric: 0.2077, Validation metric: 0.2115\nEstimator 181/1000, Train metric: 0.2075, Validation metric: 0.2114\nEstimator 182/1000, Train metric: 0.2069, Validation metric: 0.2108\nEstimator 183/1000, Train metric: 0.2067, Validation metric: 0.2106\nEstimator 184/1000, Train metric: 0.2062, Validation metric: 0.2101\nEstimator 185/1000, Train metric: 0.2051, Validation metric: 0.2090\nEstimator 186/1000, Train metric: 0.2050, Validation metric: 0.2090\nEstimator 187/1000, Train metric: 0.2049, Validation metric: 0.2089\nEstimator 188/1000, Train metric: 0.2042, Validation metric: 0.2082\nEstimator 189/1000, Train metric: 0.2031, Validation metric: 0.2072\nEstimator 190/1000, Train metric: 0.2030, Validation metric: 0.2071\nEstimator 191/1000, Train metric: 0.2023, Validation metric: 0.2065\nEstimator 192/1000, Train metric: 0.2020, Validation metric: 0.2062\nEstimator 193/1000, Train metric: 0.2014, Validation metric: 0.2055\nEstimator 194/1000, Train metric: 0.2012, Validation metric: 0.2049\nEstimator 195/1000, Train metric: 0.2007, Validation metric: 0.2045\nEstimator 196/1000, Train metric: 0.2002, Validation metric: 0.2040\nEstimator 197/1000, Train metric: 0.1995, Validation metric: 0.2033\nEstimator 198/1000, Train metric: 0.1988, Validation metric: 0.2027\nEstimator 199/1000, Train metric: 0.1988, Validation metric: 0.2026\nEstimator 200/1000, Train metric: 0.1985, Validation metric: 0.2025\nEstimator 201/1000, Train metric: 0.1985, Validation metric: 0.2024\nEstimator 202/1000, Train metric: 0.1980, Validation metric: 0.2020\nEstimator 203/1000, Train metric: 0.1971, Validation metric: 0.2011\nEstimator 204/1000, Train metric: 0.1965, Validation metric: 0.2006\nEstimator 205/1000, Train metric: 0.1961, Validation metric: 0.2002\nEstimator 206/1000, Train metric: 0.1960, Validation metric: 0.2001\nEstimator 207/1000, Train metric: 0.1957, Validation metric: 0.1998\nEstimator 208/1000, Train metric: 0.1956, Validation metric: 0.1998\nEstimator 209/1000, Train metric: 0.1950, Validation metric: 0.1993\nEstimator 210/1000, Train metric: 0.1947, Validation metric: 0.1990\nEstimator 211/1000, Train metric: 0.1939, Validation metric: 0.1982\nEstimator 212/1000, Train metric: 0.1936, Validation metric: 0.1979\nEstimator 213/1000, Train metric: 0.1930, Validation metric: 0.1974\nEstimator 214/1000, Train metric: 0.1927, Validation metric: 0.1971\nEstimator 215/1000, Train metric: 0.1926, Validation metric: 0.1971\nEstimator 216/1000, Train metric: 0.1926, Validation metric: 0.1970\nEstimator 217/1000, Train metric: 0.1924, Validation metric: 0.1968\nEstimator 218/1000, Train metric: 0.1922, Validation metric: 0.1967\nEstimator 219/1000, Train metric: 0.1922, Validation metric: 0.1967\nEstimator 220/1000, Train metric: 0.1918, Validation metric: 0.1964\nEstimator 221/1000, Train metric: 0.1914, Validation metric: 0.1960\nEstimator 222/1000, Train metric: 0.1913, Validation metric: 0.1958\nEstimator 223/1000, Train metric: 0.1903, Validation metric: 0.1949\nEstimator 224/1000, Train metric: 0.1902, Validation metric: 0.1947\nEstimator 225/1000, Train metric: 0.1895, Validation metric: 0.1941\nEstimator 226/1000, Train metric: 0.1890, Validation metric: 0.1937\nEstimator 227/1000, Train metric: 0.1887, Validation metric: 0.1934\nEstimator 228/1000, Train metric: 0.1885, Validation metric: 0.1933\nEstimator 229/1000, Train metric: 0.1884, Validation metric: 0.1931\nEstimator 230/1000, Train metric: 0.1877, Validation metric: 0.1925\nEstimator 231/1000, Train metric: 0.1872, Validation metric: 0.1920\nEstimator 232/1000, Train metric: 0.1870, Validation metric: 0.1918\nEstimator 233/1000, Train metric: 0.1863, Validation metric: 0.1911\nEstimator 234/1000, Train metric: 0.1858, Validation metric: 0.1906\nEstimator 235/1000, Train metric: 0.1852, Validation metric: 0.1901\nEstimator 236/1000, Train metric: 0.1851, Validation metric: 0.1901\nEstimator 237/1000, Train metric: 0.1849, Validation metric: 0.1899\nEstimator 238/1000, Train metric: 0.1844, Validation metric: 0.1894\nEstimator 239/1000, Train metric: 0.1843, Validation metric: 0.1893\nEstimator 240/1000, Train metric: 0.1836, Validation metric: 0.1886\nEstimator 241/1000, Train metric: 0.1835, Validation metric: 0.1887\nEstimator 242/1000, Train metric: 0.1831, Validation metric: 0.1883\nEstimator 243/1000, Train metric: 0.1828, Validation metric: 0.1881\nEstimator 244/1000, Train metric: 0.1827, Validation metric: 0.1879\nEstimator 245/1000, Train metric: 0.1826, Validation metric: 0.1879\nEstimator 246/1000, Train metric: 0.1820, Validation metric: 0.1874\nEstimator 247/1000, Train metric: 0.1819, Validation metric: 0.1874\nEstimator 248/1000, Train metric: 0.1816, Validation metric: 0.1870\nEstimator 249/1000, Train metric: 0.1813, Validation metric: 0.1868\nEstimator 250/1000, Train metric: 0.1808, Validation metric: 0.1863\nEstimator 251/1000, Train metric: 0.1803, Validation metric: 0.1858\nEstimator 252/1000, Train metric: 0.1799, Validation metric: 0.1855\nEstimator 253/1000, Train metric: 0.1794, Validation metric: 0.1851\nEstimator 254/1000, Train metric: 0.1790, Validation metric: 0.1846\nEstimator 255/1000, Train metric: 0.1786, Validation metric: 0.1843\nEstimator 256/1000, Train metric: 0.1783, Validation metric: 0.1840\nEstimator 257/1000, Train metric: 0.1782, Validation metric: 0.1839\nEstimator 258/1000, Train metric: 0.1782, Validation metric: 0.1839\nEstimator 259/1000, Train metric: 0.1778, Validation metric: 0.1835\nEstimator 260/1000, Train metric: 0.1777, Validation metric: 0.1834\nEstimator 261/1000, Train metric: 0.1774, Validation metric: 0.1832\nEstimator 262/1000, Train metric: 0.1774, Validation metric: 0.1831\nEstimator 263/1000, Train metric: 0.1769, Validation metric: 0.1827\nEstimator 264/1000, Train metric: 0.1766, Validation metric: 0.1824\nEstimator 265/1000, Train metric: 0.1762, Validation metric: 0.1821\nEstimator 266/1000, Train metric: 0.1760, Validation metric: 0.1819\nEstimator 267/1000, Train metric: 0.1759, Validation metric: 0.1818\nEstimator 268/1000, Train metric: 0.1758, Validation metric: 0.1818\nEstimator 269/1000, Train metric: 0.1757, Validation metric: 0.1817\nEstimator 270/1000, Train metric: 0.1756, Validation metric: 0.1816\nEstimator 271/1000, Train metric: 0.1752, Validation metric: 0.1813\nEstimator 272/1000, Train metric: 0.1749, Validation metric: 0.1810\nEstimator 273/1000, Train metric: 0.1744, Validation metric: 0.1805\nEstimator 274/1000, Train metric: 0.1742, Validation metric: 0.1803\nEstimator 275/1000, Train metric: 0.1736, Validation metric: 0.1797\nEstimator 276/1000, Train metric: 0.1734, Validation metric: 0.1796\nEstimator 277/1000, Train metric: 0.1732, Validation metric: 0.1793\nEstimator 278/1000, Train metric: 0.1730, Validation metric: 0.1792\nEstimator 279/1000, Train metric: 0.1725, Validation metric: 0.1787\nEstimator 280/1000, Train metric: 0.1724, Validation metric: 0.1787\nEstimator 281/1000, Train metric: 0.1723, Validation metric: 0.1787\nEstimator 282/1000, Train metric: 0.1721, Validation metric: 0.1785\nEstimator 283/1000, Train metric: 0.1717, Validation metric: 0.1782\nEstimator 284/1000, Train metric: 0.1715, Validation metric: 0.1780\nEstimator 285/1000, Train metric: 0.1714, Validation metric: 0.1778\nEstimator 286/1000, Train metric: 0.1710, Validation metric: 0.1774\nEstimator 287/1000, Train metric: 0.1707, Validation metric: 0.1771\nEstimator 288/1000, Train metric: 0.1703, Validation metric: 0.1768\nEstimator 289/1000, Train metric: 0.1701, Validation metric: 0.1766\nEstimator 290/1000, Train metric: 0.1698, Validation metric: 0.1763\nEstimator 291/1000, Train metric: 0.1694, Validation metric: 0.1760\nEstimator 292/1000, Train metric: 0.1691, Validation metric: 0.1757\nEstimator 293/1000, Train metric: 0.1688, Validation metric: 0.1754\nEstimator 294/1000, Train metric: 0.1686, Validation metric: 0.1752\nEstimator 295/1000, Train metric: 0.1685, Validation metric: 0.1751\nEstimator 296/1000, Train metric: 0.1683, Validation metric: 0.1749\nEstimator 297/1000, Train metric: 0.1680, Validation metric: 0.1747\nEstimator 298/1000, Train metric: 0.1679, Validation metric: 0.1749\nEstimator 299/1000, Train metric: 0.1678, Validation metric: 0.1750\nEstimator 300/1000, Train metric: 0.1675, Validation metric: 0.1746\nEstimator 301/1000, Train metric: 0.1673, Validation metric: 0.1745\nEstimator 302/1000, Train metric: 0.1673, Validation metric: 0.1745\nEstimator 303/1000, Train metric: 0.1671, Validation metric: 0.1742\nEstimator 304/1000, Train metric: 0.1668, Validation metric: 0.1740\nEstimator 305/1000, Train metric: 0.1665, Validation metric: 0.1737\nEstimator 306/1000, Train metric: 0.1662, Validation metric: 0.1734\nEstimator 307/1000, Train metric: 0.1656, Validation metric: 0.1730\nEstimator 308/1000, Train metric: 0.1653, Validation metric: 0.1726\nEstimator 309/1000, Train metric: 0.1651, Validation metric: 0.1725\nEstimator 310/1000, Train metric: 0.1646, Validation metric: 0.1720\nEstimator 311/1000, Train metric: 0.1644, Validation metric: 0.1718\nEstimator 312/1000, Train metric: 0.1640, Validation metric: 0.1715\nEstimator 313/1000, Train metric: 0.1640, Validation metric: 0.1714\nEstimator 314/1000, Train metric: 0.1638, Validation metric: 0.1713\nEstimator 315/1000, Train metric: 0.1636, Validation metric: 0.1711\nEstimator 316/1000, Train metric: 0.1634, Validation metric: 0.1709\nEstimator 317/1000, Train metric: 0.1633, Validation metric: 0.1708\nEstimator 318/1000, Train metric: 0.1632, Validation metric: 0.1707\nEstimator 319/1000, Train metric: 0.1628, Validation metric: 0.1704\nEstimator 320/1000, Train metric: 0.1627, Validation metric: 0.1703\nEstimator 321/1000, Train metric: 0.1626, Validation metric: 0.1702\nEstimator 322/1000, Train metric: 0.1622, Validation metric: 0.1699\nEstimator 323/1000, Train metric: 0.1618, Validation metric: 0.1696\nEstimator 324/1000, Train metric: 0.1615, Validation metric: 0.1693\nEstimator 325/1000, Train metric: 0.1614, Validation metric: 0.1693\nEstimator 326/1000, Train metric: 0.1614, Validation metric: 0.1692\nEstimator 327/1000, Train metric: 0.1613, Validation metric: 0.1692\nEstimator 328/1000, Train metric: 0.1610, Validation metric: 0.1689\nEstimator 329/1000, Train metric: 0.1608, Validation metric: 0.1687\nEstimator 330/1000, Train metric: 0.1607, Validation metric: 0.1687\nEstimator 331/1000, Train metric: 0.1606, Validation metric: 0.1687\nEstimator 332/1000, Train metric: 0.1603, Validation metric: 0.1684\nEstimator 333/1000, Train metric: 0.1602, Validation metric: 0.1682\nEstimator 334/1000, Train metric: 0.1600, Validation metric: 0.1679\nEstimator 335/1000, Train metric: 0.1598, Validation metric: 0.1677\nEstimator 336/1000, Train metric: 0.1597, Validation metric: 0.1676\nEstimator 337/1000, Train metric: 0.1596, Validation metric: 0.1675\nEstimator 338/1000, Train metric: 0.1595, Validation metric: 0.1675\nEstimator 339/1000, Train metric: 0.1594, Validation metric: 0.1675\nEstimator 340/1000, Train metric: 0.1593, Validation metric: 0.1674\nEstimator 341/1000, Train metric: 0.1590, Validation metric: 0.1672\nEstimator 342/1000, Train metric: 0.1588, Validation metric: 0.1669\nEstimator 343/1000, Train metric: 0.1586, Validation metric: 0.1668\nEstimator 344/1000, Train metric: 0.1585, Validation metric: 0.1666\nEstimator 345/1000, Train metric: 0.1583, Validation metric: 0.1666\nEstimator 346/1000, Train metric: 0.1580, Validation metric: 0.1664\nEstimator 347/1000, Train metric: 0.1580, Validation metric: 0.1663\nEstimator 348/1000, Train metric: 0.1579, Validation metric: 0.1663\nEstimator 349/1000, Train metric: 0.1579, Validation metric: 0.1663\nEstimator 350/1000, Train metric: 0.1577, Validation metric: 0.1661\nEstimator 351/1000, Train metric: 0.1575, Validation metric: 0.1659\nEstimator 352/1000, Train metric: 0.1572, Validation metric: 0.1658\nEstimator 353/1000, Train metric: 0.1570, Validation metric: 0.1655\nEstimator 354/1000, Train metric: 0.1566, Validation metric: 0.1651\nEstimator 355/1000, Train metric: 0.1563, Validation metric: 0.1648\nEstimator 356/1000, Train metric: 0.1562, Validation metric: 0.1648\nEstimator 357/1000, Train metric: 0.1560, Validation metric: 0.1646\nEstimator 358/1000, Train metric: 0.1559, Validation metric: 0.1645\nEstimator 359/1000, Train metric: 0.1558, Validation metric: 0.1644\nEstimator 360/1000, Train metric: 0.1556, Validation metric: 0.1642\nEstimator 361/1000, Train metric: 0.1554, Validation metric: 0.1642\nEstimator 362/1000, Train metric: 0.1551, Validation metric: 0.1639\nEstimator 363/1000, Train metric: 0.1550, Validation metric: 0.1638\nEstimator 364/1000, Train metric: 0.1549, Validation metric: 0.1639\nEstimator 365/1000, Train metric: 0.1549, Validation metric: 0.1639\nEstimator 366/1000, Train metric: 0.1547, Validation metric: 0.1637\nEstimator 367/1000, Train metric: 0.1546, Validation metric: 0.1636\nEstimator 368/1000, Train metric: 0.1\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nain metric: 0.1413\nEstimator 484/1000, Train metric: 0.1412\nEstimator 485/1000, Train metric: 0.1412\nEstimator 486/1000, Train metric: 0.1411\nEstimator 487/1000, Train metric: 0.1411\nEstimator 488/1000, Train metric: 0.1411\nEstimator 489/1000, Train metric: 0.1410\nEstimator 490/1000, Train metric: 0.1409\nEstimator 491/1000, Train metric: 0.1408\nEstimator 492/1000, Train metric: 0.1407\nEstimator 493/1000, Train metric: 0.1406\nEstimator 494/1000, Train metric: 0.1406\nEstimator 495/1000, Train metric: 0.1405\nEstimator 496/1000, Train metric: 0.1404\nEstimator 497/1000, Train metric: 0.1404\nEstimator 498/1000, Train metric: 0.1403\nEstimator 499/1000, Train metric: 0.1403\nEstimator 500/1000, Train metric: 0.1402\nEstimator 501/1000, Train metric: 0.1401\nEstimator 502/1000, Train metric: 0.1400\nEstimator 503/1000, Train metric: 0.1399\nEstimator 504/1000, Train metric: 0.1399\nEstimator 505/1000, Train metric: 0.1397\nEstimator 506/1000, Train metric: 0.1396\nEstimator 507/1000, Train metric: 0.1394\nEstimator 508/1000, Train metric: 0.1394\nEstimator 509/1000, Train metric: 0.1393\nEstimator 510/1000, Train metric: 0.1393\nEstimator 511/1000, Train metric: 0.1392\nEstimator 512/1000, Train metric: 0.1391\nEstimator 513/1000, Train metric: 0.1390\nEstimator 514/1000, Train metric: 0.1389\nEstimator 515/1000, Train metric: 0.1388\nEstimator 516/1000, Train metric: 0.1387\nEstimator 517/1000, Train metric: 0.1386\nEstimator 518/1000, Train metric: 0.1385\nEstimator 519/1000, Train metric: 0.1385\nEstimator 520/1000, Train metric: 0.1384\nEstimator 521/1000, Train metric: 0.1383\nEstimator 522/1000, Train metric: 0.1382\nEstimator 523/1000, Train metric: 0.1382\nEstimator 524/1000, Train metric: 0.1381\nEstimator 525/1000, Train metric: 0.1381\nEstimator 526/1000, Train metric: 0.1379\nEstimator 527/1000, Train metric: 0.1378\nEstimator 528/1000, Train metric: 0.1377\nEstimator 529/1000, Train metric: 0.1376\nEstimator 530/1000, Train metric: 0.1375\nEstimator 531/1000, Train metric: 0.1374\nEstimator 532/1000, Train metric: 0.1374\nEstimator 533/1000, Train metric: 0.1373\nEstimator 534/1000, Train metric: 0.1372\nEstimator 535/1000, Train metric: 0.1371\nEstimator 536/1000, Train metric: 0.1370\nEstimator 537/1000, Train metric: 0.1370\nEstimator 538/1000, Train metric: 0.1369\nEstimator 539/1000, Train metric: 0.1369\nEstimator 540/1000, Train metric: 0.1367\nEstimator 541/1000, Train metric: 0.1367\nEstimator 542/1000, Train metric: 0.1366\nEstimator 543/1000, Train metric: 0.1366\nEstimator 544/1000, Train metric: 0.1364\nEstimator 545/1000, Train metric: 0.1364\nEstimator 546/1000, Train metric: 0.1363\nEstimator 547/1000, Train metric: 0.1363\nEstimator 548/1000, Train metric: 0.1362\nEstimator 549/1000, Train metric: 0.1362\nEstimator 550/1000, Train metric: 0.1361\nEstimator 551/1000, Train metric: 0.1361\nEstimator 552/1000, Train metric: 0.1360\nEstimator 553/1000, Train metric: 0.1359\nEstimator 554/1000, Train metric: 0.1358\nEstimator 555/1000, Train metric: 0.1357\nEstimator 556/1000, Train metric: 0.1356\nEstimator 557/1000, Train metric: 0.1356\nEstimator 558/1000, Train metric: 0.1355\nEstimator 559/1000, Train metric: 0.1354\nEstimator 560/1000, Train metric: 0.1353\nEstimator 561/1000, Train metric: 0.1352\nEstimator 562/1000, Train metric: 0.1351\nEstimator 563/1000, Train metric: 0.1350\nEstimator 564/1000, Train metric: 0.1350\nEstimator 565/1000, Train metric: 0.1349\nEstimator 566/1000, Train metric: 0.1348\nEstimator 567/1000, Train metric: 0.1348\nEstimator 568/1000, Train metric: 0.1348\nEstimator 569/1000, Train metric: 0.1346\nEstimator 570/1000, Train metric: 0.1346\nEstimator 571/1000, Train metric: 0.1345\nEstimator 572/1000, Train metric: 0.1344\nEstimator 573/1000, Train metric: 0.1343\nEstimator 574/1000, Train metric: 0.1343\nEstimator 575/1000, Train metric: 0.1342\nEstimator 576/1000, Train metric: 0.1341\nEstimator 577/1000, Train metric: 0.1340\nEstimator 578/1000, Train metric: 0.1340\nEstimator 579/1000, Train metric: 0.1339\nEstimator 580/1000, Train metric: 0.1338\nEstimator 581/1000, Train metric: 0.1338\nEstimator 582/1000, Train metric: 0.1338\nEstimator 583/1000, Train metric: 0.1338\nEstimator 584/1000, Train metric: 0.1337\nEstimator 585/1000, Train metric: 0.1335\nEstimator 586/1000, Train metric: 0.1335\nEstimator 587/1000, Train metric: 0.1334\nEstimator 588/1000, Train metric: 0.1334\nEstimator 589/1000, Train metric: 0.1333\nEstimator 590/1000, Train metric: 0.1333\nEstimator 591/1000, Train metric: 0.1331\nEstimator 592/1000, Train metric: 0.1331\nEstimator 593/1000, Train metric: 0.1330\nEstimator 594/1000, Train metric: 0.1330\nEstimator 595/1000, Train metric: 0.1329\nEstimator 596/1000, Train metric: 0.1329\nEstimator 597/1000, Train metric: 0.1329\nEstimator 598/1000, Train metric: 0.1328\nEstimator 599/1000, Train metric: 0.1327\nEstimator 600/1000, Train metric: 0.1327\nEstimator 601/1000, Train metric: 0.1327\nEstimator 602/1000, Train metric: 0.1326\nEstimator 603/1000, Train metric: 0.1326\nEstimator 604/1000, Train metric: 0.1326\nEstimator 605/1000, Train metric: 0.1325\nEstimator 606/1000, Train metric: 0.1325\nEstimator 607/1000, Train metric: 0.1324\nEstimator 608/1000, Train metric: 0.1323\nEstimator 609/1000, Train metric: 0.1323\nEstimator 610/1000, Train metric: 0.1322\nEstimator 611/1000, Train metric: 0.1321\nEstimator 612/1000, Train metric: 0.1320\nEstimator 613/1000, Train metric: 0.1319\nEstimator 614/1000, Train metric: 0.1319\nEstimator 615/1000, Train metric: 0.1318\nEstimator 616/1000, Train metric: 0.1318\nEstimator 617/1000, Train metric: 0.1318\nEstimator 618/1000, Train metric: 0.1317\nEstimator 619/1000, Train metric: 0.1317\nEstimator 620/1000, Train metric: 0.1316\nEstimator 621/1000, Train metric: 0.1315\nEstimator 622/1000, Train metric: 0.1315\nEstimator 623/1000, Train metric: 0.1314\nEstimator 624/1000, Train metric: 0.1314\nEstimator 625/1000, Train metric: 0.1313\nEstimator 626/1000, Train metric: 0.1312\nEstimator 627/1000, Train metric: 0.1311\nEstimator 628/1000, Train metric: 0.1311\nEstimator 629/1000, Train metric: 0.1311\nEstimator 630/1000, Train metric: 0.1310\nEstimator 631/1000, Train metric: 0.1310\nEstimator 632/1000, Train metric: 0.1310\nEstimator 633/1000, Train metric: 0.1309\nEstimator 634/1000, Train metric: 0.1309\nEstimator 635/1000, Train metric: 0.1308\nEstimator 636/1000, Train metric: 0.1308\nEstimator 637/1000, Train metric: 0.1307\nEstimator 638/1000, Train metric: 0.1307\nEstimator 639/1000, Train metric: 0.1306\nEstimator 640/1000, Train metric: 0.1306\nEstimator 641/1000, Train metric: 0.1305\nEstimator 642/1000, Train metric: 0.1304\nEstimator 643/1000, Train metric: 0.1303\nEstimator 644/1000, Train metric: 0.1303\nEstimator 645/1000, Train metric: 0.1302\nEstimator 646/1000, Train metric: 0.1302\nEstimator 647/1000, Train metric: 0.1301\nEstimator 648/1000, Train metric: 0.1301\nEstimator 649/1000, Train metric: 0.1300\nEstimator 650/1000, Train metric: 0.1299\nEstimator 651/1000, Train metric: 0.1299\nEstimator 652/1000, Train metric: 0.1299\nEstimator 653/1000, Train metric: 0.1299\nEstimator 654/1000, Train metric: 0.1298\nEstimator 655/1000, Train metric: 0.1297\nEstimator 656/1000, Train metric: 0.1297\nEstimator 657/1000, Train metric: 0.1296\nEstimator 658/1000, Train metric: 0.1296\nEstimator 659/1000, Train metric: 0.1295\nEstimator 660/1000, Train metric: 0.1295\nEstimator 661/1000, Train metric: 0.1294\nEstimator 662/1000, Train metric: 0.1293\nEstimator 663/1000, Train metric: 0.1292\nEstimator 664/1000, Train metric: 0.1290\nEstimator 665/1000, Train metric: 0.1290\nEstimator 666/1000, Train metric: 0.1289\nEstimator 667/1000, Train metric: 0.1289\nEstimator 668/1000, Train metric: 0.1288\nEstimator 669/1000, Train metric: 0.1288\nEstimator 670/1000, Train metric: 0.1287\nEstimator 671/1000, Train metric: 0.1287\nEstimator 672/1000, Train metric: 0.1287\nEstimator 673/1000, Train metric: 0.1286\nEstimator 674/1000, Train metric: 0.1286\nEstimator 675/1000, Train metric: 0.1286\nEstimator 676/1000, Train metric: 0.1285\nEstimator 677/1000, Train metric: 0.1285\nEstimator 678/1000, Train metric: 0.1284\nEstimator 679/1000, Train metric: 0.1283\nEstimator 680/1000, Train metric: 0.1283\nEstimator 681/1000, Train metric: 0.1283\nEstimator 682/1000, Train metric: 0.1282\nEstimator 683/1000, Train metric: 0.1281\nEstimator 684/1000, Train metric: 0.1281\nEstimator 685/1000, Train metric: 0.1281\nEstimator 686/1000, Train metric: 0.1280\nEstimator 687/1000, Train metric: 0.1279\nEstimator 688/1000, Train metric: 0.1279\nEstimator 689/1000, Train metric: 0.1278\nEstimator 690/1000, Train metric: 0.1278\nEstimator 691/1000, Train metric: 0.1277\nEstimator 692/1000, Train metric: 0.1277\nEstimator 693/1000, Train metric: 0.1277\nEstimator 694/1000, Train metric: 0.1276\nEstimator 695/1000, Train metric: 0.1276\nEstimator 696/1000, Train metric: 0.1276\nEstimator 697/1000, Train metric: 0.1275\nEstimator 698/1000, Train metric: 0.1275\nEstimator 699/1000, Train metric: 0.1275\nEstimator 700/1000, Train metric: 0.1274\nEstimator 701/1000, Train metric: 0.1273\nEstimator 702/1000, Train metric: 0.1273\nEstimator 703/1000, Train metric: 0.1273\nEstimator 704/1000, Train metric: 0.1272\nEstimator 705/1000, Train metric: 0.1272\nEstimator 706/1000, Train metric: 0.1271\nEstimator 707/1000, Train metric: 0.1270\nEstimator 708/1000, Train metric: 0.1269\nEstimator 709/1000, Train metric: 0.1269\nEstimator 710/1000, Train metric: 0.1269\nEstimator 711/1000, Train metric: 0.1268\nEstimator 712/1000, Train metric: 0.1268\nEstimator 713/1000, Train metric: 0.1268\nEstimator 714/1000, Train metric: 0.1267\nEstimator 715/1000, Train metric: 0.1267\nEstimator 716/1000, Train metric: 0.1266\nEstimator 717/1000, Train metric: 0.1265\nEstimator 718/1000, Train metric: 0.1265\nEstimator 719/1000, Train metric: 0.1265\nEstimator 720/1000, Train metric: 0.1264\nEstimator 721/1000, Train metric: 0.1264\nEstimator 722/1000, Train metric: 0.1263\nEstimator 723/1000, Train metric: 0.1263\nEstimator 724/1000, Train metric: 0.1263\nEstimator 725/1000, Train metric: 0.1262\nEstimator 726/1000, Train metric: 0.1262\nEstimator 727/1000, Train metric: 0.1262\nEstimator 728/1000, Train metric: 0.1261\nEstimator 729/1000, Train metric: 0.1261\nEstimator 730/1000, Train metric: 0.1261\nEstimator 731/1000, Train metric: 0.1260\nEstimator 732/1000, Train metric: 0.1259\nEstimator 733/1000, Train metric: 0.1259\nEstimator 734/1000, Train metric: 0.1258\nEstimator 735/1000, Train metric: 0.1258\nEstimator 736/1000, Train metric: 0.1258\nEstimator 737/1000, Train metric: 0.1257\nEstimator 738/1000, Train metric: 0.1257\nEstimator 739/1000, Train metric: 0.1256\nEstimator 740/1000, Train metric: 0.1256\nEstimator 741/1000, Train metric: 0.1255\nEstimator 742/1000, Train metric: 0.1255\nEstimator 743/1000, Train metric: 0.1254\nEstimator 744/1000, Train metric: 0.1254\nEstimator 745/1000, Train metric: 0.1254\nEstimator 746/1000, Train metric: 0.1253\nEstimator 747/1000, Train metric: 0.1253\nEstimator 748/1000, Train metric: 0.1252\nEstimator 749/1000, Train metric: 0.1252\nEstimator 750/1000, Train metric: 0.1252\nEstimator 751/1000, Train metric: 0.1252\nEstimator 752/1000, Train metric: 0.1252\nEstimator 753/1000, Train metric: 0.1251\nEstimator 754/1000, Train metric: 0.1251\nEstimator 755/1000, Train metric: 0.1250\nEstimator 756/1000, Train metric: 0.1250\nEstimator 757/1000, Train metric: 0.1249\nEstimator 758/1000, Train metric: 0.1249\nEstimator 759/1000, Train metric: 0.1248\nEstimator 760/1000, Train metric: 0.1248\nEstimator 761/1000, Train metric: 0.1248\nEstimator 762/1000, Train metric: 0.1248\nEstimator 763/1000, Train metric: 0.1247\nEstimator 764/1000, Train metric: 0.1247\nEstimator 765/1000, Train metric: 0.1247\nEstimator 766/1000, Train metric: 0.1246\nEstimator 767/1000, Train metric: 0.1246\nEstimator 768/1000, Train metric: 0.1246\nEstimator 769/1000, Train metric: 0.1245\nEstimator 770/1000, Train metric: 0.1245\nEstimator 771/1000, Train metric: 0.1245\nEstimator 772/1000, Train metric: 0.1245\nEstimator 773/1000, Train metric: 0.1244\nEstimator 774/1000, Train metric: 0.1244\nEstimator 775/1000, Train metric: 0.1244\nEstimator 776/1000, Train metric: 0.1243\nEstimator 777/1000, Train metric: 0.1242\nEstimator 778/1000, Train metric: 0.1242\nEstimator 779/1000, Train metric: 0.1242\nEstimator 780/1000, Train metric: 0.1241\nEstimator 781/1000, Train metric: 0.1241\nEstimator 782/1000, Train metric: 0.1241\nEstimator 783/1000, Train metric: 0.1241\nEstimator 784/1000, Train metric: 0.1240\nEstimator 785/1000, Train metric: 0.1240\nEstimator 786/1000, Train metric: 0.1240\nEstimator 787/1000, Train metric: 0.1239\nEstimator 788/1000, Train metric: 0.1239\nEstimator 789/1000, Train metric: 0.1238\nEstimator 790/1000, Train metric: 0.1238\nEstimator 791/1000, Train metric: 0.1237\nEstimator 792/1000, Train metric: 0.1237\nEstimator 793/1000, Train metric: 0.1237\nEstimator 794/1000, Train metric: 0.1236\nEstimator 795/1000, Train metric: 0.1236\nEstimator 796/1000, Train metric: 0.1235\nEstimator 797/1000, Train metric: 0.1235\nEstimator 798/1000, Train metric: 0.1234\nEstimator 799/1000, Train metric: 0.1234\nEstimator 800/1000, Train metric: 0.1233\nEstimator 801/1000, Train metric: 0.1233\nEstimator 802/1000, Train metric: 0.1232\nEstimator 803/1000, Train metric: 0.1232\nEstimator 804/1000, Train metric: 0.1232\nEstimator 805/1000, Train metric: 0.1231\nEstimator 806/1000, Train metric: 0.1231\nEstimator 807/1000, Train metric: 0.1230\nEstimator 808/1000, Train metric: 0.1230\nEstimator 809/1000, Train metric: 0.1230\nEstimator 810/1000, Train metric: 0.1230\nEstimator 811/1000, Train metric: 0.1229\nEstimator 812/1000, Train metric: 0.1229\nEstimator 813/1000, Train metric: 0.1229\nEstimator 814/1000, Train metric: 0.1228\nEstimator 815/1000, Train metric: 0.1228\nEstimator 816/1000, Train metric: 0.1227\nEstimator 817/1000, Train metric: 0.1227\nEstimator 818/1000, Train metric: 0.1226\nEstimator 819/1000, Train metric: 0.1226\nEstimator 820/1000, Train metric: 0.1226\nEstimator 821/1000, Train metric: 0.1225\nEstimator 822/1000, Train metric: 0.1224\nEstimator 823/1000, Train metric: 0.1224\nEstimator 824/1000, Train metric: 0.1224\nEstimator 825/1000, Train metric: 0.1223\nEstimator 826/1000, Train metric: 0.1223\nEstimator 827/1000, Train metric: 0.1223\nEstimator 828/1000, Train metric: 0.1222\nEstimator 829/1000, Train metric: 0.1222\nEstimator 830/1000, Train metric: 0.1222\nEstimator 831/1000, Train metric: 0.1222\nEstimator 832/1000, Train metric: 0.1221\nEstimator 833/1000, Train metric: 0.1221\nEstimator 834/1000, Train metric: 0.1220\nEstimator 835/1000, Train metric: 0.1220\nEstimator 836/1000, Train metric: 0.1220\nEstimator 837/1000, Train metric: 0.1220\nEstimator 838/1000, Train metric: 0.1219\nEstimator 839/1000, Train metric: 0.1218\nEstimator 840/1000, Train metric: 0.1218\nEstimator 841/1000, Train metric: 0.1218\nEstimator 842/1000, Train metric: 0.1218\nEstimator 843/1000, Train metric: 0.1218\nEstimator 844/1000, Train metric: 0.1217\nEstimator 845/1000, Train metric: 0.1217\nEstimator 846/1000, Train metric: 0.1216\nEstimator 847/1000, Train metric: 0.1216\nEstimator 848/1000, Train metric: 0.1216\nEstimator 849/1000, Train metric: 0.1216\nEstimator 850/1000, Train metric: 0.1215\nEstimator 851/1000, Train metric: 0.1215\nEstimator 852/1000, Train metric: 0.1214\nEstimator 853/1000, Train metric: 0.1214\nEstimator 854/1000, Train metric: 0.1214\nEstimator 855/1000, Train metric: 0.1213\nEstimator 856/1000, Train metric: 0.1213\nEstimator 857/1000, Train metric: 0.1212\nEstimator 858/1000, Train metric: 0.1211\nEstimator 859/1000, Train metric: 0.1211\nEstimator 860/1000, Train metric: 0.1210\nEstimator 861/1000, Train metric: 0.1210\nEstimator 862/1000, Train metric: 0.1210\nEstimator 863/1000, Train metric: 0.1209\nEstimator 864/1000, Train metric: 0.1209\nEstimator 865/1000, Train metric: 0.1208\nEstimator 866/1000, Train metric: 0.1208\nEstimator 867/1000, Train metric: 0.1208\nEstimator 868/1000, Train metric: 0.1208\nEstimator 869/1000, Train metric: 0.1207\nEstimator 870/1000, Train metric: 0.1207\nEstimator 871/1000, Train metric: 0.1207\nEstimator 872/1000, Train metric: 0.1206\nEstimator 873/1000, Train metric: 0.1206\nEstimator 874/1000, Train metric: 0.1205\nEstimator 875/1000, Train metric: 0.1205\nEstimator 876/1000, Train metric: 0.1205\nEstimator 877/1000, Train metric: 0.1205\nEstimator 878/1000, Train metric: 0.1204\nEstimator 879/1000, Train metric: 0.1204\nEstimator 880/1000, Train metric: 0.1204\nEstimator 881/1000, Train metric: 0.1203\nEstimator 882/1000, Train metric: 0.1202\nEstimator 883/1000, Train metric: 0.1202\nEstimator 884/1000, Train metric: 0.1202\nEstimator 885/1000, Train metric: 0.1202\nEstimator 886/1000, Train metric: 0.1201\nEstimator 887/1000, Train metric: 0.1201\nEstimator 888/1000, Train metric: 0.1201\nEstimator 889/1000, Train metric: 0.1200\nEstimator 890/1000, Train metric: 0.1200\nEstimator 891/1000, Train metric: 0.1200\nEstimator 892/1000, Train metric: 0.1199\nEstimator 893/1000, Train metric: 0.1199\nEstimator 894/1000, Train metric: 0.1199\nEstimator 895/1000, Train metric: 0.1198\nEstimator 896/1000, Train metric: 0.1198\nEstimator 897/1000, Train metric: 0.1198\nEstimator 898/1000, Train metric: 0.1197\nEstimator 899/1000, Train metric: 0.1197\nEstimator 900/1000, Train metric: 0.1197\nEstimator 901/1000, Train metric: 0.1196\nEstimator 902/1000, Train metric: 0.1196\nEstimator 903/1000, Train metric: 0.1195\nEstimator 904/1000, Train metric: 0.1195\nEstimator 905/1000, Train metric: 0.1195\nEstimator 906/1000, Train metric: 0.1195\nEstimator 907/1000, Train metric: 0.1194\nEstimator 908/1000, Train metric: 0.1194\nEstimator 909/1000, Train metric: 0.1194\nEstimator 910/1000, Train metric: 0.1194\nEstimator 911/1000, Train metric: 0.1194\nEstimator 912/1000, Train metric: 0.1193\nEstimator 913/1000, Train metric: 0.1193\nEstimator 914/1000, Train metric: 0.1192\nEstimator 915/1000, Train metric: 0.1192\nEstimator 916/1000, Train metric: 0.1192\nEstimator 917/1000, Train metric: 0.1192\nEstimator 918/1000, Train metric: 0.1191\nEstimator 919/1000, Train metric: 0.1191\nEstimator 920/1000, Train metric: 0.1191\nEstimator 921/1000, Train metric: 0.1190\nEstimator 922/1000, Train metric: 0.1190\nEstimator 923/1000, Train metric: 0.1190\nEstimator 924/1000, Train metric: 0.1190\nEstimator 925/1000, Train metric: 0.1190\nEstimator 926/1000, Train metric: 0.1189\nEstimator 927/1000, Train metric: 0.1189\nEstimator 928/1000, Train metric: 0.1189\nEstimator 929/1000, Train metric: 0.1189\nEstimator 930/1000, Train metric: 0.1188\nEstimator 931/1000, Train metric: 0.1188\nEstimator 932/1000, Train metric: 0.1187\nEstimator 933/1000, Train metric: 0.1187\nEstimator 934/1000, Train metric: 0.1186\nEstimator 935/1000, Train metric: 0.1186\nEstimator 936/1000, Train metric: 0.1186\nEstimator 937/1000, Train metric: 0.1186\nEstimator 938/1000, Train metric: 0.1186\nEstimator 939/1000, Train metric: 0.1185\nEstimator 940/1000, Train metric: 0.1185\nEstimator 941/1000, Train metric: 0.1185\nEstimator 942/1000, Train metric: 0.1185\nEstimator 943/1000, Train metric: 0.1184\nEstimator 944/1000, Train metric: 0.1184\nEstimator 945/1000, Train metric: 0.1184\nEstimator 946/1000, Train metric: 0.1183\nEstimator 947/1000, Train metric: 0.1183\nEstimator 948/1000, Train metric: 0.1183\nEstimator 949/1000, Train metric: 0.1183\nEstimator 950/1000, Train metric: 0.1183\nEstimator 951/1000, Train metric: 0.1182\nEstimator 952/1000, Train metric: 0.1182\nEstimator 953/1000, Train metric: 0.1182\nEstimator 954/1000, Train metric: 0.1181\nEstimator 955/1000, Train metric: 0.1181\nEstimator 956/1000, Train metric: 0.1181\nEstimator 957/1000, Train metric: 0.1181\nEstimator 958/1000, Train metric: 0.1180\nEstimator 959/1000, Train metric: 0.1180\nEstimator 960/1000, Train metric: 0.1179\nEstimator 961/1000, Train metric: 0.1179\nEstimator 962/1000, Train metric: 0.1179\nEstimator 963/1000, Train metric: 0.1178\nEstimator 964/1000, Train metric: 0.1178\nEstimator 965/1000, Train metric: 0.1178\nEstimator 966/1000, Train metric: 0.1178\nEstimator 967/1000, Train metric: 0.1178\nEstimator 968/1000, Train metric: 0.1177\nEstimator 969/1000, Train metric: 0.1177\nEstimator 970/1000, Train metric: 0.1177\nEstimator 971/1000, Train metric: 0.1177\nEstimator 972/1000, Train metric: 0.1176\nEstimator 973/1000, Train metric: 0.1176\nEstimator 974/1000, Train metric: 0.1175\nEstimator 975/1000, Train metric: 0.1175\nEstimator 976/1000, Train metric: 0.1175\nEstimator 977/1000, Train metric: 0.1174\nEstimator 978/1000, Train metric: 0.1174\nEstimator 979/1000, Train metric: 0.1174\nEstimator 980/1000, Train metric: 0.1173\nEstimator 981/1000, Train metric: 0.1173\nEstimator 982/1000, Train metric: 0.1172\nEstimator 983/1000, Train metric: 0.1172\nEstimator 984/1000, Train metric: 0.1172\nEstimator 985/1000, Train metric: 0.1172\nEstimator 986/1000, Train metric: 0.1172\nEstimator 987/1000, Train metric: 0.1171\nEstimator 988/1000, Train metric: 0.1171\nEstimator 989/1000, Train metric: 0.1171\nEstimator 990/1000, Train metric: 0.1170\nEstimator 991/1000, Train metric: 0.1170\nEstimator 992/1000, Train metric: 0.1170\nEstimator 993/1000, Train metric: 0.1169\nEstimator 994/1000, Train metric: 0.1169\nEstimator 995/1000, Train metric: 0.1169\nEstimator 996/1000, Train metric: 0.1169\nEstimator 997/1000, Train metric: 0.1168\nEstimator 998/1000, Train metric: 0.1168\nEstimator 999/1000, Train metric: 0.1168\nElapsed time for fitting PGBM model: 99.31 s\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/pgbm/pgbm.py:378: UserWarning: FALLBACK path has been taken inside: compileCudaFusionGroup. This is an indication that codegen Failed for some reason.\nTo debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\nTo report the issue, try enable logging via setting the envvariable ` export PYTORCH_JIT_LOG_LEVEL=manager.cpp`\n (Triggered internally at ../torch/csrc/jit/codegen/cuda/manager.cpp:239.)\n  mu = _predict_forest_mu(X_test_splits, self.nodes_idx,\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/pgbm/pgbm.py:378: UserWarning: FALLBACK path has been taken inside: runCudaFusionGroup. This is an indication that codegen Failed for some reason.\nTo debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\n (Triggered internally at ../torch/csrc/jit/codegen/cuda/manager.cpp:331.)\n  mu = _predict_forest_mu(X_test_splits, self.nodes_idx,\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/lidl_x_tum_uncertainty_estimation/uncertainty_estimation_models.py:552: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  nll_temp = torch.tensor([-dist[i].log_prob(torch.tensor(y_test[i])) for i in range(len(dist))])\npath: predictions/pointpredictions_pgbm_with_best_dist.csv\ndirname: predictions\nfilename: pointpredictions_pgbm_with_best_dist.csv\nartifact_path: predictions\npath: /tmp/tmpsq_sdqos/pointpredictions_pgbm_with_best_dist.csv\ntmp_path: /tmp/tmpsq_sdqos/pointpredictions_pgbm_with_best_dist.csv\npath: predictions/samples_pgbm_with_best_dist.csv\ndirname: predictions\nfilename: samples_pgbm_with_best_dist.csv\nartifact_path: predictions\npath: /tmp/tmpdbjjqim1/samples_pgbm_with_best_dist.csv\ntmp_path: /tmp/tmpdbjjqim1/samples_pgbm_with_best_dist.csv\npath: predictions/distparams_pgbm_with_best_dist.csv\ndirname: predictions\nfilename: distparams_pgbm_with_best_dist.csv\nartifact_path: predictions\npath: /tmp/tmpt6cm6hvr/distparams_pgbm_with_best_dist.csv\ntmp_path: /tmp/tmpt6cm6hvr/distparams_pgbm_with_best_dist.csv\npath: predictions/quantiles_pgbm_with_best_dist0.05.csv\ndirname: predictions\nfilename: quantiles_pgbm_with_best_dist0.05.csv\nartifact_path: predictions\npath: /tmp/tmpsrx4fm99/quantiles_pgbm_with_best_dist0.05.csv\ntmp_path: /tmp/tmpsrx4fm99/quantiles_pgbm_with_best_dist0.05.csv\npath: predictions/quantiles_pgbm_with_best_dist0.1.csv\ndirname: predictions\nfilename: quantiles_pgbm_with_best_dist0.1.csv\nartifact_path: predictions\npath: /tmp/tmpkos8ucjb/quantiles_pgbm_with_best_dist0.1.csv\ntmp_path: /tmp/tmpkos8ucjb/quantiles_pgbm_with_best_dist0.1.csv\npath: predictions/quantiles_pgbm_with_best_dist0.5.csv\ndirname: predictions\nfilename: quantiles_pgbm_with_best_dist0.5.csv\nartifact_path: predictions\npath: /tmp/tmpg_cjj_pt/quantiles_pgbm_with_best_dist0.5.csv\ntmp_path: /tmp/tmpg_cjj_pt/quantiles_pgbm_with_best_dist0.5.csv\npath: predictions/quantiles_pgbm_with_best_dist0.9.csv\ndirname: predictions\nfilename: quantiles_pgbm_with_best_dist0.9.csv\nartifact_path: predictions\npath: /tmp/tmpa7ma9dq6/quantiles_pgbm_with_best_dist0.9.csv\ntmp_path: /tmp/tmpa7ma9dq6/quantiles_pgbm_with_best_dist0.9.csv\npath: predictions/quantiles_pgbm_with_best_dist0.95.csv\ndirname: predictions\nfilename: quantiles_pgbm_with_best_dist0.95.csv\nartifact_path: predictions\npath: /tmp/tmpr0ly2c4x/quantiles_pgbm_with_best_dist0.95.csv\ntmp_path: /tmp/tmpr0ly2c4x/quantiles_pgbm_with_best_dist0.95.csv\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["pgbm_dist_params = {\n","    'derivatives': 'exact',\n","    'distribution': 'normal',\n","    'device': 'gpu',\n","    'gpu_device_id': 0,\n","    \"n_jobs\": -1,\n","    \"min_split_gain\": 0.0,\n","    \"min_data_in_leaf\": 1,\n","    \"max_bin\": 1024,\n","    \"max_leaves\": 64,\n","    \"max_depth\": -1,\n","    \"learning_rate\": 0.1,\n","    \"n_estimators\": 1000,\n","    \"feature_fraction\": 0.7,\n","    \"bagging_fraction\": 0.7,\n","    \"seed\": 1,\n","    \"lambda\": 1,\n","}\n","\n","early_stopping_round = 20\n","quantiles = [0.05, 0.1, 0.5, 0.9, 0.95]\n","\n","start_time = time.perf_counter()\n","\n","    \n","# fitting model on train set with early stopping on valid set\n","pgbm_dist_fit_params = {**pgbm_dist_params, \"early_stopping_round\": early_stopping_round}\n","pgbm_dist_reg = PGBM(vectorizer_without_nan, target_transformer=target_transformer)\n","pgbm_dist_reg.fit(train_val_df, TARGET, X_val=valid_df, y_val=np.array(valid_df[TARGET]), params=pgbm_dist_fit_params, apply_optimize_distribution=True, verbose=True)\n","best_distribution = pgbm_dist_reg.model.distribution\n","best_tree_correlation = pgbm_dist_reg.model.tree_correlation\n","best_iteration = pgbm_dist_reg.best_iteration\n","print(\"Early stopping performed. Best iteration:\", best_iteration)\n","print(\"Best distribution found:\", best_distribution)\n","print(\"Best tree correlation found:\", best_tree_correlation)\n","\n","# fitting model on train+val set with best_iteration\n","pgbm_dist_full_fit_params = {**pgbm_dist_params, \"n_estimators\": best_iteration}\n","pgbm_dist_full_train_reg = PGBM(vectorizer_without_nan, target_transformer=target_transformer)\n","pgbm_dist_full_train_reg.fit(train_df, TARGET, params=pgbm_dist_full_fit_params, apply_optimize_distribution=False, verbose=True)\n","\n","# predicting on test set with our fully trained model\n","pgbm_dist_full_train_reg.model.distribution = best_distribution\n","pgbm_dist_full_train_reg.model.tree_correlation = best_tree_correlation\n","pgbm_dist_pred = pgbm_dist_full_train_reg.predict(test_df, quantiles=quantiles, prediction_types=[PredEnum.POINT_ESTIMATES, PredEnum.QUANTILES, PredEnum.SAMPLES, PredEnum.DISTRIBUTION_PARAMS], sample_size=300)\n","\n","pgbm_dist_metrics = pgbm_dist_full_train_reg.metrics(np.array(test_df[TARGET]), pgbm_dist_pred, confidence_interval_quantiles=[0.1,0.9])\n","pgbm_dist_metrics['time'] = fit_time\n","    \n","end_time = time.perf_counter()\n","full_time = np.round(end_time - start_time, 2)\n","pgbm_dist_metrics['time'] = full_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pgbm_dist_metrics"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"13a71a3e-ee26-43b0-9cbb-c2226ee618fe","showTitle":false,"title":""}},"source":["## 3.6 LSF"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4bf998f5-43ab-4e1f-9b53-f45930a40096","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["2f045a3fab1a4950aa135c8030fa7a75\n","Elapsed time for fitting LSF model: 29.05 s\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/lidl_x_tum_uncertainty_estimation/uncertainty_estimation_models.py:1628: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  samples = np.array([np.array(samples_for_one_input) for samples_for_one_input in samples])\n","path: predictions/pointpredictions_lsf_w_lightgbm.csv\n","dirname: predictions\n","filename: pointpredictions_lsf_w_lightgbm.csv\n","artifact_path: predictions\n","path: /tmp/tmp1rk_axs4/pointpredictions_lsf_w_lightgbm.csv\n","tmp_path: /tmp/tmp1rk_axs4/pointpredictions_lsf_w_lightgbm.csv\n","path: predictions/quantiles_lsf_w_lightgbm0.05.csv\n","dirname: predictions\n","filename: quantiles_lsf_w_lightgbm0.05.csv\n","artifact_path: predictions\n","path: /tmp/tmp4fuc8jbc/quantiles_lsf_w_lightgbm0.05.csv\n","tmp_path: /tmp/tmp4fuc8jbc/quantiles_lsf_w_lightgbm0.05.csv\n","path: predictions/quantiles_lsf_w_lightgbm0.1.csv\n","dirname: predictions\n","filename: quantiles_lsf_w_lightgbm0.1.csv\n","artifact_path: predictions\n","path: /tmp/tmpfd_yurcw/quantiles_lsf_w_lightgbm0.1.csv\n","tmp_path: /tmp/tmpfd_yurcw/quantiles_lsf_w_lightgbm0.1.csv\n","path: predictions/quantiles_lsf_w_lightgbm0.5.csv\n","dirname: predictions\n","filename: quantiles_lsf_w_lightgbm0.5.csv\n","artifact_path: predictions\n","path: /tmp/tmpc60cl1je/quantiles_lsf_w_lightgbm0.5.csv\n","tmp_path: /tmp/tmpc60cl1je/quantiles_lsf_w_lightgbm0.5.csv\n","path: predictions/quantiles_lsf_w_lightgbm0.9.csv\n","dirname: predictions\n","filename: quantiles_lsf_w_lightgbm0.9.csv\n","artifact_path: predictions\n","path: /tmp/tmp_vlrynax/quantiles_lsf_w_lightgbm0.9.csv\n","tmp_path: /tmp/tmp_vlrynax/quantiles_lsf_w_lightgbm0.9.csv\n","path: predictions/quantiles_lsf_w_lightgbm0.95.csv\n","dirname: predictions\n","filename: quantiles_lsf_w_lightgbm0.95.csv\n","artifact_path: predictions\n","path: /tmp/tmpijs59rls/quantiles_lsf_w_lightgbm0.95.csv\n","tmp_path: /tmp/tmpijs59rls/quantiles_lsf_w_lightgbm0.95.csv\n","path: predictions/samples_lsf_w_lightgbm.csv\n","dirname: predictions\n","filename: samples_lsf_w_lightgbm.csv\n","artifact_path: predictions\n","path: /tmp/tmpbi0z4ip2/samples_lsf_w_lightgbm.csv\n","tmp_path: /tmp/tmpbi0z4ip2/samples_lsf_w_lightgbm.csv\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"2f045a3fab1a4950aa135c8030fa7a75\nElapsed time for fitting LSF model: 29.05 s\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bd68881f-9b48-45f4-b67a-46b4e0aaad48/lib/python3.9/site-packages/lidl_x_tum_uncertainty_estimation/uncertainty_estimation_models.py:1628: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  samples = np.array([np.array(samples_for_one_input) for samples_for_one_input in samples])\npath: predictions/pointpredictions_lsf_w_lightgbm.csv\ndirname: predictions\nfilename: pointpredictions_lsf_w_lightgbm.csv\nartifact_path: predictions\npath: /tmp/tmp1rk_axs4/pointpredictions_lsf_w_lightgbm.csv\ntmp_path: /tmp/tmp1rk_axs4/pointpredictions_lsf_w_lightgbm.csv\npath: predictions/quantiles_lsf_w_lightgbm0.05.csv\ndirname: predictions\nfilename: quantiles_lsf_w_lightgbm0.05.csv\nartifact_path: predictions\npath: /tmp/tmp4fuc8jbc/quantiles_lsf_w_lightgbm0.05.csv\ntmp_path: /tmp/tmp4fuc8jbc/quantiles_lsf_w_lightgbm0.05.csv\npath: predictions/quantiles_lsf_w_lightgbm0.1.csv\ndirname: predictions\nfilename: quantiles_lsf_w_lightgbm0.1.csv\nartifact_path: predictions\npath: /tmp/tmpfd_yurcw/quantiles_lsf_w_lightgbm0.1.csv\ntmp_path: /tmp/tmpfd_yurcw/quantiles_lsf_w_lightgbm0.1.csv\npath: predictions/quantiles_lsf_w_lightgbm0.5.csv\ndirname: predictions\nfilename: quantiles_lsf_w_lightgbm0.5.csv\nartifact_path: predictions\npath: /tmp/tmpc60cl1je/quantiles_lsf_w_lightgbm0.5.csv\ntmp_path: /tmp/tmpc60cl1je/quantiles_lsf_w_lightgbm0.5.csv\npath: predictions/quantiles_lsf_w_lightgbm0.9.csv\ndirname: predictions\nfilename: quantiles_lsf_w_lightgbm0.9.csv\nartifact_path: predictions\npath: /tmp/tmp_vlrynax/quantiles_lsf_w_lightgbm0.9.csv\ntmp_path: /tmp/tmp_vlrynax/quantiles_lsf_w_lightgbm0.9.csv\npath: predictions/quantiles_lsf_w_lightgbm0.95.csv\ndirname: predictions\nfilename: quantiles_lsf_w_lightgbm0.95.csv\nartifact_path: predictions\npath: /tmp/tmpijs59rls/quantiles_lsf_w_lightgbm0.95.csv\ntmp_path: /tmp/tmpijs59rls/quantiles_lsf_w_lightgbm0.95.csv\npath: predictions/samples_lsf_w_lightgbm.csv\ndirname: predictions\nfilename: samples_lsf_w_lightgbm.csv\nartifact_path: predictions\npath: /tmp/tmpbi0z4ip2/samples_lsf_w_lightgbm.csv\ntmp_path: /tmp/tmpbi0z4ip2/samples_lsf_w_lightgbm.csv\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["lsf_params = {'min_bin_size': np.log(len(train_df))**2}\n","\n","base_estimator = lightgbm_full_train_reg.model\n","quantiles = [0.05, 0.1, 0.5, 0.9, 0.95]\n","\n","start_time = time.perf_counter()\n","\n","# fitting model on train set with early stopping on valid set\n","# since we cannot apply validation we are only training on the full train dataset once\n","lsf_reg = LSF(vectorizer_with_nan, target_transformer=target_transformer, base_model=base_estimator, model_trained=True, **lsf_params)\n","\n","lsf_reg.fit(train_df, TARGET, verbose=True)\n","fit_time = np.round(end_time - start_time, 2)\n","\n","# predicting on test set with our fully trained model\n","lsf_pred = lsf_reg.predict(test_df, quantiles=[0.05, 0.1, 0.5, 0.9, 0.95], prediction_types=[PredEnum.POINT_ESTIMATES, PredEnum.QUANTILES, PredEnum.SAMPLES])\n","predict_time = np.round(end_time - start_time, 2)\n","lsf_metrics = lsf_reg.metrics(np.array(test_df[TARGET]), lsf_pred, confidence_interval_quantiles=[0.1,0.9])\n","\n","end_time = time.perf_counter()\n","full_time = np.round(end_time - start_time, 2)\n","lsf_metrics['time'] = full_time"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e7661205-7268-4cc6-b1cc-fc14b31b0ea2","showTitle":false,"title":""}},"source":["## 3.7 TFT in PytorchFC"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3ba83d69-db7c-4e46-b2d5-38f57320f185","showTitle":false,"title":""}},"source":["We need a dataset without any gaps for TFT. Missing dates for some stores will not work with the existing code implementation and missing features (meaning some days exist for some time series and in other time series they are missing) in some part of the sequence are also a problem for encoder decoder structure as different time series."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2fe6b02c-98a2-4077-aa80-3621e43fc58d","showTitle":false,"title":""}},"outputs":[],"source":["full_df_with_zero_sales = pd.read_pickle(\"/dbfs/mnt/tum/data/kaggle/rossmann/rossmann_full_df_with_zero_sales.pickle\")\n","full_df_with_zero_sales = full_df_with_zero_sales.sort_values(['Store', 'Date'])\n","full_df_with_zero_sales = full_df_with_zero_sales.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"693732cb-8ac2-49c3-86df-53c528ffb003","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[9]: 1050330"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[9]: 1050330","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["len(full_df_with_zero_sales)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f0d9341e-9cec-4a0b-9b33-13494c1ca934","showTitle":false,"title":""}},"source":["For this Neural Network Model the target it not only inferred by the features in the same row, but also by features of \"previous\" rows as they will be encoded to better predict the upcoming values.\n","We do not only need to provide the corresponding rows for the target in our forecast horizon , but also the previous features and target values in lookback length in order to predict. This means that we do not split our dataframe as for tabular data.\n","We create a `full_train_df`, that contains information from start until holdout date (train_val split will be done internally in fit method) and a `full_test_df` which contains information from start until end of holdout date."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0fde594b-3dbc-4f8d-98fe-7b3060e77ae8","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>index</th><th>Store</th><th>DayOfWeek</th><th>Date</th><th>Sales</th><th>Customers</th><th>Open</th><th>Promo</th><th>StateHoliday</th><th>SchoolHoliday</th><th>Missing</th><th>Year</th><th>Month</th><th>Week</th><th>Day</th><th>Dayofweek</th><th>Dayofyear</th><th>Is_month_end</th><th>Is_month_start</th><th>Is_quarter_end</th><th>Is_quarter_start</th><th>Is_year_end</th><th>Is_year_start</th><th>Elapsed</th><th>StoreType</th><th>Assortment</th><th>CompetitionDistance</th><th>CompetitionOpenSinceMonth</th><th>CompetitionOpenSinceYear</th><th>Promo2</th><th>Promo2SinceWeek</th><th>Promo2SinceYear</th><th>PromoInterval</th><th>State</th><th>file</th><th>week</th><th>trend</th><th>file_DE</th><th>week_DE</th><th>trend_DE</th><th>Date_DE</th><th>State_DE</th><th>Month_DE</th><th>Day_DE</th><th>Dayofweek_DE</th><th>Dayofyear_DE</th><th>Is_month_end_DE</th><th>Is_month_start_DE</th><th>Is_quarter_end_DE</th><th>Is_quarter_start_DE</th><th>Is_year_end_DE</th><th>Is_year_start_DE</th><th>Elapsed_DE</th><th>Max_TemperatureC</th><th>Mean_TemperatureC</th><th>Min_TemperatureC</th><th>Dew_PointC</th><th>MeanDew_PointC</th><th>Min_DewpointC</th><th>Max_Humidity</th><th>Mean_Humidity</th><th>Min_Humidity</th><th>Max_Sea_Level_PressurehPa</th><th>Mean_Sea_Level_PressurehPa</th><th>Min_Sea_Level_PressurehPa</th><th>Max_VisibilityKm</th><th>Mean_VisibilityKm</th><th>Min_VisibilitykM</th><th>Max_Wind_SpeedKm_h</th><th>Mean_Wind_SpeedKm_h</th><th>Max_Gust_SpeedKm_h</th><th>Precipitationmm</th><th>CloudCover</th><th>Events</th><th>WindDirDegrees</th><th>StateName</th><th>CompetitionOpenSince</th><th>CompetitionDaysOpen</th><th>CompetitionMonthsOpen</th><th>Promo2Since</th><th>Promo2Days</th><th>Promo2Weeks</th><th>AfterSchoolHoliday</th><th>BeforeSchoolHoliday</th><th>AfterStateHoliday</th><th>BeforeStateHoliday</th><th>AfterPromo</th><th>BeforePromo</th><th>SchoolHoliday_bw</th><th>StateHoliday_bw</th><th>Promo_bw</th><th>SchoolHoliday_fw</th><th>StateHoliday_fw</th><th>Promo_fw</th></tr></thead><tbody><tr><td>1050277</td><td>1115</td><td>2</td><td>2015-06-09T00:00:00.000+0000</td><td>5119.0</td><td>363.0</td><td>1.0</td><td>0.0</td><td>false</td><td>0.0</td><td>0</td><td>2015</td><td>6</td><td>24</td><td>9</td><td>1</td><td>160</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1433808000</td><td>d</td><td>c</td><td>5350.0</td><td>1</td><td>1900</td><td>1</td><td>22</td><td>2012</td><td>Mar,Jun,Sept,Dec</td><td>HE</td><td>Rossmann_DE_HE</td><td>2015-06-14 - 2015-06-20</td><td>85</td><td>Rossmann_DE</td><td>2015-06-14 - 2015-06-20</td><td>82</td><td>2015-06-14T00:00:00.000+0000</td><td>null</td><td>6</td><td>14</td><td>6</td><td>165</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1434240000</td><td>20</td><td>16</td><td>12</td><td>7</td><td>6</td><td>4</td><td>67</td><td>48</td><td>29</td><td>1026</td><td>1025</td><td>1024</td><td>10.0</td><td>10.0</td><td>10.0</td><td>32</td><td>24</td><td>47.0</td><td>0.0</td><td>6.0</td><td>null</td><td>22</td><td>Hessen</td><td>1900-01-15T00:00:00.000+0000</td><td>0</td><td>0</td><td>2012-05-28T00:00:00.000+0000</td><td>1107</td><td>25</td><td>60</td><td>-48</td><td>5</td><td>0</td><td>4</td><td>-6</td><td>0.0</td><td>1.0</td><td>3.0</td><td>0.0</td><td>0.0</td><td>1.0</td></tr><tr><td>1050278</td><td>1115</td><td>3</td><td>2015-06-10T00:00:00.000+0000</td><td>4676.0</td><td>357.0</td><td>1.0</td><td>0.0</td><td>false</td><td>0.0</td><td>0</td><td>2015</td><td>6</td><td>24</td><td>10</td><td>2</td><td>161</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1433894400</td><td>d</td><td>c</td><td>5350.0</td><td>1</td><td>1900</td><td>1</td><td>22</td><td>2012</td><td>Mar,Jun,Sept,Dec</td><td>HE</td><td>Rossmann_DE_HE</td><td>2015-06-14 - 2015-06-20</td><td>85</td><td>Rossmann_DE</td><td>2015-06-14 - 2015-06-20</td><td>82</td><td>2015-06-14T00:00:00.000+0000</td><td>null</td><td>6</td><td>14</td><td>6</td><td>165</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1434240000</td><td>21</td><td>17</td><td>12</td><td>11</td><td>8</td><td>6</td><td>67</td><td>53</td><td>39</td><td>1026</td><td>1024</td><td>1022</td><td>10.0</td><td>10.0</td><td>10.0</td><td>26</td><td>16</td><td>null</td><td>0.0</td><td>6.0</td><td>null</td><td>59</td><td>Hessen</td><td>1900-01-15T00:00:00.000+0000</td><td>0</td><td>0</td><td>2012-05-28T00:00:00.000+0000</td><td>1108</td><td>25</td><td>61</td><td>-47</td><td>6</td><td>0</td><td>5</td><td>-5</td><td>0.0</td><td>1.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>2.0</td></tr><tr><td>1050279</td><td>1115</td><td>4</td><td>2015-06-11T00:00:00.000+0000</td><td>5216.0</td><td>380.0</td><td>1.0</td><td>0.0</td><td>false</td><td>0.0</td><td>0</td><td>2015</td><td>6</td><td>24</td><td>11</td><td>3</td><td>162</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1433980800</td><td>d</td><td>c</td><td>5350.0</td><td>1</td><td>1900</td><td>1</td><td>22</td><td>2012</td><td>Mar,Jun,Sept,Dec</td><td>HE</td><td>Rossmann_DE_HE</td><td>2015-06-14 - 2015-06-20</td><td>85</td><td>Rossmann_DE</td><td>2015-06-14 - 2015-06-20</td><td>82</td><td>2015-06-14T00:00:00.000+0000</td><td>null</td><td>6</td><td>14</td><td>6</td><td>165</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1434240000</td><td>24</td><td>21</td><td>17</td><td>12</td><td>9</td><td>8</td><td>64</td><td>47</td><td>28</td><td>1022</td><td>1019</td><td>1015</td><td>10.0</td><td>10.0</td><td>10.0</td><td>23</td><td>14</td><td>null</td><td>0.0</td><td>5.0</td><td>Rain</td><td>51</td><td>Hessen</td><td>1900-01-15T00:00:00.000+0000</td><td>0</td><td>0</td><td>2012-05-28T00:00:00.000+0000</td><td>1109</td><td>25</td><td>62</td><td>-46</td><td>7</td><td>0</td><td>6</td><td>-4</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>3.0</td></tr><tr><td>1050280</td><td>1115</td><td>5</td><td>2015-06-12T00:00:00.000+0000</td><td>5315.0</td><td>378.0</td><td>1.0</td><td>0.0</td><td>false</td><td>0.0</td><td>0</td><td>2015</td><td>6</td><td>24</td><td>12</td><td>4</td><td>163</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1434067200</td><td>d</td><td>c</td><td>5350.0</td><td>1</td><td>1900</td><td>1</td><td>22</td><td>2012</td><td>Mar,Jun,Sept,Dec</td><td>HE</td><td>Rossmann_DE_HE</td><td>2015-06-14 - 2015-06-20</td><td>85</td><td>Rossmann_DE</td><td>2015-06-14 - 2015-06-20</td><td>82</td><td>2015-06-14T00:00:00.000+0000</td><td>null</td><td>6</td><td>14</td><td>6</td><td>165</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1434240000</td><td>31</td><td>22</td><td>14</td><td>16</td><td>12</td><td>9</td><td>78</td><td>54</td><td>25</td><td>1015</td><td>1012</td><td>1009</td><td>31.0</td><td>15.0</td><td>10.0</td><td>40</td><td>11</td><td>58.0</td><td>0.0</td><td>5.0</td><td>Rain</td><td>42</td><td>Hessen</td><td>1900-01-15T00:00:00.000+0000</td><td>0</td><td>0</td><td>2012-05-28T00:00:00.000+0000</td><td>1110</td><td>25</td><td>63</td><td>-45</td><td>8</td><td>0</td><td>7</td><td>-3</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.0</td></tr><tr><td>1050281</td><td>1115</td><td>6</td><td>2015-06-13T00:00:00.000+0000</td><td>7736.0</td><td>503.0</td><td>1.0</td><td>0.0</td><td>false</td><td>0.0</td><td>0</td><td>2015</td><td>6</td><td>24</td><td>13</td><td>5</td><td>164</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1434153600</td><td>d</td><td>c</td><td>5350.0</td><td>1</td><td>1900</td><td>1</td><td>22</td><td>2012</td><td>Mar,Jun,Sept,Dec</td><td>HE</td><td>Rossmann_DE_HE</td><td>2015-06-14 - 2015-06-20</td><td>85</td><td>Rossmann_DE</td><td>2015-06-14 - 2015-06-20</td><td>82</td><td>2015-06-14T00:00:00.000+0000</td><td>null</td><td>6</td><td>14</td><td>6</td><td>165</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>1434240000</td><td>26</td><td>21</td><td>17</td><td>17</td><td>14</td><td>9</td><td>94</td><td>65</td><td>27</td><td>1011</td><td>1010</td><td>1008</td><td>31.0</td><td>13.0</td><td>10.0</td><td>29</td><td>11</td><td>39.0</td><td>0.0</td><td>6.0</td><td>Rain</td><td>240</td><td>Hessen</td><td>1900-01-15T00:00:00.000+0000</td><td>0</td><td>0</td><td>2012-05-28T00:00:00.000+0000</td><td>1111</td><td>25</td><td>64</td><td>-44</td><td>9</td><td>0</td><td>8</td><td>-2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1050277,1115,2,"2015-06-09T00:00:00.000+0000",5119,363,1,0,false,0,0,2015,6,24,9,1,160,false,false,false,false,false,false,1433808000,"d","c",5350,1,1900,1,22,2012,"Mar,Jun,Sept,Dec","HE","Rossmann_DE_HE","2015-06-14 - 2015-06-20",85,"Rossmann_DE","2015-06-14 - 2015-06-20",82,"2015-06-14T00:00:00.000+0000",null,6,14,6,165,false,false,false,false,false,false,1434240000,20,16,12,7,6,4,67,48,29,1026,1025,1024,10,10,10,32,24,47,0,6,null,22,"Hessen","1900-01-15T00:00:00.000+0000",0,0,"2012-05-28T00:00:00.000+0000",1107,25,60,-48,5,0,4,-6,0,1,3,0,0,1],[1050278,1115,3,"2015-06-10T00:00:00.000+0000",4676,357,1,0,false,0,0,2015,6,24,10,2,161,false,false,false,false,false,false,1433894400,"d","c",5350,1,1900,1,22,2012,"Mar,Jun,Sept,Dec","HE","Rossmann_DE_HE","2015-06-14 - 2015-06-20",85,"Rossmann_DE","2015-06-14 - 2015-06-20",82,"2015-06-14T00:00:00.000+0000",null,6,14,6,165,false,false,false,false,false,false,1434240000,21,17,12,11,8,6,67,53,39,1026,1024,1022,10,10,10,26,16,null,0,6,null,59,"Hessen","1900-01-15T00:00:00.000+0000",0,0,"2012-05-28T00:00:00.000+0000",1108,25,61,-47,6,0,5,-5,0,1,2,0,0,2],[1050279,1115,4,"2015-06-11T00:00:00.000+0000",5216,380,1,0,false,0,0,2015,6,24,11,3,162,false,false,false,false,false,false,1433980800,"d","c",5350,1,1900,1,22,2012,"Mar,Jun,Sept,Dec","HE","Rossmann_DE_HE","2015-06-14 - 2015-06-20",85,"Rossmann_DE","2015-06-14 - 2015-06-20",82,"2015-06-14T00:00:00.000+0000",null,6,14,6,165,false,false,false,false,false,false,1434240000,24,21,17,12,9,8,64,47,28,1022,1019,1015,10,10,10,23,14,null,0,5,"Rain",51,"Hessen","1900-01-15T00:00:00.000+0000",0,0,"2012-05-28T00:00:00.000+0000",1109,25,62,-46,7,0,6,-4,0,0,1,0,0,3],[1050280,1115,5,"2015-06-12T00:00:00.000+0000",5315,378,1,0,false,0,0,2015,6,24,12,4,163,false,false,false,false,false,false,1434067200,"d","c",5350,1,1900,1,22,2012,"Mar,Jun,Sept,Dec","HE","Rossmann_DE_HE","2015-06-14 - 2015-06-20",85,"Rossmann_DE","2015-06-14 - 2015-06-20",82,"2015-06-14T00:00:00.000+0000",null,6,14,6,165,false,false,false,false,false,false,1434240000,31,22,14,16,12,9,78,54,25,1015,1012,1009,31,15,10,40,11,58,0,5,"Rain",42,"Hessen","1900-01-15T00:00:00.000+0000",0,0,"2012-05-28T00:00:00.000+0000",1110,25,63,-45,8,0,7,-3,0,0,0,0,0,4],[1050281,1115,6,"2015-06-13T00:00:00.000+0000",7736,503,1,0,false,0,0,2015,6,24,13,5,164,false,false,false,false,false,false,1434153600,"d","c",5350,1,1900,1,22,2012,"Mar,Jun,Sept,Dec","HE","Rossmann_DE_HE","2015-06-14 - 2015-06-20",85,"Rossmann_DE","2015-06-14 - 2015-06-20",82,"2015-06-14T00:00:00.000+0000",null,6,14,6,165,false,false,false,false,false,false,1434240000,26,21,17,17,14,9,94,65,27,1011,1010,1008,31,13,10,29,11,39,0,6,"Rain",240,"Hessen","1900-01-15T00:00:00.000+0000",0,0,"2012-05-28T00:00:00.000+0000",1111,25,64,-44,9,0,8,-2,0,0,0,0,0,5]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"index","type":"\"long\""},{"metadata":"{}","name":"Store","type":"\"long\""},{"metadata":"{}","name":"DayOfWeek","type":"\"long\""},{"metadata":"{}","name":"Date","type":"\"timestamp\""},{"metadata":"{}","name":"Sales","type":"\"double\""},{"metadata":"{}","name":"Customers","type":"\"double\""},{"metadata":"{}","name":"Open","type":"\"double\""},{"metadata":"{}","name":"Promo","type":"\"double\""},{"metadata":"{}","name":"StateHoliday","type":"\"boolean\""},{"metadata":"{}","name":"SchoolHoliday","type":"\"double\""},{"metadata":"{}","name":"Missing","type":"\"long\""},{"metadata":"{}","name":"Year","type":"\"long\""},{"metadata":"{}","name":"Month","type":"\"long\""},{"metadata":"{}","name":"Week","type":"\"long\""},{"metadata":"{}","name":"Day","type":"\"long\""},{"metadata":"{}","name":"Dayofweek","type":"\"long\""},{"metadata":"{}","name":"Dayofyear","type":"\"long\""},{"metadata":"{}","name":"Is_month_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_month_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_start","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_end","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_start","type":"\"boolean\""},{"metadata":"{}","name":"Elapsed","type":"\"long\""},{"metadata":"{}","name":"StoreType","type":"\"string\""},{"metadata":"{}","name":"Assortment","type":"\"string\""},{"metadata":"{}","name":"CompetitionDistance","type":"\"double\""},{"metadata":"{}","name":"CompetitionOpenSinceMonth","type":"\"integer\""},{"metadata":"{}","name":"CompetitionOpenSinceYear","type":"\"integer\""},{"metadata":"{}","name":"Promo2","type":"\"long\""},{"metadata":"{}","name":"Promo2SinceWeek","type":"\"integer\""},{"metadata":"{}","name":"Promo2SinceYear","type":"\"integer\""},{"metadata":"{}","name":"PromoInterval","type":"\"string\""},{"metadata":"{}","name":"State","type":"\"string\""},{"metadata":"{}","name":"file","type":"\"string\""},{"metadata":"{}","name":"week","type":"\"string\""},{"metadata":"{}","name":"trend","type":"\"long\""},{"metadata":"{}","name":"file_DE","type":"\"string\""},{"metadata":"{}","name":"week_DE","type":"\"string\""},{"metadata":"{}","name":"trend_DE","type":"\"long\""},{"metadata":"{}","name":"Date_DE","type":"\"timestamp\""},{"metadata":"{}","name":"State_DE","type":"\"void\""},{"metadata":"{}","name":"Month_DE","type":"\"long\""},{"metadata":"{}","name":"Day_DE","type":"\"long\""},{"metadata":"{}","name":"Dayofweek_DE","type":"\"long\""},{"metadata":"{}","name":"Dayofyear_DE","type":"\"long\""},{"metadata":"{}","name":"Is_month_end_DE","type":"\"boolean\""},{"metadata":"{}","name":"Is_month_start_DE","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_end_DE","type":"\"boolean\""},{"metadata":"{}","name":"Is_quarter_start_DE","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_end_DE","type":"\"boolean\""},{"metadata":"{}","name":"Is_year_start_DE","type":"\"boolean\""},{"metadata":"{}","name":"Elapsed_DE","type":"\"long\""},{"metadata":"{}","name":"Max_TemperatureC","type":"\"long\""},{"metadata":"{}","name":"Mean_TemperatureC","type":"\"long\""},{"metadata":"{}","name":"Min_TemperatureC","type":"\"long\""},{"metadata":"{}","name":"Dew_PointC","type":"\"long\""},{"metadata":"{}","name":"MeanDew_PointC","type":"\"long\""},{"metadata":"{}","name":"Min_DewpointC","type":"\"long\""},{"metadata":"{}","name":"Max_Humidity","type":"\"long\""},{"metadata":"{}","name":"Mean_Humidity","type":"\"long\""},{"metadata":"{}","name":"Min_Humidity","type":"\"long\""},{"metadata":"{}","name":"Max_Sea_Level_PressurehPa","type":"\"long\""},{"metadata":"{}","name":"Mean_Sea_Level_PressurehPa","type":"\"long\""},{"metadata":"{}","name":"Min_Sea_Level_PressurehPa","type":"\"long\""},{"metadata":"{}","name":"Max_VisibilityKm","type":"\"double\""},{"metadata":"{}","name":"Mean_VisibilityKm","type":"\"double\""},{"metadata":"{}","name":"Min_VisibilitykM","type":"\"double\""},{"metadata":"{}","name":"Max_Wind_SpeedKm_h","type":"\"long\""},{"metadata":"{}","name":"Mean_Wind_SpeedKm_h","type":"\"long\""},{"metadata":"{}","name":"Max_Gust_SpeedKm_h","type":"\"double\""},{"metadata":"{}","name":"Precipitationmm","type":"\"double\""},{"metadata":"{}","name":"CloudCover","type":"\"double\""},{"metadata":"{}","name":"Events","type":"\"string\""},{"metadata":"{}","name":"WindDirDegrees","type":"\"long\""},{"metadata":"{}","name":"StateName","type":"\"string\""},{"metadata":"{}","name":"CompetitionOpenSince","type":"\"timestamp\""},{"metadata":"{}","name":"CompetitionDaysOpen","type":"\"long\""},{"metadata":"{}","name":"CompetitionMonthsOpen","type":"\"long\""},{"metadata":"{}","name":"Promo2Since","type":"\"timestamp\""},{"metadata":"{}","name":"Promo2Days","type":"\"long\""},{"metadata":"{}","name":"Promo2Weeks","type":"\"long\""},{"metadata":"{}","name":"AfterSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforeSchoolHoliday","type":"\"integer\""},{"metadata":"{}","name":"AfterStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"BeforeStateHoliday","type":"\"integer\""},{"metadata":"{}","name":"AfterPromo","type":"\"integer\""},{"metadata":"{}","name":"BeforePromo","type":"\"integer\""},{"metadata":"{}","name":"SchoolHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_bw","type":"\"double\""},{"metadata":"{}","name":"Promo_bw","type":"\"double\""},{"metadata":"{}","name":"SchoolHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"StateHoliday_fw","type":"\"double\""},{"metadata":"{}","name":"Promo_fw","type":"\"double\""}],"type":"table"}},"output_type":"display_data"}],"source":["# inputs required for neural network\n","full_train_df = full_df_with_zero_sales[full_df_with_zero_sales['Date'] < \"20150614\"].sort_values(['Store', 'Date'])\n","full_test_df = full_df_with_zero_sales.copy()\n","display(full_train_df.tail())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"53de6161-5a60-41eb-ac89-985877e0da7f","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Time index called \"time_index_tft\" added to provided dataframe\n","Time index called \"time_index_tft\" added to provided dataframe\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Time index called \"time_index_tft\" added to provided dataframe\nTime index called \"time_index_tft\" added to provided dataframe\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["static_cat_vars = ['Store', 'StoreType', 'CompetitionOpenSinceYear', 'Promo2SinceYear', 'Assortment']\n","dynamic_cat_vars = ['DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'PromoInterval', 'Week', 'Promo_fw', \n","    'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw', 'SchoolHoliday_fw', 'SchoolHoliday_bw']\n","cont_vars = ['AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']\n","\n","# bring categorical features to type 'str'\n","full_train_df[static_cat_vars+dynamic_cat_vars] = full_train_df[static_cat_vars+dynamic_cat_vars].astype(str)\n","full_test_df[static_cat_vars+dynamic_cat_vars] = full_test_df[static_cat_vars+dynamic_cat_vars].astype(str)\n","\n","# # add_time_idx_to_df()\n","full_train_df = TFTPytorchFC.add_time_idx_to_df(X=full_train_df, group_ids=\"Store\")\n","full_test_df = TFTPytorchFC.add_time_idx_to_df(X=full_test_df, group_ids=\"Store\")\n","\n","# obtain_y_test_out_of_X_test()\n","tft_y_test = TFTPytorchFC.obtain_y_test_out_of_X_test(X_test= full_test_df, forecast_horizon=forecast_horizon, time_idx=\"time_index_tft\", target=TARGET, group_ids=\"Store\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5cf3d7e9-a815-40a5-93b3-99f3fd206020","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["04f0f472ed284b2684f2f297c8e6e395\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-ba4997f9-65ba-4da8-aeea-58739f99d3d5/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n","  rank_zero_warn(\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-ba4997f9-65ba-4da8-aeea-58739f99d3d5/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n","  rank_zero_warn(\n","Missing logger folder: /databricks/driver/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name                               | Type                            | Params\n","----------------------------------------------------------------------------------------\n","0  | loss                               | QuantileLoss                    | 0     \n","1  | logging_metrics                    | ModuleList                      | 0     \n","2  | input_embeddings                   | MultiEmbedding                  | 92.0 K\n","3  | prescalers                         | ModuleDict                      | 96    \n","4  | static_variable_selection          | VariableSelectionNetwork        | 4.2 K \n","5  | encoder_variable_selection         | VariableSelectionNetwork        | 51.5 K\n","6  | decoder_variable_selection         | VariableSelectionNetwork        | 45.2 K\n","7  | static_context_variable_selection  | GatedResidualNetwork            | 231 K \n","8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 231 K \n","9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 231 K \n","10 | static_context_enrichment          | GatedResidualNetwork            | 231 K \n","11 | lstm_encoder                       | LSTM                            | 925 K \n","12 | lstm_decoder                       | LSTM                            | 925 K \n","13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 115 K \n","14 | post_lstm_add_norm_encoder         | AddNorm                         | 480   \n","15 | static_enrichment                  | GatedResidualNetwork            | 289 K \n","16 | multihead_attn                     | InterpretableMultiHeadAttention | 144 K \n","17 | post_attn_gate_norm                | GateAddNorm                     | 116 K \n","18 | pos_wise_ff                        | GatedResidualNetwork            | 231 K \n","19 | pre_output_gate_norm               | GateAddNorm                     | 116 K \n","20 | output_layer                       | Linear                          | 2.7 K \n","----------------------------------------------------------------------------------------\n","4.0 M     Trainable params\n","0         Non-trainable params\n","4.0 M     Total params\n","15.953    Total estimated model params size (MB)\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"04f0f472ed284b2684f2f297c8e6e395\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-ba4997f9-65ba-4da8-aeea-58739f99d3d5/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n  rank_zero_warn(\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-ba4997f9-65ba-4da8-aeea-58739f99d3d5/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n  rank_zero_warn(\nMissing logger folder: /databricks/driver/lightning_logs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n   | Name                               | Type                            | Params\n----------------------------------------------------------------------------------------\n0  | loss                               | QuantileLoss                    | 0     \n1  | logging_metrics                    | ModuleList                      | 0     \n2  | input_embeddings                   | MultiEmbedding                  | 92.0 K\n3  | prescalers                         | ModuleDict                      | 96    \n4  | static_variable_selection          | VariableSelectionNetwork        | 4.2 K \n5  | encoder_variable_selection         | VariableSelectionNetwork        | 51.5 K\n6  | decoder_variable_selection         | VariableSelectionNetwork        | 45.2 K\n7  | static_context_variable_selection  | GatedResidualNetwork            | 231 K \n8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 231 K \n9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 231 K \n10 | static_context_enrichment          | GatedResidualNetwork            | 231 K \n11 | lstm_encoder                       | LSTM                            | 925 K \n12 | lstm_decoder                       | LSTM                            | 925 K \n13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 115 K \n14 | post_lstm_add_norm_encoder         | AddNorm                         | 480   \n15 | static_enrichment                  | GatedResidualNetwork            | 289 K \n16 | multihead_attn                     | InterpretableMultiHeadAttention | 144 K \n17 | post_attn_gate_norm                | GateAddNorm                     | 116 K \n18 | pos_wise_ff                        | GatedResidualNetwork            | 231 K \n19 | pre_output_gate_norm               | GateAddNorm                     | 116 K \n20 | output_layer                       | Linear                          | 2.7 K \n----------------------------------------------------------------------------------------\n4.0 M     Trainable params\n0         Non-trainable params\n4.0 M     Total params\n15.953    Total estimated model params size (MB)\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"020be022de094290b4cd563aed44e72b","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"020be022de094290b4cd563aed44e72b","version_major":2,"version_minor":0},"text/plain":"Sanity Checking: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"945553a101ac497a9c54c1e182bdea0c","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"945553a101ac497a9c54c1e182bdea0c","version_major":2,"version_minor":0},"text/plain":"Training: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f94894c47084cde9c12bc927abf6d47","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f94894c47084cde9c12bc927abf6d47","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bce97b4d8714f53950318510542ee8f","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bce97b4d8714f53950318510542ee8f","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bb57fc80b8148a3af8e55629ce6fb2d","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bb57fc80b8148a3af8e55629ce6fb2d","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6782168924724782a874b7898e501136","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6782168924724782a874b7898e501136","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44d1a4a176f146d58402cffcd735464e","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44d1a4a176f146d58402cffcd735464e","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f31d1eb561754163bfb193c260eabbbf","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f31d1eb561754163bfb193c260eabbbf","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83a230c5934f46a8942dfa49ab7825ce","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83a230c5934f46a8942dfa49ab7825ce","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1d65d727f9e43b8bdd375f3008ccbab","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1d65d727f9e43b8bdd375f3008ccbab","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a58c863178fb4d0a9569e78408779767","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a58c863178fb4d0a9569e78408779767","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45fa968df7ed4999a188708b15e1c90a","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45fa968df7ed4999a188708b15e1c90a","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"403dd5c2bc944cbba5273d35d632910c","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"403dd5c2bc944cbba5273d35d632910c","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae3555c78e1942bf933336ab6d76a0eb","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae3555c78e1942bf933336ab6d76a0eb","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27d0f5232b2f4b12bab4c1a44196beac","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27d0f5232b2f4b12bab4c1a44196beac","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5995376f789f4ef884e82650897213ae","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5995376f789f4ef884e82650897213ae","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf297fd84f364d8cb3b0fcae8137bf5c","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf297fd84f364d8cb3b0fcae8137bf5c","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f8c7ca8830d42d5aedd14884a2c1b22","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f8c7ca8830d42d5aedd14884a2c1b22","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08765f7f17f74c259c8c6cd67d66170e","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08765f7f17f74c259c8c6cd67d66170e","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b93691bc36704c7f99aaa9cf8c44fa62","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b93691bc36704c7f99aaa9cf8c44fa62","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0cc76fd900c4bd2abd5e03a536637c1","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0cc76fd900c4bd2abd5e03a536637c1","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc00c320916f4929a8405671ccfd50be","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc00c320916f4929a8405671ccfd50be","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e917fa1e69704e95862d2e9090fb7945","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e917fa1e69704e95862d2e9090fb7945","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb8ce6c4a41f4327b0c1ab791b2db7c4","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb8ce6c4a41f4327b0c1ab791b2db7c4","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cb71b21c9fd49329cba93a9dae1c3a1","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cb71b21c9fd49329cba93a9dae1c3a1","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0664ea120b2c470d890a22ad791b3c7e","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0664ea120b2c470d890a22ad791b3c7e","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae13068c5512403ea682586c22ed2c47","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae13068c5512403ea682586c22ed2c47","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34a1e471db2c4c0fbe55f8324a94ab08","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34a1e471db2c4c0fbe55f8324a94ab08","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eecf7b5ab2cd4c0184f6d4ef64f7c27c","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eecf7b5ab2cd4c0184f6d4ef64f7c27c","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2627d5e4f9f1442088bfc584673739ef","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2627d5e4f9f1442088bfc584673739ef","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4dba0bc1aee4f4284c6fd71a9324742","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4dba0bc1aee4f4284c6fd71a9324742","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64c2dd9d7277426faa16af12fd5d5fb4","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64c2dd9d7277426faa16af12fd5d5fb4","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a026c85724524ce2b5a0d4fd3779866a","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a026c85724524ce2b5a0d4fd3779866a","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a9ab83e402c499f881982cb40f511f5","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a9ab83e402c499f881982cb40f511f5","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0cc2ee14dc64a4bb999bd6db0e688f3","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0cc2ee14dc64a4bb999bd6db0e688f3","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35cfab17cba3451ab7c902bc58d0f187","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35cfab17cba3451ab7c902bc58d0f187","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c38ace7fddc54fd2893d09095ac6be88","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c38ace7fddc54fd2893d09095ac6be88","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19d19e47c6fe4f468794e01431f754fd","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19d19e47c6fe4f468794e01431f754fd","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ab29a8f6d4242f3a5ad41ea04d200f0","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ab29a8f6d4242f3a5ad41ea04d200f0","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de1d7d5489d949ff85ee523eff4acba1","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de1d7d5489d949ff85ee523eff4acba1","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"187eaaf760b7480cb417ec1ded17da97","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"187eaaf760b7480cb417ec1ded17da97","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10d60fc92e244f1d90b579b3282b4e43","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10d60fc92e244f1d90b579b3282b4e43","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0eb5088e0f64601a35a671bf3924256","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0eb5088e0f64601a35a671bf3924256","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f2c00b4c7624041bb11d0fbe6764c14","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f2c00b4c7624041bb11d0fbe6764c14","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88353d773eaa46fb8c7481d19c4af17f","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88353d773eaa46fb8c7481d19c4af17f","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50c43356d8274a1c996948b2f6a45986","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50c43356d8274a1c996948b2f6a45986","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d20d531f0c943fba5bbddd44fb73a20","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d20d531f0c943fba5bbddd44fb73a20","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3692675b019241f7a18c8000f48351f9","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3692675b019241f7a18c8000f48351f9","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98d23d1599574ec08accbcf1ec646ad8","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98d23d1599574ec08accbcf1ec646ad8","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42390b5a41564a82a951a4c31bb34343","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42390b5a41564a82a951a4c31bb34343","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45c85eb84afd48fca9a88b59e7d5b00a","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45c85eb84afd48fca9a88b59e7d5b00a","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bd8e631787442e895fd0a9ff4bf89e3","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bd8e631787442e895fd0a9ff4bf89e3","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbb56a04cd514cca829f5da70f62ad5a","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbb56a04cd514cca829f5da70f62ad5a","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e31094dc7d3444d7ab6b128609886606","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e31094dc7d3444d7ab6b128609886606","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"524a172c0ea9425b92c0318a9f5e1f8c","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"524a172c0ea9425b92c0318a9f5e1f8c","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e5d1a073ae244e88bdd2b74e44f577d","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e5d1a073ae244e88bdd2b74e44f577d","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4082513dad5f4739893a9b73fb5ae79c","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4082513dad5f4739893a9b73fb5ae79c","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"244f419453ba400bb2e8adec681cd526","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"244f419453ba400bb2e8adec681cd526","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9059eb10659407790e3b72aa171f358","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9059eb10659407790e3b72aa171f358","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"549af9917a984316b4dcaa9ba742e0bc","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"549af9917a984316b4dcaa9ba742e0bc","version_major":2,"version_minor":0},"text/plain":"Validation: 0it [00:00, ?it/s]"},"datasetInfos":[],"executionCount":null,"metadata":{"kernelSessionId":"9885b657-5f5e726985f528e9cd3731dc"},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"text/plain":["Elapsed time for fitting TFTPytorchFC model: 28962.81 s\n","path: models/model_tft.pkl\n","dirname: models\n","filename: model_tft.pkl\n","artifact_path: models\n","path: /tmp/tmp7_qtmds1/model_tft.pkl\n","tmp_path: /tmp/tmp7_qtmds1/model_tft.pkl\n","path: models/trainer_tft.pkl\n","dirname: models\n","filename: trainer_tft.pkl\n","artifact_path: models\n","path: /tmp/tmpkxyqz8h6/trainer_tft.pkl\n","tmp_path: /tmp/tmpkxyqz8h6/trainer_tft.pkl\n","path: predictions/pointpredictions_tft.csv\n","dirname: predictions\n","filename: pointpredictions_tft.csv\n","artifact_path: predictions\n","path: /tmp/tmp5o_nzck5/pointpredictions_tft.csv\n","tmp_path: /tmp/tmp5o_nzck5/pointpredictions_tft.csv\n","path: predictions/groundtruth_tft.csv\n","dirname: predictions\n","filename: groundtruth_tft.csv\n","artifact_path: predictions\n","path: /tmp/tmpu0xjeenx/groundtruth_tft.csv\n","tmp_path: /tmp/tmpu0xjeenx/groundtruth_tft.csv\n","path: predictions/quantiles_tft0.05.csv\n","dirname: predictions\n","filename: quantiles_tft0.05.csv\n","artifact_path: predictions\n","path: /tmp/tmppnmju156/quantiles_tft0.05.csv\n","tmp_path: /tmp/tmppnmju156/quantiles_tft0.05.csv\n","path: predictions/quantiles_tft0.1.csv\n","dirname: predictions\n","filename: quantiles_tft0.1.csv\n","artifact_path: predictions\n","path: /tmp/tmpqvtg4yf6/quantiles_tft0.1.csv\n","tmp_path: /tmp/tmpqvtg4yf6/quantiles_tft0.1.csv\n","path: predictions/quantiles_tft0.2.csv\n","dirname: predictions\n","filename: quantiles_tft0.2.csv\n","artifact_path: predictions\n","path: /tmp/tmpvk1skj88/quantiles_tft0.2.csv\n","tmp_path: /tmp/tmpvk1skj88/quantiles_tft0.2.csv\n","path: predictions/quantiles_tft0.3.csv\n","dirname: predictions\n","filename: quantiles_tft0.3.csv\n","artifact_path: predictions\n","path: /tmp/tmply11zbkt/quantiles_tft0.3.csv\n","tmp_path: /tmp/tmply11zbkt/quantiles_tft0.3.csv\n","path: predictions/quantiles_tft0.4.csv\n","dirname: predictions\n","filename: quantiles_tft0.4.csv\n","artifact_path: predictions\n","path: /tmp/tmps5eo906v/quantiles_tft0.4.csv\n","tmp_path: /tmp/tmps5eo906v/quantiles_tft0.4.csv\n","path: predictions/quantiles_tft0.5.csv\n","dirname: predictions\n","filename: quantiles_tft0.5.csv\n","artifact_path: predictions\n","path: /tmp/tmpogilpvkj/quantiles_tft0.5.csv\n","tmp_path: /tmp/tmpogilpvkj/quantiles_tft0.5.csv\n","path: predictions/quantiles_tft0.6.csv\n","dirname: predictions\n","filename: quantiles_tft0.6.csv\n","artifact_path: predictions\n","path: /tmp/tmp_c8yzqs7/quantiles_tft0.6.csv\n","tmp_path: /tmp/tmp_c8yzqs7/quantiles_tft0.6.csv\n","path: predictions/quantiles_tft0.7.csv\n","dirname: predictions\n","filename: quantiles_tft0.7.csv\n","artifact_path: predictions\n","path: /tmp/tmp4wxiik9z/quantiles_tft0.7.csv\n","tmp_path: /tmp/tmp4wxiik9z/quantiles_tft0.7.csv\n","path: predictions/quantiles_tft0.8.csv\n","dirname: predictions\n","filename: quantiles_tft0.8.csv\n","artifact_path: predictions\n","path: /tmp/tmpwe7wa977/quantiles_tft0.8.csv\n","tmp_path: /tmp/tmpwe7wa977/quantiles_tft0.8.csv\n","path: predictions/quantiles_tft0.9.csv\n","dirname: predictions\n","filename: quantiles_tft0.9.csv\n","artifact_path: predictions\n","path: /tmp/tmpwfq2vlk1/quantiles_tft0.9.csv\n","tmp_path: /tmp/tmpwfq2vlk1/quantiles_tft0.9.csv\n","path: predictions/quantiles_tft0.95.csv\n","dirname: predictions\n","filename: quantiles_tft0.95.csv\n","artifact_path: predictions\n","path: /tmp/tmpqxspn9av/quantiles_tft0.95.csv\n","tmp_path: /tmp/tmpqxspn9av/quantiles_tft0.95.csv\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Elapsed time for fitting TFTPytorchFC model: 28962.81 s\npath: models/model_tft.pkl\ndirname: models\nfilename: model_tft.pkl\nartifact_path: models\npath: /tmp/tmp7_qtmds1/model_tft.pkl\ntmp_path: /tmp/tmp7_qtmds1/model_tft.pkl\npath: models/trainer_tft.pkl\ndirname: models\nfilename: trainer_tft.pkl\nartifact_path: models\npath: /tmp/tmpkxyqz8h6/trainer_tft.pkl\ntmp_path: /tmp/tmpkxyqz8h6/trainer_tft.pkl\npath: predictions/pointpredictions_tft.csv\ndirname: predictions\nfilename: pointpredictions_tft.csv\nartifact_path: predictions\npath: /tmp/tmp5o_nzck5/pointpredictions_tft.csv\ntmp_path: /tmp/tmp5o_nzck5/pointpredictions_tft.csv\npath: predictions/groundtruth_tft.csv\ndirname: predictions\nfilename: groundtruth_tft.csv\nartifact_path: predictions\npath: /tmp/tmpu0xjeenx/groundtruth_tft.csv\ntmp_path: /tmp/tmpu0xjeenx/groundtruth_tft.csv\npath: predictions/quantiles_tft0.05.csv\ndirname: predictions\nfilename: quantiles_tft0.05.csv\nartifact_path: predictions\npath: /tmp/tmppnmju156/quantiles_tft0.05.csv\ntmp_path: /tmp/tmppnmju156/quantiles_tft0.05.csv\npath: predictions/quantiles_tft0.1.csv\ndirname: predictions\nfilename: quantiles_tft0.1.csv\nartifact_path: predictions\npath: /tmp/tmpqvtg4yf6/quantiles_tft0.1.csv\ntmp_path: /tmp/tmpqvtg4yf6/quantiles_tft0.1.csv\npath: predictions/quantiles_tft0.2.csv\ndirname: predictions\nfilename: quantiles_tft0.2.csv\nartifact_path: predictions\npath: /tmp/tmpvk1skj88/quantiles_tft0.2.csv\ntmp_path: /tmp/tmpvk1skj88/quantiles_tft0.2.csv\npath: predictions/quantiles_tft0.3.csv\ndirname: predictions\nfilename: quantiles_tft0.3.csv\nartifact_path: predictions\npath: /tmp/tmply11zbkt/quantiles_tft0.3.csv\ntmp_path: /tmp/tmply11zbkt/quantiles_tft0.3.csv\npath: predictions/quantiles_tft0.4.csv\ndirname: predictions\nfilename: quantiles_tft0.4.csv\nartifact_path: predictions\npath: /tmp/tmps5eo906v/quantiles_tft0.4.csv\ntmp_path: /tmp/tmps5eo906v/quantiles_tft0.4.csv\npath: predictions/quantiles_tft0.5.csv\ndirname: predictions\nfilename: quantiles_tft0.5.csv\nartifact_path: predictions\npath: /tmp/tmpogilpvkj/quantiles_tft0.5.csv\ntmp_path: /tmp/tmpogilpvkj/quantiles_tft0.5.csv\npath: predictions/quantiles_tft0.6.csv\ndirname: predictions\nfilename: quantiles_tft0.6.csv\nartifact_path: predictions\npath: /tmp/tmp_c8yzqs7/quantiles_tft0.6.csv\ntmp_path: /tmp/tmp_c8yzqs7/quantiles_tft0.6.csv\npath: predictions/quantiles_tft0.7.csv\ndirname: predictions\nfilename: quantiles_tft0.7.csv\nartifact_path: predictions\npath: /tmp/tmp4wxiik9z/quantiles_tft0.7.csv\ntmp_path: /tmp/tmp4wxiik9z/quantiles_tft0.7.csv\npath: predictions/quantiles_tft0.8.csv\ndirname: predictions\nfilename: quantiles_tft0.8.csv\nartifact_path: predictions\npath: /tmp/tmpwe7wa977/quantiles_tft0.8.csv\ntmp_path: /tmp/tmpwe7wa977/quantiles_tft0.8.csv\npath: predictions/quantiles_tft0.9.csv\ndirname: predictions\nfilename: quantiles_tft0.9.csv\nartifact_path: predictions\npath: /tmp/tmpwfq2vlk1/quantiles_tft0.9.csv\ntmp_path: /tmp/tmpwfq2vlk1/quantiles_tft0.9.csv\npath: predictions/quantiles_tft0.95.csv\ndirname: predictions\nfilename: quantiles_tft0.95.csv\nartifact_path: predictions\npath: /tmp/tmpqxspn9av/quantiles_tft0.95.csv\ntmp_path: /tmp/tmpqxspn9av/quantiles_tft0.95.csv\n","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"},{"data":{"text/plain":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n","\u001b[0;32m<command-4489943609334959>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n","\u001b[1;32m     52\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtft_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number_of_rows_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmaller_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number_of_entities\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmaller_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Store'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     56\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number_of_rows_test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmaller_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;31mNameError\u001b[0m: name 'smaller_train_df' is not defined"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<command-4489943609334959>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtft_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number_of_rows_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmaller_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number_of_entities\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmaller_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Store'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number_of_rows_test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmaller_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mNameError\u001b[0m: name 'smaller_train_df' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'smaller_train_df' is not defined","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["from pytorch_lightning import Trainer as Lightning_Trainer\n","from pytorch_lightning.callbacks import EarlyStopping\n","from pytorch_forecasting.data.encoders import TorchNormalizer, NaNLabelEncoder\n","\n","tft_pytorch_params = {\n","    'hidden_size': 240, \n","    'lstm_layers': 2, \n","    'dropout': 0.1, \n","    'attention_head_size': 4,\n","    'learning_rate': 0.001, \n","    'log_interval': -1,\n","    'log_val_interval': -1,\n","    'reduce_on_plateau_patience': 1000,  \n","}\n","\n","trainer_params = {'max_epochs': 100,\n","                  'accelerator': 'gpu',\n","                  'devices': 1,\n","                  'limit_train_batches': 100, \n","                  'gradient_clip_algorithm': 'norm', \n","                  'gradient_clip_val': 100 \n","                 }\n","\n","params_dataloader = {\n","    'num_workers': 8,\n","    'batch_size': 128\n","}\n","\n","lookback = forecast_horizon*3\n","\n","trainer_params['callbacks'] = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n","lightning_trainer = Lightning_Trainer(**trainer_params)\n","\n","start_time = time.perf_counter()\n","    \n","tft_reg = TFTPytorchFC(lookback=lookback, forecast_horizon=forecast_horizon, time_idx=\"time_index_tft\", group_ids=[\"Store\"], static_categoricals=static_cat_vars, time_varying_known_categoricals=dynamic_cat_vars, time_varying_known_reals=cont_vars,  time_varying_unknown_reals = [TARGET], quantiles = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95])\n","    \n","tft_model_trained = tft_reg.fit(full_train_df, TARGET, lightning_trainer=lightning_trainer, params_tft=tft_pytorch_params, params_dataloader=params_dataloader, params_dataset_creation={}, verbose = True)\n","tft_time = np.round(end_time - start_time, 2)\n","\n","tft_pred = tft_reg.predict(full_test_df, prediction_types=[PredEnum.POINT_ESTIMATES, PredEnum.QUANTILES])\n","tft_metrics = tft_reg.metrics(tft_y_test, tft_pred, confidence_interval_quantiles=[0.1,0.9])\n","\n","\n","#Evaluate MAPE & RMSPE without zero values as in Kaggle competition\n","y_test = np.reshape(tft_y_test, newshape=(full_train_df['Store'].nunique()*forecast_horizon,1))\n","predictions = np.reshape(tft_pred[PredEnum.POINT_ESTIMATES], newshape=(full_train_df['Store'].nunique()*forecast_horizon,1))\n","indices_nonzero = np.where(y_test!=0)\n","# Take only entries which have no zeros in ground truth\n","y_test = y_test[indices_nonzero]\n","predictions = predictions[indices_nonzero]\n","tft_metrics['rmspe_only_nonzero'] = np.sqrt(np.mean(np.square((y_test - predictions) / (y_test))))\n","tft_metrics['mape_only_nonzero'] = mean_absolute_percentage_error(y_test, predictions)\n","\n","end_time = time.perf_counter()\n","full_time = np.round(end_time - start_time, 2)\n","tft_metrics['time'] = full_time"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"944786e4-59cc-4e66-adbe-7dce65c8b58e","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[38]: {<PredEnum.POINT_ESTIMATES: 'point_estimates'>: array([[3.3465129e-01, 7.0914424e+03, 5.5127896e+03, ..., 5.7591460e+03,\n","         6.3972549e+03, 6.3521089e+03],\n","        [3.5204864e-01, 8.2911348e+03, 6.7564336e+03, ..., 6.8420571e+03,\n","         7.2872529e+03, 7.5049224e+03],\n","        [3.5193133e-01, 1.2279685e+04, 9.4032070e+03, ..., 1.0585309e+04,\n","         1.1704678e+04, 1.2021682e+04],\n","        ...,\n","        [3.4777778e-01, 8.1329141e+03, 6.1861294e+03, ..., 6.8212578e+03,\n","         7.5689517e+03, 7.3007705e+03],\n","        [3.2237825e-01, 6.8060566e+03, 5.4586694e+03, ..., 5.6836724e+03,\n","         6.0889873e+03, 6.1645010e+03],\n","        [3.7114045e-01, 1.4601336e+04, 1.1182717e+04, ..., 1.2082696e+04,\n","         1.3536629e+04, 1.3765246e+04]], dtype=float32),\n"," <PredEnum.QUANTILES: 'quantiles'>: {0.05: array([[1.6902703e-01, 5.8649287e+03, 4.6701143e+03, ..., 4.7916885e+03,\n","          5.3095605e+03, 5.2118535e+03],\n","         [2.1492589e-01, 6.8672153e+03, 5.7575532e+03, ..., 5.8242168e+03,\n","          6.2260327e+03, 6.3270811e+03],\n","         [1.8143345e-01, 9.9975332e+03, 7.8807803e+03, ..., 8.8381543e+03,\n","          9.8192383e+03, 9.8070146e+03],\n","         ...,\n","         [1.5471733e-01, 6.5833877e+03, 5.1313687e+03, ..., 5.6349873e+03,\n","          6.2914248e+03, 5.9561787e+03],\n","         [1.7336409e-01, 5.6567598e+03, 4.6462979e+03, ..., 4.8096118e+03,\n","          5.1670220e+03, 5.1195908e+03],\n","         [2.1253738e-01, 1.1869393e+04, 9.3473203e+03, ..., 9.8719375e+03,\n","          1.1165931e+04, 1.1146448e+04]], dtype=float32),\n","  0.1: array([[2.5514913e-01, 6.2258369e+03, 4.9426836e+03, ..., 5.0147393e+03,\n","          5.5114385e+03, 5.4442349e+03],\n","         [3.0131251e-01, 7.2959253e+03, 6.0964258e+03, ..., 6.0370205e+03,\n","          6.3872256e+03, 6.5224976e+03],\n","         [2.7225128e-01, 1.0741912e+04, 8.4566904e+03, ..., 9.1840449e+03,\n","          1.0028179e+04, 1.0203607e+04],\n","         ...,\n","         [2.5092125e-01, 7.0661504e+03, 5.4939336e+03, ..., 5.8815449e+03,\n","          6.4873921e+03, 6.2086274e+03],\n","         [2.4861738e-01, 5.9932271e+03, 4.9035103e+03, ..., 4.9987886e+03,\n","          5.3582656e+03, 5.3581680e+03],\n","         [3.0292064e-01, 1.2745008e+04, 1.0012677e+04, ..., 1.0368133e+04,\n","          1.1604383e+04, 1.1618990e+04]], dtype=float32),\n","  0.2: array([[3.0940488e-01, 6.4952246e+03, 5.1017441e+03, ..., 5.2812544e+03,\n","          5.8567837e+03, 5.7884116e+03],\n","         [3.5042366e-01, 7.5860952e+03, 6.2614917e+03, ..., 6.3428940e+03,\n","          6.7517051e+03, 6.8909019e+03],\n","         [3.2684255e-01, 1.1149170e+04, 8.6765107e+03, ..., 9.6930898e+03,\n","          1.0743890e+04, 1.0857140e+04],\n","         ...,\n","         [3.0909023e-01, 7.3742617e+03, 5.6874634e+03, ..., 6.2175229e+03,\n","          6.9195981e+03, 6.6061455e+03],\n","         [3.0371487e-01, 6.2287832e+03, 5.0534326e+03, ..., 5.2377637e+03,\n","          5.6071870e+03, 5.6598037e+03],\n","         [3.6504874e-01, 1.3276180e+04, 1.0324152e+04, ..., 1.0935353e+04,\n","          1.2268235e+04, 1.2361788e+04]], dtype=float32),\n","  0.3: array([[3.1782967e-01, 6.7146045e+03, 5.2525981e+03, ..., 5.3914512e+03,\n","          5.9706719e+03, 5.9387236e+03],\n","         [3.4045586e-01, 7.8216147e+03, 6.4260962e+03, ..., 6.4771099e+03,\n","          6.8889038e+03, 7.0651592e+03],\n","         [3.3835205e-01, 1.1573346e+04, 8.9295713e+03, ..., 9.8909561e+03,\n","          1.0935895e+04, 1.1164035e+04],\n","         ...,\n","         [3.2044888e-01, 7.6403530e+03, 5.8478916e+03, ..., 6.3489653e+03,\n","          7.0500000e+03, 6.7958525e+03],\n","         [3.0982140e-01, 6.4398467e+03, 5.2027295e+03, ..., 5.3654614e+03,\n","          5.7555874e+03, 5.8270503e+03],\n","         [3.5688224e-01, 1.3807888e+04, 1.0644119e+04, ..., 1.1343028e+04,\n","          1.2689480e+04, 1.2822869e+04]], dtype=float32),\n","  0.4: array([[2.9069993e-01, 6.9015645e+03, 5.3933081e+03, ..., 5.6110127e+03,\n","          6.1331294e+03, 6.1428594e+03],\n","         [3.3087128e-01, 8.0334834e+03, 6.5971416e+03, ..., 6.7318545e+03,\n","          7.0580669e+03, 7.2955703e+03],\n","         [3.0005595e-01, 1.1840797e+04, 9.1612285e+03, ..., 1.0363615e+04,\n","          1.1269120e+04, 1.1658142e+04],\n","         ...,\n","         [2.9106063e-01, 7.8815396e+03, 6.0311904e+03, ..., 6.6273955e+03,\n","          7.2280254e+03, 7.0458457e+03],\n","         [2.8429240e-01, 6.6250381e+03, 5.3459365e+03, ..., 5.5983320e+03,\n","          5.9113477e+03, 6.0328359e+03],\n","         [3.4454837e-01, 1.4187788e+04, 1.0927658e+04, ..., 1.1819225e+04,\n","          1.3063557e+04, 1.3283299e+04]], dtype=float32),\n","  0.5: array([[3.3465129e-01, 7.0914424e+03, 5.5127896e+03, ..., 5.7591460e+03,\n","          6.3972549e+03, 6.3521089e+03],\n","         [3.5204864e-01, 8.2911348e+03, 6.7564336e+03, ..., 6.8420571e+03,\n","          7.2872529e+03, 7.5049224e+03],\n","         [3.5193133e-01, 1.2279685e+04, 9.4032070e+03, ..., 1.0585309e+04,\n","          1.1704678e+04, 1.2021682e+04],\n","         ...,\n","         [3.4777778e-01, 8.1329141e+03, 6.1861294e+03, ..., 6.8212578e+03,\n","          7.5689517e+03, 7.3007705e+03],\n","         [3.2237825e-01, 6.8060566e+03, 5.4586694e+03, ..., 5.6836724e+03,\n","          6.0889873e+03, 6.1645010e+03],\n","         [3.7114045e-01, 1.4601336e+04, 1.1182717e+04, ..., 1.2082696e+04,\n","          1.3536629e+04, 1.3765246e+04]], dtype=float32),\n","  0.6: array([[3.5282379e-01, 7.2714365e+03, 5.6526284e+03, ..., 5.8764922e+03,\n","          6.5339546e+03, 6.5382686e+03],\n","         [3.6764050e-01, 8.4450918e+03, 6.8793960e+03, ..., 6.9926138e+03,\n","          7.4491885e+03, 7.6806411e+03],\n","         [3.7599048e-01, 1.2596885e+04, 9.6388818e+03, ..., 1.0789721e+04,\n","          1.1930203e+04, 1.2292352e+04],\n","         ...,\n","         [3.6573756e-01, 8.3344648e+03, 6.3326357e+03, ..., 6.9637275e+03,\n","          7.7605684e+03, 7.5205361e+03],\n","         [3.4262061e-01, 6.9392217e+03, 5.5585630e+03, ..., 5.7512026e+03,\n","          6.2119507e+03, 6.3111387e+03],\n","         [3.9111328e-01, 1.5016192e+04, 1.1504958e+04, ..., 1.2324234e+04,\n","          1.3892137e+04, 1.4230558e+04]], dtype=float32),\n","  0.7: array([[4.66377884e-01, 7.64589355e+03, 5.90371240e+03, ...,\n","          6.07768945e+03, 6.72074902e+03, 6.73058984e+03],\n","         [4.94491965e-01, 8.88926758e+03, 7.17541553e+03, ...,\n","          7.21782910e+03, 7.64284570e+03, 7.90024268e+03],\n","         [4.98955399e-01, 1.33581406e+04, 1.01077930e+04, ...,\n","          1.12151875e+04, 1.23114180e+04, 1.27263867e+04],\n","         ...,\n","         [4.81059790e-01, 8.83267676e+03, 6.65662646e+03, ...,\n","          7.21217236e+03, 7.97294336e+03, 7.74887451e+03],\n","         [4.51392293e-01, 7.30396729e+03, 5.79267578e+03, ...,\n","          5.97907227e+03, 6.44017236e+03, 6.54531348e+03],\n","         [5.17554164e-01, 1.58365557e+04, 1.20096377e+04, ...,\n","          1.28145439e+04, 1.44861797e+04, 1.46453525e+04]], dtype=float32),\n","  0.8: array([[4.4067717e-01, 7.7514321e+03, 5.9843389e+03, ..., 6.2698525e+03,\n","          6.9632295e+03, 6.9531348e+03],\n","         [4.4738656e-01, 8.9577041e+03, 7.2316665e+03, ..., 7.3814746e+03,\n","          7.8437432e+03, 8.0851934e+03],\n","         [4.7383443e-01, 1.3587139e+04, 1.0250927e+04, ..., 1.1455074e+04,\n","          1.2632822e+04, 1.3108013e+04],\n","         ...,\n","         [4.5858902e-01, 8.9432686e+03, 6.7479131e+03, ..., 7.4568379e+03,\n","          8.2947256e+03, 8.0338896e+03],\n","         [4.2201462e-01, 7.3710898e+03, 5.8503799e+03, ..., 6.1027031e+03,\n","          6.6125317e+03, 6.7150654e+03],\n","         [4.7985685e-01, 1.6001191e+04, 1.2155156e+04, ..., 1.3044496e+04,\n","          1.4742724e+04, 1.5010780e+04]], dtype=float32),\n","  0.9: array([[5.0066751e-01, 8.0797354e+03, 6.1551997e+03, ..., 6.5753110e+03,\n","          7.2140781e+03, 7.2200806e+03],\n","         [4.9983695e-01, 9.2615508e+03, 7.4178931e+03, ..., 7.6720312e+03,\n","          8.0563188e+03, 8.3604355e+03],\n","         [5.3516006e-01, 1.4044742e+04, 1.0507014e+04, ..., 1.2050806e+04,\n","          1.3102775e+04, 1.3604358e+04],\n","         ...,\n","         [5.3339833e-01, 9.3959277e+03, 6.9888862e+03, ..., 7.9034380e+03,\n","          8.6393018e+03, 8.3737549e+03],\n","         [4.7224349e-01, 7.6323921e+03, 6.0184443e+03, ..., 6.3358979e+03,\n","          6.7656875e+03, 6.9315679e+03],\n","         [5.3926706e-01, 1.6587658e+04, 1.2480621e+04, ..., 1.3715082e+04,\n","          1.5479892e+04, 1.5926679e+04]], dtype=float32),\n","  0.95: array([[3.9205819e-01, 8.1785293e+03, 6.2233193e+03, ..., 6.7840010e+03,\n","          7.5627603e+03, 7.4856484e+03],\n","         [4.0245253e-01, 9.3740596e+03, 7.4636401e+03, ..., 7.8186982e+03,\n","          8.3002891e+03, 8.5933955e+03],\n","         [4.1802436e-01, 1.4162642e+04, 1.0537612e+04, ..., 1.2357084e+04,\n","          1.3746367e+04, 1.4183053e+04],\n","         ...,\n","         [4.1757682e-01, 9.5660254e+03, 7.0747148e+03, ..., 8.1434395e+03,\n","          9.0569209e+03, 8.6903086e+03],\n","         [3.7366778e-01, 7.7397817e+03, 6.0873203e+03, ..., 6.4606973e+03,\n","          6.9725005e+03, 7.1846606e+03],\n","         [4.2801189e-01, 1.7076918e+04, 1.2712605e+04, ..., 1.4270326e+04,\n","          1.6115890e+04, 1.6507816e+04]], dtype=float32)}}"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[38]: {<PredEnum.POINT_ESTIMATES: 'point_estimates'>: array([[3.3465129e-01, 7.0914424e+03, 5.5127896e+03, ..., 5.7591460e+03,\n         6.3972549e+03, 6.3521089e+03],\n        [3.5204864e-01, 8.2911348e+03, 6.7564336e+03, ..., 6.8420571e+03,\n         7.2872529e+03, 7.5049224e+03],\n        [3.5193133e-01, 1.2279685e+04, 9.4032070e+03, ..., 1.0585309e+04,\n         1.1704678e+04, 1.2021682e+04],\n        ...,\n        [3.4777778e-01, 8.1329141e+03, 6.1861294e+03, ..., 6.8212578e+03,\n         7.5689517e+03, 7.3007705e+03],\n        [3.2237825e-01, 6.8060566e+03, 5.4586694e+03, ..., 5.6836724e+03,\n         6.0889873e+03, 6.1645010e+03],\n        [3.7114045e-01, 1.4601336e+04, 1.1182717e+04, ..., 1.2082696e+04,\n         1.3536629e+04, 1.3765246e+04]], dtype=float32),\n <PredEnum.QUANTILES: 'quantiles'>: {0.05: array([[1.6902703e-01, 5.8649287e+03, 4.6701143e+03, ..., 4.7916885e+03,\n          5.3095605e+03, 5.2118535e+03],\n         [2.1492589e-01, 6.8672153e+03, 5.7575532e+03, ..., 5.8242168e+03,\n          6.2260327e+03, 6.3270811e+03],\n         [1.8143345e-01, 9.9975332e+03, 7.8807803e+03, ..., 8.8381543e+03,\n          9.8192383e+03, 9.8070146e+03],\n         ...,\n         [1.5471733e-01, 6.5833877e+03, 5.1313687e+03, ..., 5.6349873e+03,\n          6.2914248e+03, 5.9561787e+03],\n         [1.7336409e-01, 5.6567598e+03, 4.6462979e+03, ..., 4.8096118e+03,\n          5.1670220e+03, 5.1195908e+03],\n         [2.1253738e-01, 1.1869393e+04, 9.3473203e+03, ..., 9.8719375e+03,\n          1.1165931e+04, 1.1146448e+04]], dtype=float32),\n  0.1: array([[2.5514913e-01, 6.2258369e+03, 4.9426836e+03, ..., 5.0147393e+03,\n          5.5114385e+03, 5.4442349e+03],\n         [3.0131251e-01, 7.2959253e+03, 6.0964258e+03, ..., 6.0370205e+03,\n          6.3872256e+03, 6.5224976e+03],\n         [2.7225128e-01, 1.0741912e+04, 8.4566904e+03, ..., 9.1840449e+03,\n          1.0028179e+04, 1.0203607e+04],\n         ...,\n         [2.5092125e-01, 7.0661504e+03, 5.4939336e+03, ..., 5.8815449e+03,\n          6.4873921e+03, 6.2086274e+03],\n         [2.4861738e-01, 5.9932271e+03, 4.9035103e+03, ..., 4.9987886e+03,\n          5.3582656e+03, 5.3581680e+03],\n         [3.0292064e-01, 1.2745008e+04, 1.0012677e+04, ..., 1.0368133e+04,\n          1.1604383e+04, 1.1618990e+04]], dtype=float32),\n  0.2: array([[3.0940488e-01, 6.4952246e+03, 5.1017441e+03, ..., 5.2812544e+03,\n          5.8567837e+03, 5.7884116e+03],\n         [3.5042366e-01, 7.5860952e+03, 6.2614917e+03, ..., 6.3428940e+03,\n          6.7517051e+03, 6.8909019e+03],\n         [3.2684255e-01, 1.1149170e+04, 8.6765107e+03, ..., 9.6930898e+03,\n          1.0743890e+04, 1.0857140e+04],\n         ...,\n         [3.0909023e-01, 7.3742617e+03, 5.6874634e+03, ..., 6.2175229e+03,\n          6.9195981e+03, 6.6061455e+03],\n         [3.0371487e-01, 6.2287832e+03, 5.0534326e+03, ..., 5.2377637e+03,\n          5.6071870e+03, 5.6598037e+03],\n         [3.6504874e-01, 1.3276180e+04, 1.0324152e+04, ..., 1.0935353e+04,\n          1.2268235e+04, 1.2361788e+04]], dtype=float32),\n  0.3: array([[3.1782967e-01, 6.7146045e+03, 5.2525981e+03, ..., 5.3914512e+03,\n          5.9706719e+03, 5.9387236e+03],\n         [3.4045586e-01, 7.8216147e+03, 6.4260962e+03, ..., 6.4771099e+03,\n          6.8889038e+03, 7.0651592e+03],\n         [3.3835205e-01, 1.1573346e+04, 8.9295713e+03, ..., 9.8909561e+03,\n          1.0935895e+04, 1.1164035e+04],\n         ...,\n         [3.2044888e-01, 7.6403530e+03, 5.8478916e+03, ..., 6.3489653e+03,\n          7.0500000e+03, 6.7958525e+03],\n         [3.0982140e-01, 6.4398467e+03, 5.2027295e+03, ..., 5.3654614e+03,\n          5.7555874e+03, 5.8270503e+03],\n         [3.5688224e-01, 1.3807888e+04, 1.0644119e+04, ..., 1.1343028e+04,\n          1.2689480e+04, 1.2822869e+04]], dtype=float32),\n  0.4: array([[2.9069993e-01, 6.9015645e+03, 5.3933081e+03, ..., 5.6110127e+03,\n          6.1331294e+03, 6.1428594e+03],\n         [3.3087128e-01, 8.0334834e+03, 6.5971416e+03, ..., 6.7318545e+03,\n          7.0580669e+03, 7.2955703e+03],\n         [3.0005595e-01, 1.1840797e+04, 9.1612285e+03, ..., 1.0363615e+04,\n          1.1269120e+04, 1.1658142e+04],\n         ...,\n         [2.9106063e-01, 7.8815396e+03, 6.0311904e+03, ..., 6.6273955e+03,\n          7.2280254e+03, 7.0458457e+03],\n         [2.8429240e-01, 6.6250381e+03, 5.3459365e+03, ..., 5.5983320e+03,\n          5.9113477e+03, 6.0328359e+03],\n         [3.4454837e-01, 1.4187788e+04, 1.0927658e+04, ..., 1.1819225e+04,\n          1.3063557e+04, 1.3283299e+04]], dtype=float32),\n  0.5: array([[3.3465129e-01, 7.0914424e+03, 5.5127896e+03, ..., 5.7591460e+03,\n          6.3972549e+03, 6.3521089e+03],\n         [3.5204864e-01, 8.2911348e+03, 6.7564336e+03, ..., 6.8420571e+03,\n          7.2872529e+03, 7.5049224e+03],\n         [3.5193133e-01, 1.2279685e+04, 9.4032070e+03, ..., 1.0585309e+04,\n          1.1704678e+04, 1.2021682e+04],\n         ...,\n         [3.4777778e-01, 8.1329141e+03, 6.1861294e+03, ..., 6.8212578e+03,\n          7.5689517e+03, 7.3007705e+03],\n         [3.2237825e-01, 6.8060566e+03, 5.4586694e+03, ..., 5.6836724e+03,\n          6.0889873e+03, 6.1645010e+03],\n         [3.7114045e-01, 1.4601336e+04, 1.1182717e+04, ..., 1.2082696e+04,\n          1.3536629e+04, 1.3765246e+04]], dtype=float32),\n  0.6: array([[3.5282379e-01, 7.2714365e+03, 5.6526284e+03, ..., 5.8764922e+03,\n          6.5339546e+03, 6.5382686e+03],\n         [3.6764050e-01, 8.4450918e+03, 6.8793960e+03, ..., 6.9926138e+03,\n          7.4491885e+03, 7.6806411e+03],\n         [3.7599048e-01, 1.2596885e+04, 9.6388818e+03, ..., 1.0789721e+04,\n          1.1930203e+04, 1.2292352e+04],\n         ...,\n         [3.6573756e-01, 8.3344648e+03, 6.3326357e+03, ..., 6.9637275e+03,\n          7.7605684e+03, 7.5205361e+03],\n         [3.4262061e-01, 6.9392217e+03, 5.5585630e+03, ..., 5.7512026e+03,\n          6.2119507e+03, 6.3111387e+03],\n         [3.9111328e-01, 1.5016192e+04, 1.1504958e+04, ..., 1.2324234e+04,\n          1.3892137e+04, 1.4230558e+04]], dtype=float32),\n  0.7: array([[4.66377884e-01, 7.64589355e+03, 5.90371240e+03, ...,\n          6.07768945e+03, 6.72074902e+03, 6.73058984e+03],\n         [4.94491965e-01, 8.88926758e+03, 7.17541553e+03, ...,\n          7.21782910e+03, 7.64284570e+03, 7.90024268e+03],\n         [4.98955399e-01, 1.33581406e+04, 1.01077930e+04, ...,\n          1.12151875e+04, 1.23114180e+04, 1.27263867e+04],\n         ...,\n         [4.81059790e-01, 8.83267676e+03, 6.65662646e+03, ...,\n          7.21217236e+03, 7.97294336e+03, 7.74887451e+03],\n         [4.51392293e-01, 7.30396729e+03, 5.79267578e+03, ...,\n          5.97907227e+03, 6.44017236e+03, 6.54531348e+03],\n         [5.17554164e-01, 1.58365557e+04, 1.20096377e+04, ...,\n          1.28145439e+04, 1.44861797e+04, 1.46453525e+04]], dtype=float32),\n  0.8: array([[4.4067717e-01, 7.7514321e+03, 5.9843389e+03, ..., 6.2698525e+03,\n          6.9632295e+03, 6.9531348e+03],\n         [4.4738656e-01, 8.9577041e+03, 7.2316665e+03, ..., 7.3814746e+03,\n          7.8437432e+03, 8.0851934e+03],\n         [4.7383443e-01, 1.3587139e+04, 1.0250927e+04, ..., 1.1455074e+04,\n          1.2632822e+04, 1.3108013e+04],\n         ...,\n         [4.5858902e-01, 8.9432686e+03, 6.7479131e+03, ..., 7.4568379e+03,\n          8.2947256e+03, 8.0338896e+03],\n         [4.2201462e-01, 7.3710898e+03, 5.8503799e+03, ..., 6.1027031e+03,\n          6.6125317e+03, 6.7150654e+03],\n         [4.7985685e-01, 1.6001191e+04, 1.2155156e+04, ..., 1.3044496e+04,\n          1.4742724e+04, 1.5010780e+04]], dtype=float32),\n  0.9: array([[5.0066751e-01, 8.0797354e+03, 6.1551997e+03, ..., 6.5753110e+03,\n          7.2140781e+03, 7.2200806e+03],\n         [4.9983695e-01, 9.2615508e+03, 7.4178931e+03, ..., 7.6720312e+03,\n          8.0563188e+03, 8.3604355e+03],\n         [5.3516006e-01, 1.4044742e+04, 1.0507014e+04, ..., 1.2050806e+04,\n          1.3102775e+04, 1.3604358e+04],\n         ...,\n         [5.3339833e-01, 9.3959277e+03, 6.9888862e+03, ..., 7.9034380e+03,\n          8.6393018e+03, 8.3737549e+03],\n         [4.7224349e-01, 7.6323921e+03, 6.0184443e+03, ..., 6.3358979e+03,\n          6.7656875e+03, 6.9315679e+03],\n         [5.3926706e-01, 1.6587658e+04, 1.2480621e+04, ..., 1.3715082e+04,\n          1.5479892e+04, 1.5926679e+04]], dtype=float32),\n  0.95: array([[3.9205819e-01, 8.1785293e+03, 6.2233193e+03, ..., 6.7840010e+03,\n          7.5627603e+03, 7.4856484e+03],\n         [4.0245253e-01, 9.3740596e+03, 7.4636401e+03, ..., 7.8186982e+03,\n          8.3002891e+03, 8.5933955e+03],\n         [4.1802436e-01, 1.4162642e+04, 1.0537612e+04, ..., 1.2357084e+04,\n          1.3746367e+04, 1.4183053e+04],\n         ...,\n         [4.1757682e-01, 9.5660254e+03, 7.0747148e+03, ..., 8.1434395e+03,\n          9.0569209e+03, 8.6903086e+03],\n         [3.7366778e-01, 7.7397817e+03, 6.0873203e+03, ..., 6.4606973e+03,\n          6.9725005e+03, 7.1846606e+03],\n         [4.2801189e-01, 1.7076918e+04, 1.2712605e+04, ..., 1.4270326e+04,\n          1.6115890e+04, 1.6507816e+04]], dtype=float32)}}","datasetInfos":[],"metadata":{},"name":null,"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["tft_metrics"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"01_rossmann_training_simplified","notebookOrigID":228038575925283,"widgets":{}},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"cfd4d087e7a556ba97e2c34f2cfa4f042f258f3501cc6522584d6d4cf4e7ebf3"}}},"nbformat":4,"nbformat_minor":0}
